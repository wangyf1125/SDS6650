{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "5ccb1d8a",
      "metadata": {
        "id": "5ccb1d8a"
      },
      "source": [
        "# Intermediate Machine Learning: Assignment 5\n",
        "\n",
        "**Deadline**\n",
        "\n",
        "Assignment 5 is due Thursday, December 4 by 11:59pm. Late work will not be accepted as per the course policies (see the Syllabus and Course policies on Canvas).\n",
        "\n",
        "Directly sharing answers is not okay, but discussing problems with the course staff or with other students is encouraged. Acknowledge any use of an AI system such as ChatGPT or Copilot.\n",
        "\n",
        "You should start early so that you have time to get help if you're stuck. The drop-in office hours schedule can be found on Canvas. You can also post questions or start discussions on Ed Discussion. The assignment may look long at first glance, but the problems are broken up into steps that should help you to make steady progress.\n",
        "\n",
        "**Submission**\n",
        "\n",
        "Submit your assignment as a pdf file on Gradescope, and as a notebook (.ipynb) on Canvas. You can access Gradescope through Canvas on the left-side of the class home page. The problems in each homework assignment are numbered. Note: When submitting on Gradescope, please select the correct pages of your pdf that correspond to each problem. This will allow graders to more easily find your complete solution to each problem.\n",
        "\n",
        "To produce the .pdf, please convert to html and then print to pdf. (You may want to use your pdf print menu to scale the pages to be sure that cells are not truncated.) To convert to html, you can use this [converter notebook](https://colab.research.google.com/github/YData123/sds365-fa25/blob/main/assignments/Convert_ipynb_to_HTML_in_Colab.ipynb).\n",
        "\n",
        "You should start early so that you have time to get help if you're stuck. The drop-in office hours schedule can be found on Canvas. You can also post questions or start discussions on Ed Discussion. The assignment may look long at first glance, but the problems are broken up into steps that should help you to make steady progress.\n",
        "\n",
        "**Topics**\n",
        "\n",
        " * RNNs and GRUs\n",
        " * Transformers\n",
        "\n",
        "After doing this assignment you will have a working knowledge of RNNs and Transformers, and more solid Python skills."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "78f64b66",
      "metadata": {
        "id": "78f64b66"
      },
      "source": [
        "## Problem 1: Elephants Can Remember (25 points)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ba3663cd",
      "metadata": {
        "id": "ba3663cd"
      },
      "source": [
        "![ECR](https://upload.wikimedia.org/wikipedia/en/e/e3/Elephants_can_Remember_First_Edition_Cover_1972.jpg)\n",
        "\n",
        "In this problem, we will work with \"vanilla\" Recurrent Neural Networks (RNNs) and Recurrent Neural Networks with Gated Recurrent Units (GRUs). The models in this part of the assignment will be character-based models, trained on an extract of the book [Elephants Can Remember](https://en.wikipedia.org/wiki/Elephants_Can_Remember) by Agatha Christie. To reduce the size of our vocabulary, the text is pre-processed by converting the letters to lower case and removing numbers. The code below shows some information about our training and test set. All the necessary files for this problem are available on [github](https://github.com/YData123/sds365-fa25/tree/main/assignments/assn5).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "id": "0804721c",
      "metadata": {
        "id": "0804721c"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import tensorflow as tf\n",
        "import random\n",
        "import json\n",
        "\n",
        "# Ensure tf.keras is the main keras module\n",
        "# import tensorflow.keras as keras # This line is no longer strictly necessary if all components are explicitly imported below\n",
        "\n",
        "from tensorflow.keras.models import Sequential, model_from_json\n",
        "from tensorflow.keras.layers import Dense, Activation\n",
        "from tensorflow.keras.layers import GRU, SimpleRNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "id": "3qF67L0FBdmT",
      "metadata": {
        "id": "3qF67L0FBdmT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "06849ddb-1742-4d94-d26c-258b9579e16d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "data_path = '/content/drive/MyDrive/assn5/' # set to the path where you store the assignment files from github\n",
        "                # e.g., '/content/drive/MyDrive/assn5/'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "id": "f93de082",
      "metadata": {
        "id": "f93de082"
      },
      "outputs": [],
      "source": [
        "with open(data_path + 'gru_problem/Agatha_Christie_train.txt', 'r') as file:\n",
        "    train_text = file.read()\n",
        "\n",
        "with open(data_path + 'gru_problem/Agatha_Christie_test.txt', 'r') as file:\n",
        "    test_text = file.read()\n",
        "\n",
        "vocabulary = sorted(list(set(train_text + test_text)))\n",
        "vocab_size = len(vocabulary)\n",
        "\n",
        "# Dictionaries to go from a character to index and vice versa\n",
        "char_to_indices = dict((c, i) for i, c in enumerate(vocabulary))\n",
        "indices_to_char = dict((i, c) for i, c in enumerate(vocabulary))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "id": "7320ed64",
      "metadata": {
        "id": "7320ed64",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "9a750b79-3b96-4ec9-b712-6c95916fc1a5"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'mrs. oliver looked at herself in the glass. she gave a brief, sideways look towards the clock on the mantelpiece, which she had some idea was twenty minutes slow. then she resumed her study of her coiffure. the trouble with mrs. oliver was--and she admitted it freely--that her styles of hairdressing were always being changed. she had tried almost everything in turn. a severe pompadour at one time, then a wind-swept style where you brushed back your locks to display an intellectual brow, at least'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 50
        }
      ],
      "source": [
        "# The first 500 characters of our training set\n",
        "train_text[0:500]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "id": "3a50abac",
      "metadata": {
        "id": "3a50abac",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3af16964-6abe-4627-a398-b4333a6f449e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The vocabulary contains 44 characters\n",
            "The training set contains 262174 characters\n",
            "The test set contains 7209 characters\n"
          ]
        }
      ],
      "source": [
        "print(\"The vocabulary contains\", vocab_size, \"characters\")\n",
        "print(\"The training set contains\", len(train_text) ,\"characters\")\n",
        "print(\"The test set contains\", len(test_text) ,\"characters\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d02ce399",
      "metadata": {
        "id": "d02ce399"
      },
      "source": [
        "### Problem 1.1: The Diversity of Language Models"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6951a417",
      "metadata": {
        "id": "6951a417"
      },
      "source": [
        "Before jumping into coding, let's start with comparing the language models we will be using in this assigment."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "472741ad",
      "metadata": {
        "id": "472741ad"
      },
      "source": [
        "1. Describe the differences between a Vanilla RNN and a GRU network. In your explanation, make sure you mention the issues with vanilla RNNs and how GRUs try to solve them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "06832930",
      "metadata": {
        "id": "06832930"
      },
      "outputs": [],
      "source": [
        "# Your markdown here"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Differences between a Vanilla RNN and a GRU Network\n",
        "\n",
        "A **Vanilla Recurrent Neural Network (RNN)** updates its hidden state using a simple transition:\n",
        "\n",
        "$$\n",
        "h_t = \\tanh(W_h h_{t-1} + W_x x_t + b)\n",
        "$$\n",
        "\n",
        "Although vanilla RNNs are able to model sequential data, they suffer from **vanishing and exploding gradient problems** during backpropagation through time (BPTT). As a result, they struggle to learn **long-term dependencies**, meaning information from far earlier time steps is often forgotten.\n",
        "\n",
        "A **Gated Recurrent Unit (GRU)** addresses these issues by introducing **gating mechanisms** that control how information flows through time. A GRU contains two gates:\n",
        "\n",
        "- **Update gate** $$ z_t $$: controls how much of the previous hidden state is kept  \n",
        "- **Reset gate** $$ r_t $$: controls how much past information is forgotten  \n",
        "\n",
        "The GRU updates are given by:\n",
        "\n",
        "$$\n",
        "z_t = \\sigma(W_z x_t + U_z h_{t-1})\n",
        "$$\n",
        "\n",
        "$$\n",
        "r_t = \\sigma(W_r x_t + U_r h_{t-1})\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\tilde{h}_t = \\tanh(W x_t + U (r_t \\odot h_{t-1}))\n",
        "$$\n",
        "\n",
        "$$\n",
        "h_t = (1 - z_t) \\odot h_{t-1} + z_t \\odot \\tilde{h}_t\n",
        "$$\n",
        "\n",
        "By using these gates, GRUs can:\n",
        "- Preserve important long-term information  \n",
        "- Mitigate vanishing gradients  \n",
        "- Adaptively forget irrelevant history  \n",
        "\n",
        "As a result, **GRUs are more stable and effective than vanilla RNNs for modeling long sequences**, such as character-based text.\n"
      ],
      "metadata": {
        "id": "T6tOrRskiODx"
      },
      "id": "T6tOrRskiODx"
    },
    {
      "cell_type": "markdown",
      "id": "61c45e86",
      "metadata": {
        "id": "61c45e86"
      },
      "source": [
        "2. Describe at least two advantages of a character based language model over a word based language model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8b07db8a",
      "metadata": {
        "id": "8b07db8a"
      },
      "outputs": [],
      "source": [
        "# Your markdown here"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### 2. Advantages of a Character-Based Language Model over a Word-Based Model\n",
        "\n",
        "#### (1) No Out-of-Vocabulary (OOV) Problem\n",
        "\n",
        "A character-based model works on individual characters instead of full words. Therefore, it can generate **any word**, including:\n",
        "- Rare words  \n",
        "- New words  \n",
        "- Misspelled words  \n",
        "- Names not seen in training  \n",
        "\n",
        "In contrast, word-based models cannot handle words that are not in their predefined vocabulary.\n",
        "\n",
        "---\n",
        "\n",
        "#### (2) Smaller and Simpler Vocabulary\n",
        "\n",
        "Character vocabularies are typically very small (e.g., 40–100 characters), whereas word vocabularies often contain **tens of thousands of unique tokens**. This leads to:\n",
        "- Lower memory usage  \n",
        "- Smaller embedding/output layers  \n",
        "- Easier training and deployment  \n",
        "\n",
        "---\n",
        "\n",
        "#### (3) Better Modeling of Morphology and Spelling\n",
        "\n",
        "Character-based models naturally learn:\n",
        "- Prefixes and suffixes  \n",
        "- Spelling patterns  \n",
        "- Punctuation and formatting  \n",
        "\n",
        "This is especially helpful for modeling **literary text**, such as novels, where stylistic details matter."
      ],
      "metadata": {
        "id": "Kxhfe99vieqA"
      },
      "id": "Kxhfe99vieqA"
    },
    {
      "cell_type": "markdown",
      "id": "ccac684e",
      "metadata": {
        "id": "ccac684e"
      },
      "source": [
        "### Problem 1.2: Generating Text with the Vanilla RNN"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "85bdca8b",
      "metadata": {
        "id": "85bdca8b"
      },
      "source": [
        "The code below loads in a pretrained vanilla RNN model with two layers. The model is set up exactly like in the lecture slides (with tanh activation layers in the recurrent layers) with the addition of biases (intercepts) in every layer (i.e. the recurrent layer and the dense layer). The training process consisted of 30 epochs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "id": "45bd5d3b",
      "metadata": {
        "id": "45bd5d3b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1d82f4fe-0ac8-439d-b210-4d268700e8df"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/rnn/rnn.py:199: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        }
      ],
      "source": [
        "# load json and create model\n",
        "json_file = open(data_path + 'gru_problem/RNN_model.json', 'r')\n",
        "loaded_model_json_str = json_file.read()\n",
        "json_file.close()\n",
        "\n",
        "RNN_model = model_from_json(loaded_model_json_str, custom_objects={'Sequential': Sequential, 'SimpleRNN': SimpleRNN})\n",
        "RNN_model.load_weights(data_path + \"gru_problem/RNN_model.h5\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "id": "648273ea",
      "metadata": {
        "id": "648273ea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 259
        },
        "outputId": "be282705-42da-469c-c79c-fbbc5d35cc06"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ Vanilla_RNN_1 (\u001b[38;5;33mSimpleRNN\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │        \u001b[38;5;34m22,144\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ Vanilla_RNN_2 (\u001b[38;5;33mSimpleRNN\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │        \u001b[38;5;34m12,352\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ Dense_layer (\u001b[38;5;33mDense\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m44\u001b[0m)             │         \u001b[38;5;34m2,860\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ Softmax_layer (\u001b[38;5;33mActivation\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m44\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ Vanilla_RNN_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SimpleRNN</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │        <span style=\"color: #00af00; text-decoration-color: #00af00\">22,144</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ Vanilla_RNN_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SimpleRNN</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">12,352</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ Dense_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">44</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,860</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ Softmax_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">44</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m37,356\u001b[0m (145.92 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">37,356</span> (145.92 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m37,356\u001b[0m (145.92 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">37,356</span> (145.92 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# load in the weights and show summary\n",
        "weights_RNN = RNN_model.get_weights()\n",
        "RNN_model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a4bcccd8",
      "metadata": {
        "id": "a4bcccd8"
      },
      "source": [
        "Finish the following function that uses a vanilla RNN architecture to generate text, given the weights of the RNN model, a text prompt, and the number of characters to return. The function should be completed by **only using numpy functions**. Use your knowledge of how every weight plays its role in the RNN architecture. Do not worry about the weight extraction part, this is already provided for you. The weight matrix $W_{xh1}$, for example, denotes the weight matrix to go from the input x to the first hidden state layer h1. The hidden states $h_1$ and $h_2$ are initialized to a vector of zeros.\n",
        "\n",
        "The embedding of each character has to be done by a one-hot encoding, where you will need the dictionaries defined in the introduction to go from a character to an index position."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "id": "736ff633",
      "metadata": {
        "id": "736ff633"
      },
      "outputs": [],
      "source": [
        "def sample_text_RNN(weights, prompt, N):\n",
        "    '''\n",
        "    Uses a pretrained RNN to generate text, starting from a prompt,\n",
        "    only using the weights and numpy commands\n",
        "            Parameters:\n",
        "                    weights (list): Weights of the pretrained RNN model\n",
        "                    prompt (string): Start of generated sentence\n",
        "                    N (int): Length of output sentence (including prompt)\n",
        "            Returns:\n",
        "                    output_sentence (string): Text generated by RNN\n",
        "    '''\n",
        "    # Extracting weights and biases\n",
        "    # Dimensions of matrices are same format as lecture slides\n",
        "\n",
        "    # First Recurrent Layer\n",
        "    W_xh1 = weights[0].T\n",
        "    W_h1h1 = weights[1].T\n",
        "    b_h1 = np.expand_dims(weights[2], axis=1)\n",
        "\n",
        "    # Second Recurrent Layer\n",
        "    W_h1h2 = weights[3].T\n",
        "    W_h2h2 = weights[4].T\n",
        "    b_h2 = np.expand_dims(weights[5], axis=1)\n",
        "\n",
        "    # Linear (dense) layer\n",
        "    W_h2y = weights[6].T\n",
        "    b_y = np.expand_dims(weights[7], axis=1)\n",
        "\n",
        "    # Initiate the hidden states\n",
        "    h1 = np.zeros((W_h1h1.shape[0], 1))\n",
        "    h2 = np.zeros((W_h2h2.shape[0], 1))\n",
        "\n",
        "    # -----------------------------------------------\n",
        "\n",
        "    # Your code starts here\n",
        "    # output_sentence = \"\"\n",
        "\n",
        "    vocab_size = W_h2y.shape[0]\n",
        "\n",
        "    # ----- helper functions (numpy only) -----\n",
        "    def one_hot(char_idx):\n",
        "        x = np.zeros((vocab_size, 1))\n",
        "        x[char_idx, 0] = 1.0\n",
        "        return x\n",
        "\n",
        "    def softmax(z):\n",
        "        z = z - np.max(z)        # numerical stability\n",
        "        e = np.exp(z)\n",
        "        return e / np.sum(e)\n",
        "\n",
        "    # If prompt is empty, start from a space character (or any valid char)\n",
        "    if len(prompt) == 0:\n",
        "        # choose a deterministic start, here: first char in vocabulary\n",
        "        first_idx = 0\n",
        "        prompt = indices_to_char[first_idx]\n",
        "\n",
        "    output_sentence = prompt\n",
        "\n",
        "    # ---------- 1) Run the RNN over the prompt ----------\n",
        "    # We update h1, h2 sequentially for every character in the prompt\n",
        "    for c in prompt:\n",
        "        idx = char_to_indices[c]\n",
        "        x_t = one_hot(idx)\n",
        "\n",
        "        # First recurrent layer\n",
        "        h1 = np.tanh(W_xh1 @ x_t + W_h1h1 @ h1 + b_h1)\n",
        "\n",
        "        # Second recurrent layer\n",
        "        h2 = np.tanh(W_h1h2 @ h1 + W_h2h2 @ h2 + b_h2)\n",
        "\n",
        "        # Output layer (logits and softmax)\n",
        "        y_logits = W_h2y @ h2 + b_y\n",
        "        y_prob = softmax(y_logits)   # (V, 1)\n",
        "\n",
        "    # At this point, h1, h2, y_prob encode the information after the prompt\n",
        "\n",
        "    # ---------- 2) Generate new characters ----------\n",
        "    num_to_generate = max(0, N - len(prompt))\n",
        "\n",
        "    for _ in range(num_to_generate):\n",
        "        # Sample next character index according to the predicted distribution\n",
        "        next_idx = np.random.choice(np.arange(vocab_size), p=y_prob.ravel())\n",
        "        next_char = indices_to_char[next_idx]\n",
        "        output_sentence += next_char\n",
        "\n",
        "        # Use the generated character as input for the next time step\n",
        "        x_t = one_hot(next_idx)\n",
        "\n",
        "        h1 = np.tanh(W_xh1 @ x_t + W_h1h1 @ h1 + b_h1)\n",
        "        h2 = np.tanh(W_h1h2 @ h1 + W_h2h2 @ h2 + b_h2)\n",
        "\n",
        "        y_logits = W_h2y @ h2 + b_y\n",
        "        y_prob = softmax(y_logits)\n",
        "\n",
        "\n",
        "    return output_sentence"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cfc7420f",
      "metadata": {
        "id": "cfc7420f"
      },
      "source": [
        "Test out your function by running the following code cell. Use it as a sanity check that your code is working. The generated text should not be perfect English, but at least you should be able to recognize some words."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "id": "12458ad0",
      "metadata": {
        "id": "12458ad0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "853e9cb7-be45-4635-9d86-36435cab617f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mrs. oliver looked at herself in the glass. she gave a brief, sideways looked--inteed to have heapsides.\" \"oh, yes, ther master the couldn't like about hame or at. couser the maysed on somemaly. theremaress of uthiph, you speried,\" she well at my tpempors of the olyon, yes. i shi'n same but i sust changiin thearthanch togany mrs. oliver heard it. seal noh someone that's not soep in effencal a amy?\" \"and was a like with differe time theme, been yes andoristila, you know. whit will knlw she riges.. mand outorst i thives somother hiss. they mastortsceld her longien.\" \"i know i varwert. he were come forgeatelothar, it have amothere elgarrelast, hay dilperesting you but whother could girlows her father. one mrsturdancresser right be fon erpenary.\" \"it's erinclitt kithcreilly welled. and lrtall, conled. she had she wene?\" \"i ks your becallugh did mrs. oliver, i shoul heard you most aland or to wime swents.\" \"ald destle.\" \"a cambong. they't know the likel her her?\" \"went and chill.\" \"i's not \n"
          ]
        }
      ],
      "source": [
        "print(sample_text_RNN(weights_RNN,\n",
        "                      'mrs. oliver looked at herself in the glass. she gave a brief, sideways look',\n",
        "                      1000))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(sample_text_RNN(weights_RNN,\n",
        "                      'mrs. oliver looked at herself in the glass. she gave a brief, sideways look',\n",
        "                      1000))"
      ],
      "metadata": {
        "id": "sz_aczr_ufnh",
        "outputId": "a70ee2a4-5522-4fbd-8785-f69b779affd7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "sz_aczr_ufnh",
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mrs. oliver looked at herself in the glass. she gave a brief, sideways looked to as hes, i've not at parting sundy.\" \"it is it me to day welk but ord rimver. by not seemnlayty. \"and mrso, i think they'd tagn chacceraptsbyeally.\" \"oh, 'he howadnowang about and thes. ohe disiound and thear elea\"bed,\" said marpen. there were she'cluthere was a sting she want. well. thes whow we what you meand opester of spasing welly, the have happen he fatted corable is there rathers, i have have been she con,, in find year?\" \"for afor at anything it's satee this and then is antw she was sen in the faind any case marryok for then id to nothing all may dalffechhos,\" shell ceap. but of course it.\" \"oh, yes, it me taged.\" \"oh, you dong haspence?\" \"wed yeo forgeresuclly.\" \"reall, soc--ithersednathit's all thas,\" \"but what, itted way, that she can seng the falle i remembers eice biwn an wonlead she was was.\" \"and happoneysing hels bite him. held do fur asked seemelt,\" said mrs. oliver. for hos, they madaible?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(sample_text_RNN(weights_RNN,\n",
        "                      'it was a tragedy. as monsieur poirot here has said, it was a real tragedy of two people who loved each other.',\n",
        "                      1000))"
      ],
      "metadata": {
        "id": "wOtqJ8Zduh3A",
        "outputId": "72e5be9c-732e-440e-8d39-3cb3c38deb37",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "wOtqJ8Zduh3A",
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "it was a tragedy. as monsieur poirot here has said, it was a real tragedy of two people who loved each other. \"and they can helps--will, i said specalferted foch by the taink not it?\" \"becllinhich had about they well. i pust rempersed anythen. there was anlighing.\" \"she's wad'm not at pasied was ansperong.\" \"i wantick a winf rimebeft, or the came well niges. one doele and dable, suctiraccolagher, i dis jutt asont age lave very dence see to know. i belanr there was not stapteen, what was beenty they tight. she like who another,\" said i'd on the ilwell, you meap chapsefferlatiseling and mate at the filled to thers, a curmre. something at meet.-\"seally.\" \"yes, that myster surdenly un the right in there that wift all time wilat. they or beept a fisquile. she motest at oft?\" \"anythan when i mean, you know condecbodaid, if lout the cwarroun nate after and sending,\" said save been twren. you know, you dif wess, that elephant thought, \"matis to wants onceres really.\" \"nol no.\" \"they have had a\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c950eae8",
      "metadata": {
        "id": "c950eae8"
      },
      "source": [
        "### Problem 1.3: Generating Text with the GRU"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1dd93ee7",
      "metadata": {
        "id": "1dd93ee7"
      },
      "source": [
        "The code below loads in a pretrained GRU model. The model is set up exactly like in the lecture slides (with sigmoid activation layers for the gates and tanh activation layers in the recurrent layer). The model is trained for only 10 epochs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "id": "82291909",
      "metadata": {
        "id": "82291909"
      },
      "outputs": [],
      "source": [
        "# load json and create model\n",
        "json_file = open(data_path + 'gru_problem/GRU_model.json', 'r')\n",
        "loaded_model_json_str = json_file.read()\n",
        "json_file.close()\n",
        "\n",
        "GRU_model = model_from_json(loaded_model_json_str, custom_objects={'Sequential': Sequential, 'GRU': GRU})\n",
        "GRU_model.load_weights(data_path + \"gru_problem/GRU_model.h5\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "id": "738974b5",
      "metadata": {
        "id": "738974b5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 226
        },
        "outputId": "5bad1018-da5c-4c25-857f-09a0baf67788"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ gru (\u001b[38;5;33mGRU\u001b[0m)                       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │       \u001b[38;5;34m857,088\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m44\u001b[0m)             │        \u001b[38;5;34m22,572\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ activation (\u001b[38;5;33mActivation\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m44\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ gru (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GRU</span>)                       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">857,088</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">44</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">22,572</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ activation (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">44</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m879,660\u001b[0m (3.36 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">879,660</span> (3.36 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m879,660\u001b[0m (3.36 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">879,660</span> (3.36 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# load in the weights and show summary\n",
        "weights_GRU = GRU_model.get_weights()\n",
        "GRU_model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cad5b34f",
      "metadata": {
        "id": "cad5b34f"
      },
      "source": [
        "Finish the following function that uses a GRU architecture to generate text, given the weights of the GRU model, a text prompt, and the number of characters to return. The function should be completed by **only using numpy functions**. Use your knowledge of how every weight plays its role in the GRU architecture. Do not worry about the weight extraction part, this is already provided for you. The hidden state $h$ is initialized to a vector of zeros.\n",
        "\n",
        "The embedding of each character has to be done by a one-hot encoding, where you will need the dictionaries defined in the introduction to go from a character to an index position.\n",
        "\n",
        "Note: a slightly different version of the GRU is used, where the candidate state $c_t$ is calculated as:\n",
        "\n",
        "$$\n",
        "c_t = \\text{tanh} \\left(W_{hx} x_t \\ + \\ \\Gamma_t^r \\odot (W_{hh} h_{t-1}) \\ + \\ b_h \\right)\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "id": "259c1d40",
      "metadata": {
        "id": "259c1d40"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Helper function\n",
        "def sigmoid(z):\n",
        "    return 1 / (1 + np.exp(-z))\n",
        "\n",
        "def sample_text_GRU(weights, prompt, N):\n",
        "    '''\n",
        "    Uses a pretrained GRU to generate text, starting from a prompt,\n",
        "    only using the weights and numpy commands\n",
        "    Parameters:\n",
        "        weights (list): Weights of the pretrained GRU model\n",
        "        prompt (string): Start of generated sentence\n",
        "        N (int): Total length of output sentence\n",
        "    Returns:\n",
        "        output_sentence (string): Text generated by GRU\n",
        "    '''\n",
        "    # Extracting weights and biases\n",
        "    # Dimensions of matrices are same format as lecture slides\n",
        "\n",
        "    # GRU Layer\n",
        "\n",
        "    W_ux, W_rx, W_hx = np.split(weights[0].T, 3, axis=0)\n",
        "    W_uh, W_rh, W_hh = np.split(weights[1].T, 3, axis=0)\n",
        "\n",
        "    bias = np.sum(weights[2], axis=0)\n",
        "    b_u, b_r, b_h = np.split(np.expand_dims(bias, axis=1), 3)\n",
        "\n",
        "    # Linear (dense) layer\n",
        "    W_y = weights[3].T\n",
        "    b_y = np.expand_dims(weights[4], axis=1)\n",
        "\n",
        "    # Initiate hidden state\n",
        "    h = np.zeros((W_hh.shape[0], 1))\n",
        "\n",
        "    V = W_ux.shape[1]\n",
        "\n",
        "    def one_hot(idx):\n",
        "        x = np.zeros((V, 1))\n",
        "        x[idx, 0] = 1.0\n",
        "        return x\n",
        "\n",
        "    def softmax(z):\n",
        "        z = z - np.max(z)\n",
        "        ez = np.exp(z)\n",
        "        return ez / np.sum(ez)\n",
        "\n",
        "    output_sentence = prompt if prompt is not None else \"\"\n",
        "    if output_sentence == \"\":\n",
        "        prev_idx = char_to_indices.get(\" \", 0)\n",
        "    else:\n",
        "        for ch in output_sentence:\n",
        "            idx = char_to_indices.get(ch, char_to_indices.get(\" \", 0))\n",
        "            x = one_hot(idx)\n",
        "\n",
        "            # gates\n",
        "            u = sigmoid(W_ux @ x + W_uh @ h + b_u)  # update gate\n",
        "            r = sigmoid(W_rx @ x + W_rh @ h + b_r)  # reset gate\n",
        "\n",
        "            # candidate state\n",
        "            hh = W_hh @ h\n",
        "            c = np.tanh(W_hx @ x + (r * hh) + b_h)\n",
        "\n",
        "            # GRU update\n",
        "            h = u * h + (1 - u) * c\n",
        "\n",
        "        prev_idx = char_to_indices.get(output_sentence[-1], char_to_indices.get(\" \", 0))\n",
        "\n",
        "\n",
        "    to_generate = max(0, N - len(output_sentence))\n",
        "    for _ in range(to_generate):\n",
        "        x = one_hot(prev_idx)\n",
        "\n",
        "        # GRU forward\n",
        "        u = sigmoid(W_ux @ x + W_uh @ h + b_u)\n",
        "        r = sigmoid(W_rx @ x + W_rh @ h + b_r)\n",
        "\n",
        "        hh = W_hh @ h\n",
        "        c = np.tanh(W_hx @ x + (r * hh) + b_h)\n",
        "\n",
        "        h = u * h + (1 - u) * c\n",
        "\n",
        "        # output distribution over next char\n",
        "        logits = W_y @ h + b_y\n",
        "        probs = softmax(logits).reshape(-1)\n",
        "\n",
        "        next_idx = np.random.choice(np.arange(V), p=probs)\n",
        "        next_ch = indices_to_char[int(next_idx)]\n",
        "\n",
        "        output_sentence += next_ch\n",
        "        prev_idx = int(next_idx)\n",
        "\n",
        "    return output_sentence\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "117fdc8f",
      "metadata": {
        "id": "117fdc8f"
      },
      "source": [
        "Test out your function by running the following code cell. Use it as a sanity check that your code is working. The generated text should not be perfect English, but at least you should be able to recognize some words."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "id": "3fc14584",
      "metadata": {
        "id": "3fc14584",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bbac3de6-239d-4ae6-ec07-3b6ec2d8b47c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mrs. oliver looked at herself in the glass. she gave a brief, sideways look a gerse. atithe toblots of wht is a you ahe arrea the soll celia to desoriate to see the hear stouils. the tomere the rese the look of the atty and it. the moget premigede lish other in the reserter in a little about it and that i may bat being a little mole to samely that withisain meriny resomething bat andelly asordentted the deal with the the table, to a sortion stien..\" \"oo. steen treet to that that she tood take it fact in the it his signering. sometimeses of the light to have thes or a willp engrowhor it i can \" doush hame to enely dido. it was or yourthing pasticu a row thought that said this sting seith, and a teresting point. i mean, i nowit mast ce is. the well that to i happened about she had to her at a little either und interested about them there. the exonter ty show we know atway, and some was she stood to epprivily she faiter to ase propost to look a nirll it interesting. the looked atate. i th\n"
          ]
        }
      ],
      "source": [
        "print(sample_text_GRU(weights_GRU,\n",
        "                      'mrs. oliver looked at herself in the glass. she gave a brief, sideways look',\n",
        "                      1000))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(sample_text_GRU(weights_GRU,\n",
        "                      'mrs. oliver looked at herself in the glass. she gave a brief, sideways look',\n",
        "                      1000))"
      ],
      "metadata": {
        "id": "sJGCorB0uQl1",
        "outputId": "01b4c59b-24d1-4884-acee-3560ddfe723d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "sJGCorB0uQl1",
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mrs. oliver looked at herself in the glass. she gave a brief, sideways look against i have or turted out amoties. suseeneral or anything it the time i sid her mother is a come to looved twaptich. i think it is one of the tire so more ates rople so nowownem she his the dos more time. i think if when a telemben you alrot them and wey, that it inti as tho elter the worly about same of loo, i may same that sometim sometimes she thought that howe you ago aust it will be wift. quite a lott to the same to leath now, i shonly there ar sust a kislo it. that i was stilin she went or listed trought that i can de a young looked. it.\" \"that well a little my only her rome after that people be nom?\" \"not quite about it. i think she wort in it on the the tell a metter too in it or some not sugh a his all had been abrood or looker at lide the always are or it about it. peoyle lift everything after that swo do that i mean. and the sistery were beoree bedievedything like to be a lioke everything.. i reme\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(sample_text_GRU(weights_GRU,\n",
        "                      'it was a tragedy. as monsieur poirot here has said, it was a real tragedy of two people who loved each other.',\n",
        "                      1000))"
      ],
      "metadata": {
        "id": "ZUdqFMTquWiR",
        "outputId": "3ac7004e-693a-4d76-fa29-e03098bc38e1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "ZUdqFMTquWiR",
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "it was a tragedy. as monsieur poirot here has said, it was a real tragedy of two people who loved each other. beathour things.\" \"i don't really y a meat, though you to eatter that i am so me, we talked at enilien in the hears her fee came to a seemor poirot all reessininan mertice bord like phesently thought me. she was the etwo sixcestand like it has a sprrien. and like treer to to to look a time into the ravenscroft had some interesten before so. a niculs to an any lift the you?\" \"a lasterers of the renerbly right. i so really leashing. it it. i tell in the hears to all the one to the atably time she's hear agoint to show entalw afferty it will the vill she too looked at going in the elie or hee sort of dusbat was the same temester to ase youn i am it for she ragher thing that had. the geremal atricish the strave he heritiniting and girly a litter of the lent,\" said mrs. oliver, that ithis all, but the alwers knows there air rapenscroft. the frienss on the trese and to doet sixher sh\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6a35ba68",
      "metadata": {
        "id": "6a35ba68"
      },
      "source": [
        "### Problem 1.4: Can Elephants Remember Better?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "09cd85c4",
      "metadata": {
        "id": "09cd85c4"
      },
      "source": [
        "Perplexity is a measure to quantify how \"good\" a language model $M$ is, based on a test (or validation) set. The perplexity on a sequence $s$ of characters $a_i$ of size $N$ is defined as:\n",
        "\n",
        "$$\n",
        "\\text{Perplexity}(M) = M(s)^{(-1/N)} = \\left\\{p(a_1, \\ldots, a_N)\\right\\}^{(-1/N)} = \\left\\{p(a_1) \\ p(a_2|a_1) \\ \\ldots \\ p(a_N|a_1, \\ldots, a_{N-1})\\right\\}^{(-1/N)}\n",
        "$$\n",
        "\n",
        "> The intuition behind this metric is that, if a model assigns a high probability to a test set, it is not surprised to see it (not perplexed by it), which means the model $M$ has a good understanding of how the language works. Hence, a good model has, in theory, a lower perplexity. The exponent $(-1/N)$ in the formula is just a normalizing strategy (geometric average), because adding more characters to a test set would otherwise introduce more uncertainty (i.e. larger test sets would have lower probability). So by introducing the geometric average, we have a metric that is independent of the size of the test set.\n",
        "\n",
        "When calculating the perplexity, it is important to know that taking the product of a bunch of probabilities will most likely lead to a zero value by the computer. To prevent this, make use of a log-transformation:\n",
        "\n",
        "$$\n",
        "\\text{Log-Perplexity}(M) = -\\frac{1}{N} log\\left\\{p(a_1, \\ldots, a_N)\\right\\} = -\\frac{1}{N} \\left\\{log \\ p(a_1) + \\ log \\ p(a_2|a_1) + \\ \\ldots \\ + log \\  p(a_N|a_1, \\ldots, a_{N-1})\\right\\}\n",
        "$$\n",
        "\n",
        "Don't forget to go back to the normal perplexity after this transformation."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "709c6d4e",
      "metadata": {
        "id": "709c6d4e"
      },
      "source": [
        "1. Before calculating the perplexity of a test sequence, start with comparing the outputs of 2.2 and 2.3. Do you see any differences in the generated text of the Vanilla RNN model and the GRU model? Rerun your functions a couple of times (because of stochasticity) and use different prompts. Briefly discuss why you would expect (or not expect) certain differences."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3ccf8083",
      "metadata": {
        "id": "3ccf8083"
      },
      "outputs": [],
      "source": [
        "# Your markdown here"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, clear differences can be observed between the Vanilla RNN and the GRU in the generated text.\n",
        "\n",
        "- The **Vanilla RNN** outputs tend to deteriorate more quickly into misspelled words and seemingly random character sequences. As the generated sequence becomes longer, coherence is often lost and the text becomes difficult to interpret.\n",
        "- The **GRU**, while still imperfect, generally maintains **basic sentence structure, more recognizable words, and a more consistent narrative style** over longer spans of generated text.\n",
        "\n",
        "These differences are expected from a modeling perspective. Vanilla RNNs suffer from **vanishing and exploding gradient problems**, which makes it difficult for them to capture long-range dependencies. In contrast, GRUs use **gating mechanisms** that regulate how information is stored and forgotten, allowing them to better preserve relevant context over time.\n",
        "\n",
        "Because the generation process is **stochastic**, the exact outputs vary across runs and across different prompts. Additionally, the two models were trained with **different numbers of epochs**, which can further contribute to variability in their performance.\n",
        "\n"
      ],
      "metadata": {
        "id": "zjaqkZGNzsQe"
      },
      "id": "zjaqkZGNzsQe"
    },
    {
      "cell_type": "markdown",
      "id": "5c9a5bf5",
      "metadata": {
        "id": "5c9a5bf5"
      },
      "source": [
        "2. Calculate the perplexity of each language model by using test_text, an unseen extract of the book. Choose the prompt as the first $m$ letters of the test set, where $m$ is a parameter that you can choose yourself. You should be able to reuse the majority of your previous code in this calculation. Discuss your results at the end."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "id": "e0c2a390",
      "metadata": {
        "id": "e0c2a390",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "00df744b-a8da-41c3-8e67-417037495806"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prompt length m = 100, evaluated chars = 7109\n",
            "Vanilla RNN: perplexity = 5.6660, avg NLL = 1.7345\n",
            "GRU:        perplexity = 3.9406, avg NLL = 1.3713\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "def softmax(z):\n",
        "    z = z - np.max(z)\n",
        "    ez = np.exp(z)\n",
        "    return ez / np.sum(ez)\n",
        "\n",
        "def sigmoid(z):\n",
        "    return 1 / (1 + np.exp(-z))\n",
        "\n",
        "# Vanilla RNN perplexity\n",
        "def perplexity_RNN(weights, text, m=100, eps=1e-12):\n",
        "    # Extract weights\n",
        "    W_xh1 = weights[0].T\n",
        "    W_h1h1 = weights[1].T\n",
        "    b_h1 = np.expand_dims(weights[2], axis=1)\n",
        "\n",
        "    W_h1h2 = weights[3].T\n",
        "    W_h2h2 = weights[4].T\n",
        "    b_h2 = np.expand_dims(weights[5], axis=1)\n",
        "\n",
        "    W_h2y = weights[6].T\n",
        "    b_y = np.expand_dims(weights[7], axis=1)\n",
        "\n",
        "    V = W_xh1.shape[1]\n",
        "    H1 = W_h1h1.shape[0]\n",
        "    H2 = W_h2h2.shape[0]\n",
        "\n",
        "    def one_hot(idx):\n",
        "        x = np.zeros((V, 1))\n",
        "        x[idx, 0] = 1.0\n",
        "        return x\n",
        "\n",
        "    L = len(text)\n",
        "    if m < 1 or m >= L:\n",
        "        raise ValueError(\"Choose m such that 1 <= m < len(test_text).\")\n",
        "\n",
        "    # initial hidden states\n",
        "    h1 = np.zeros((H1, 1))\n",
        "    h2 = np.zeros((H2, 1))\n",
        "\n",
        "    prompt = text[:m]\n",
        "    for ch in prompt[:-1]:\n",
        "        idx = char_to_indices[ch]\n",
        "        x = one_hot(idx)\n",
        "        h1 = np.tanh(W_xh1 @ x + W_h1h1 @ h1 + b_h1)\n",
        "        h2 = np.tanh(W_h1h2 @ h1 + W_h2h2 @ h2 + b_h2)\n",
        "\n",
        "    prev_idx = char_to_indices[prompt[-1]]\n",
        "\n",
        "    # score remaining characters\n",
        "    loglik = 0.0\n",
        "    N_eval = L - m\n",
        "\n",
        "    for i in range(m, L):\n",
        "        x = one_hot(prev_idx)\n",
        "        h1 = np.tanh(W_xh1 @ x + W_h1h1 @ h1 + b_h1)\n",
        "        h2 = np.tanh(W_h1h2 @ h1 + W_h2h2 @ h2 + b_h2)\n",
        "\n",
        "        logits = W_h2y @ h2 + b_y\n",
        "        probs = softmax(logits).reshape(-1)\n",
        "\n",
        "        true_idx = char_to_indices[text[i]]\n",
        "        loglik += np.log(probs[true_idx] + eps)\n",
        "\n",
        "        prev_idx = true_idx\n",
        "\n",
        "    avg_nll = -loglik / N_eval\n",
        "    perp = float(np.exp(avg_nll))\n",
        "    return perp, float(avg_nll), N_eval\n",
        "\n",
        "\n",
        "# GRU perplexity\n",
        "def perplexity_GRU(weights, text, m=100, eps=1e-12):\n",
        "    # GRU weights\n",
        "    W_ux, W_rx, W_hx = np.split(weights[0].T, 3, axis=0)\n",
        "    W_uh, W_rh, W_hh = np.split(weights[1].T, 3, axis=0)\n",
        "\n",
        "    bias = np.sum(weights[2], axis=0)\n",
        "    b_u, b_r, b_h = np.split(np.expand_dims(bias, axis=1), 3)\n",
        "\n",
        "    W_y = weights[3].T\n",
        "    b_y = np.expand_dims(weights[4], axis=1)\n",
        "\n",
        "    V = W_ux.shape[1]\n",
        "    H = W_hh.shape[0]\n",
        "\n",
        "    def one_hot(idx):\n",
        "        x = np.zeros((V, 1))\n",
        "        x[idx, 0] = 1.0\n",
        "        return x\n",
        "\n",
        "    L = len(text)\n",
        "    if m < 1 or m >= L:\n",
        "        raise ValueError(\"Choose m such that 1 <= m < len(test_text).\")\n",
        "\n",
        "    h = np.zeros((H, 1))\n",
        "\n",
        "    prompt = text[:m]\n",
        "    for ch in prompt[:-1]:\n",
        "        idx = char_to_indices[ch]\n",
        "        x = one_hot(idx)\n",
        "\n",
        "        u = sigmoid(W_ux @ x + W_uh @ h + b_u)\n",
        "        r = sigmoid(W_rx @ x + W_rh @ h + b_r)\n",
        "\n",
        "        # candidate\n",
        "        c = np.tanh(W_hx @ x + (r * (W_hh @ h)) + b_h)\n",
        "\n",
        "        # update\n",
        "        h = u * h + (1 - u) * c\n",
        "\n",
        "    prev_idx = char_to_indices[prompt[-1]]\n",
        "\n",
        "    loglik = 0.0\n",
        "    N_eval = L - m\n",
        "\n",
        "    for i in range(m, L):\n",
        "        x = one_hot(prev_idx)\n",
        "\n",
        "        u = sigmoid(W_ux @ x + W_uh @ h + b_u)\n",
        "        r = sigmoid(W_rx @ x + W_rh @ h + b_r)\n",
        "        c = np.tanh(W_hx @ x + (r * (W_hh @ h)) + b_h)\n",
        "        h = u * h + (1 - u) * c\n",
        "\n",
        "        logits = W_y @ h + b_y\n",
        "        probs = softmax(logits).reshape(-1)\n",
        "\n",
        "        true_idx = char_to_indices[text[i]]\n",
        "        loglik += np.log(probs[true_idx] + eps)\n",
        "\n",
        "        prev_idx = true_idx\n",
        "\n",
        "    avg_nll = -loglik / N_eval\n",
        "    perp = float(np.exp(avg_nll))\n",
        "    return perp, float(avg_nll), N_eval\n",
        "\n",
        "# run\n",
        "m = 100\n",
        "rnn_perp, rnn_nll, n_eval = perplexity_RNN(weights_RNN, test_text, m=m)\n",
        "gru_perp, gru_nll, _      = perplexity_GRU(weights_GRU, test_text, m=m)\n",
        "\n",
        "print(f\"Prompt length m = {m}, evaluated chars = {n_eval}\")\n",
        "print(f\"Vanilla RNN: perplexity = {rnn_perp:.4f}, avg NLL = {rnn_nll:.4f}\")\n",
        "print(f\"GRU:        perplexity = {gru_perp:.4f}, avg NLL = {gru_nll:.4f}\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using the first $m = 100$ characters of the test set as the prompt and evaluating on the remaining $7109$ characters, we obtain the following results:\n",
        "\n",
        "- **Vanilla RNN**  \n",
        "  Perplexity = **5.6660**, average NLL = **1.7345**\n",
        "\n",
        "- **GRU**  \n",
        "  Perplexity = **3.9406**, average NLL = **1.3713**\n",
        "\n",
        "Since **lower perplexity and lower average negative log-likelihood (NLL) indicate better language modeling performance**, the GRU again clearly outperforms the vanilla RNN on the unseen test text. The smaller perplexity value means that the GRU assigns higher probability to the true next characters and is therefore less surprised by the test data.\n",
        "\n",
        "These results are consistent with our qualitative observations from text generation. The **GRU benefits from its gating mechanisms**, which allow it to better preserve long-range dependencies and regulate information flow over time. In contrast, the **vanilla RNN is more susceptible to vanishing gradients** and therefore struggles to model long-term structure as effectively. As a result, the GRU achieves both lower NLL and lower perplexity.\n",
        "\n"
      ],
      "metadata": {
        "id": "ne-KdRaO3jNR"
      },
      "id": "ne-KdRaO3jNR"
    },
    {
      "cell_type": "markdown",
      "id": "9ab728c9",
      "metadata": {
        "id": "9ab728c9"
      },
      "source": [
        "3. As seen in part 2 and 3 of this problem, the text generation is not perfect. Describe some possible model improvements that could make the quality of the generated text better."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8df914d2",
      "metadata": {
        "id": "8df914d2"
      },
      "outputs": [],
      "source": [
        "# Your markdown here"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "The text generated by both the vanilla RNN and the GRU is still far from perfect English. Several improvements could be made to enhance the quality of the generated text:\n",
        "\n",
        "1. **Train for more epochs and use more data.**  \n",
        "   The models in this assignment are trained on a limited corpus for a relatively small number of epochs (especially the GRU). Training longer and with more data would help the model better capture long-range dependencies and rare patterns.\n",
        "\n",
        "2. **Use deeper and more expressive architectures.**  \n",
        "   We only use shallow recurrent networks (two-layer RNN and single-layer GRU). Deeper GRU or LSTM models can better model complex temporal dependencies and typically achieve lower perplexity in language modeling tasks.\n",
        "\n",
        "3. **Change the tokenization strategy.**  \n",
        "   Character-based models are good at capturing spelling patterns but struggle with long-range syntax and semantics. Using subword-level or word-level tokens would allow the model to operate on more meaningful linguistic units and improve grammatical consistency.\n",
        "\n",
        "4. **Apply regularization and better optimization.**  \n",
        "   Techniques such as dropout, layer normalization, or improved learning-rate schedules could help stabilize training and prevent overfitting, leading to better generalization on unseen text.\n",
        "\n",
        "5. **Use improved sampling strategies.**  \n",
        "   Instead of directly sampling from the full softmax distribution, techniques such as temperature scaling, top-\\(k\\) sampling, or nucleus (top-\\(p\\)) sampling could reduce extremely low-probability characters and produce more coherent text.\n"
      ],
      "metadata": {
        "id": "nundTnH43fCY"
      },
      "id": "nundTnH43fCY"
    },
    {
      "cell_type": "markdown",
      "id": "b13b7fe6",
      "metadata": {
        "id": "b13b7fe6"
      },
      "source": [
        "## Problem 2: Be the Bard (35 points)\n",
        "\n",
        "<img src=\"https://upload.wikimedia.org/wikipedia/commons/a/a2/Shakespeare.jpg\" alt=\"William\" width=\"200\" style=\"float:left; padding:15px\"/>\n",
        "\n",
        "Transformer models are the current state of the art in many sequence modeling tasks, and the Transformer architecture underlies most Large Language Models (LLMs), including ChatGPT, Llama, Mistral, etc.\n",
        "\n",
        "In this problem, we will implement a Transformer language model from scratch in `numpy`. You will be provided with the weights of a small, slightly simplified Transformer language model that we trained on the works of Shakespeare. We will walk through implementing each component of the Transformer architecture and ultimately assemble this into a language model that can generate some text in the style of the Bard."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "8e4a7940",
      "metadata": {
        "id": "8e4a7940"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pickle\n",
        "#!pip install tiktoken\n",
        "import tiktoken\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "28091130",
      "metadata": {
        "id": "28091130"
      },
      "outputs": [],
      "source": [
        "d_model = 512\n",
        "n_layers = 4\n",
        "n_heads = 8\n",
        "d_ff = 4 * d_model\n",
        "vocab_size = 1024\n",
        "block_size = 128\n",
        "\n",
        "model_name = f'BardGPT_weights_D{d_model}L{n_layers}H{n_heads}.npy'\n",
        "transformer_model_weights = np.load(data_path + '/transformer_problem/' + model_name, allow_pickle=True).item()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "2d91252f",
      "metadata": {
        "id": "2d91252f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a6e999c3-d3d9-475d-a669-25452c19ac08"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parameters:\n",
            "-----------\n",
            "word_embedding.weight: (1024, 512) [float32]\n",
            "layers.0.attention.wq.weight: (512, 512) [float32]\n",
            "layers.0.attention.wk.weight: (512, 512) [float32]\n",
            "layers.0.attention.wv.weight: (512, 512) [float32]\n",
            "layers.0.attention.wo.weight: (512, 512) [float32]\n",
            "layers.0.attn_norm.weight: (512,) [float32]\n",
            "layers.0.attn_norm.bias: (512,) [float32]\n",
            "layers.0.feed_forward.0.weight: (2048, 512) [float32]\n",
            "layers.0.feed_forward.2.weight: (512, 2048) [float32]\n",
            "layers.0.ff_norm.weight: (512,) [float32]\n",
            "layers.0.ff_norm.bias: (512,) [float32]\n",
            "layers.1.attention.wq.weight: (512, 512) [float32]\n",
            "layers.1.attention.wk.weight: (512, 512) [float32]\n",
            "layers.1.attention.wv.weight: (512, 512) [float32]\n",
            "layers.1.attention.wo.weight: (512, 512) [float32]\n",
            "layers.1.attn_norm.weight: (512,) [float32]\n",
            "layers.1.attn_norm.bias: (512,) [float32]\n",
            "layers.1.feed_forward.0.weight: (2048, 512) [float32]\n",
            "layers.1.feed_forward.2.weight: (512, 2048) [float32]\n",
            "layers.1.ff_norm.weight: (512,) [float32]\n",
            "layers.1.ff_norm.bias: (512,) [float32]\n",
            "layers.2.attention.wq.weight: (512, 512) [float32]\n",
            "layers.2.attention.wk.weight: (512, 512) [float32]\n",
            "layers.2.attention.wv.weight: (512, 512) [float32]\n",
            "layers.2.attention.wo.weight: (512, 512) [float32]\n",
            "layers.2.attn_norm.weight: (512,) [float32]\n",
            "layers.2.attn_norm.bias: (512,) [float32]\n",
            "layers.2.feed_forward.0.weight: (2048, 512) [float32]\n",
            "layers.2.feed_forward.2.weight: (512, 2048) [float32]\n",
            "layers.2.ff_norm.weight: (512,) [float32]\n",
            "layers.2.ff_norm.bias: (512,) [float32]\n",
            "layers.3.attention.wq.weight: (512, 512) [float32]\n",
            "layers.3.attention.wk.weight: (512, 512) [float32]\n",
            "layers.3.attention.wv.weight: (512, 512) [float32]\n",
            "layers.3.attention.wo.weight: (512, 512) [float32]\n",
            "layers.3.attn_norm.weight: (512,) [float32]\n",
            "layers.3.attn_norm.bias: (512,) [float32]\n",
            "layers.3.feed_forward.0.weight: (2048, 512) [float32]\n",
            "layers.3.feed_forward.2.weight: (512, 2048) [float32]\n",
            "layers.3.ff_norm.weight: (512,) [float32]\n",
            "layers.3.ff_norm.bias: (512,) [float32]\n",
            "fc_out.weight: (1024, 512) [float32]\n",
            "fc_out.bias: (1024,) [float32]\n"
          ]
        }
      ],
      "source": [
        "print('Parameters:')\n",
        "print('-----------' )\n",
        "for k, v in transformer_model_weights.items():\n",
        "    print(f'{k}: {v.shape} [{v.dtype}]')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "65e3edfe",
      "metadata": {
        "id": "65e3edfe"
      },
      "source": [
        "### Problem 2.1: Describe & Count the model parameters\n",
        "\n",
        "**Part (a):** Describe the role of the parameters of the model. What are the weights `wq.weight`, `wk.weight`, `wv.weight`, and `wo.weight`? What are the dimensions of these matrices? What about the role and dimensions of the feedforward weights `feed_forward.0.weight` and `feed_forward.2.weight`?\n",
        "\n",
        "You can refer to the descriptions below as well as lecture notes. Note, in particular, in the comments preceding Problem 2.4 that the attention matrices across heads are \"packed together\" when you read in the parameters in each layer.\n",
        "\n",
        "**Part (b):** Write an expression for the total number of parameters in the model, broken down by each component (Embedding Layer, Attention Layers, Feed Forward Layers, Normalization Layers, and final fully connected layer). Confirm that your answer matches the parameter count in the model weights given to you. Consult the list of parameters above and their shape to resolve any ambiguity regarding variations in Transformer architectures. Write down the expression in terms of: vocabulary size $V$, model dimension $D$, number of layers $L$, and feedforward expansion factor $F$."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ea271309",
      "metadata": {
        "id": "ea271309"
      },
      "source": [
        "[Your Markdown Here]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Problem 2.1 (a): Role and Dimensions of the Main Weights\n",
        "\n",
        "For this model, the hyperparameters are:\n",
        "$D = 512$, $L = 4$, $H = 8$, $F = 4$, so $FD = 2048$.\n",
        "\n",
        "**Attention weights (per layer)**  \n",
        "The four attention matrices project the hidden states into queries, keys, values, and then back to the model dimension:\n",
        "$$\n",
        "Q = XW_Q, \\quad K = XW_K, \\quad V = XW_V, \\quad \\text{Output} = \\text{Concat}(\\text{heads})W_O.\n",
        "$$\n",
        "Their dimensions for each layer $i=0,1,2,3$ are:\n",
        "- `layers.i.attention.wq.weight`: $(512,512)$  \n",
        "- `layers.i.attention.wk.weight`: $(512,512)$  \n",
        "- `layers.i.attention.wv.weight`: $(512,512)$  \n",
        "- `layers.i.attention.wo.weight`: $(512,512)$  \n",
        "\n",
        "Each head therefore has dimension\n",
        "$$\n",
        "d_h = \\frac{512}{8} = 64.\n",
        "$$\n",
        "\n",
        "**Feed-forward weights (per layer)**  \n",
        "The two-layer MLP is\n",
        "$$\n",
        "\\text{FFN}(x) = W_2 \\sigma(W_1 x),\n",
        "$$\n",
        "with dimensions:\n",
        "- `layers.i.feed_forward.0.weight`: $(2048,512)$  \n",
        "- `layers.i.feed_forward.2.weight`: $(512,2048)$  \n",
        "\n",
        "Here, $W_1$ expands the representation from $512$ to $2048$, and $W_2$ projects it back to $512$ so it can be added through the residual connection.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "BYgtJM18A3iF"
      },
      "id": "BYgtJM18A3iF"
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "### (b) Total Parameter Count\n",
        "\n",
        "Let:\n",
        "- Vocabulary size = $V$\n",
        "- Model dimension = $D$\n",
        "- Number of layers = $L$\n",
        "- Feed-forward expansion factor = $F$\n",
        "\n",
        "---\n",
        "\n",
        "**1. Embedding layer**\n",
        "$$\n",
        "\\#\\text{params}_{\\text{embed}} = V D\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "**2. Attention parameters (per layer)**  \n",
        "- 4 projection matrices: $4 D^2$  \n",
        "- LayerNorm scale and bias: $2D$\n",
        "\n",
        "$$\n",
        "\\#\\text{params}_{\\text{attn, per layer}} = 4 D^2 + 2D\n",
        "$$\n",
        "\n",
        "For all layers:\n",
        "$$\n",
        "\\#\\text{params}_{\\text{attn}} = L (4 D^2 + 2D)\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "**3. Feed-forward parameters (per layer)**  \n",
        "- Two linear maps: $2 F D^2$  \n",
        "- LayerNorm scale and bias: $2D$\n",
        "\n",
        "$$\n",
        "\\#\\text{params}_{\\text{ff, per layer}} = 2 F D^2 + 2D\n",
        "$$\n",
        "\n",
        "For all layers:\n",
        "$$\n",
        "\\#\\text{params}_{\\text{ff}} = L (2 F D^2 + 2D)\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "**4. Final output layer**\n",
        "- Weight: $V D$\n",
        "- Bias: $V$\n",
        "\n",
        "$$\n",
        "\\#\\text{params}_{\\text{fc}} = V D + V\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "### **Total Parameter Formula**\n",
        "\n",
        "$$\n",
        "\\#\\text{params}_{\\text{total}}\n",
        "= V D\n",
        "+ L (4 D^2 + 2D)\n",
        "+ L (2 F D^2 + 2D)\n",
        "+ (V D + V)\n",
        "$$\n",
        "\n",
        "Simplified:\n",
        "$$\n",
        "\\#\\text{params}_{\\text{total}}\n",
        "= 2 V D + L \\big[(4 + 2F) D^2 + 4D \\big] + V\n",
        "$$\n",
        "\n",
        "For $V = 1024,\\; D = 512,\\; L = 4,\\; F = 4$,\n",
        "- Total = 13,640,704"
      ],
      "metadata": {
        "id": "WIs48Atiw_Z6"
      },
      "id": "WIs48Atiw_Z6"
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "aba563fb",
      "metadata": {
        "id": "aba563fb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c0afd3bc-4135-4aae-833d-b9eef9c594b1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# of Parameters: 13,640,704\n"
          ]
        }
      ],
      "source": [
        "param_count = np.sum([np.prod(v.shape) for k,v in transformer_model_weights.items()])\n",
        "print(f\"# of Parameters: {param_count:,}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "80b1b0b8",
      "metadata": {
        "id": "80b1b0b8"
      },
      "source": [
        "### Tokenizer\n",
        "\n",
        "To process text with a neural network model, we need to first tokenize it in order to convert it to a numerical format that the model can understand and process. The tokenizer converts strings into sequences of integer tokens in a fixed vocabulary. There are different ways to do this. For this problem, we trained a custom [Byte-Pair Encoding](https://en.wikipedia.org/wiki/Byte_pair_encoding)  (BPE) tokenizer on Shakespeare text, setting the vocabulary size to 1024. The code below demonstrates how it works."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "f3856d12",
      "metadata": {
        "id": "f3856d12"
      },
      "outputs": [],
      "source": [
        "# load the BPE tokenizer\n",
        "with open(data_path + 'transformer_problem/bpe1024_enc_full.pkl', 'rb') as pickle_file:\n",
        "    enc = pickle.load(pickle_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "603324d0",
      "metadata": {
        "id": "603324d0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f4b7a7b0-8460-4dc7-bbea-93a6c497ee2a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original text:\n",
            "What's in a name? that which we call a rose\n",
            "By any other name would smell as sweet;\n",
            "\n",
            "Encoded:\n",
            "[462, 320, 307, 258, 813, 63, 323, 621, 331, 800, 258, 697, 305, 10, 889, 801, 845, 813, 504, 260, 109, 408, 366, 818, 59]\n",
            "\n",
            "Tokens: \n",
            "['What', \"'s\", ' in', ' a', ' name', '?', ' that', ' which', ' we', ' call', ' a', ' ro', 'se', '\\n', 'By', ' any', ' other', ' name', ' would', ' s', 'm', 'ell', ' as', ' sweet', ';']\n",
            "\n",
            "Decoded text:\n",
            "What's in a name? that which we call a rose\n",
            "By any other name would smell as sweet;\n"
          ]
        }
      ],
      "source": [
        "# text to tokenize\n",
        "text = \"\"\"What's in a name? that which we call a rose\n",
        "By any other name would smell as sweet;\"\"\"\n",
        "\n",
        "print('Original text:')\n",
        "print(text)\n",
        "encoded = enc.encode(text)\n",
        "print()\n",
        "\n",
        "print('Encoded:')\n",
        "print(encoded)\n",
        "print()\n",
        "\n",
        "print('Tokens: ')\n",
        "print([enc.decode([idx]) for idx in encoded])\n",
        "decoded = enc.decode(encoded)\n",
        "print()\n",
        "\n",
        "print('Decoded text:')\n",
        "print(decoded)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "af975db1",
      "metadata": {
        "id": "af975db1"
      },
      "source": [
        "### Problem 2.2: Token Embeddings\n",
        "\n",
        "After tokenization, we get a sequence of integers that represent the text to processed, with each integer index corresponding to a particular token in the vocabulary (e.g., a word or word-part). The first step in processing this text is to turn it into a vector representation. This is done via a learned embedding look-up table. For each token in the vocabulary $t \\in \\mathcal{V}$, we learn an embedding $E_t \\in \\reals^d$. A sequence of tokens $(t_1, ..., t_n)$ is transformed to a vector representation by mapping each token to its embedding $(E_{t_1}, ..., E_{t_n}) \\in \\reals^{n \\times d}$. This sequence of vectors is what the neural network model ultimately operates over."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "11b0b829",
      "metadata": {
        "id": "11b0b829"
      },
      "outputs": [],
      "source": [
        "def embed_tokens(tokens, params):\n",
        "    \"\"\"\n",
        "    Embed tokens using the input embeddings.\n",
        "\n",
        "    Args:\n",
        "        tokens (np.array): array of token indices, shape (n_tokens,)\n",
        "        params (dict): dictionary containing the model parameters\n",
        "    \"\"\"\n",
        "\n",
        "    # get needed parameters\n",
        "    embeddings = params['word_embedding.weight'] # shape (vocab_size, d_model)\n",
        "    # embeddings is a look up table for embeddings, with rows corresponding to token indices\n",
        "\n",
        "    # look up embeddings\n",
        "    # your code here\n",
        "    embedded_tokens = embeddings[tokens]  # shape (n_tokens, d_model)\n",
        "\n",
        "    return embedded_tokens\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "14fa3cf4",
      "metadata": {
        "id": "14fa3cf4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4fd879cf-7463-41d0-d37b-75b5b393edf0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 1.8202915 ,  0.49289942, -0.08474425, -1.4825039 ,  1.1505613 ],\n",
              "       [ 0.01529888,  0.4088627 , -1.3310498 ,  1.5467082 ,  0.7893555 ],\n",
              "       [ 1.2610923 , -0.06854534, -0.3776905 , -0.45263755,  1.1006896 ]],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "embed_tokens(np.array([1, 2, 3]), params=transformer_model_weights)[:3, :5]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c8416544",
      "metadata": {
        "id": "c8416544"
      },
      "source": [
        "Expected answer:\n",
        "\n",
        "```\n",
        "array([[ 1.8202915 ,  0.49289942, -0.08474425, -1.4825039 ,  1.1505613 ],\n",
        "       [ 0.01529888,  0.4088627 , -1.3310498 ,  1.5467082 ,  0.7893555 ],\n",
        "       [ 1.2610923 , -0.06854534, -0.3776905 , -0.45263755,  1.1006896 ]],\n",
        "      dtype=float32)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3db9b1c2",
      "metadata": {
        "id": "3db9b1c2"
      },
      "source": [
        "### Positional Encoding\n",
        "\n",
        "Transformer models are by-default permutation-equivariant. That is, they don't understand order or position. To make them understand positional information, we need to encode it directly in the token embeddings. One way to do this is to represent each possible position $i$ with its own position embedding ${PE}_i \\in \\mathbb{R}^{d}$, and to simpthe positional embedding representing the position of the token.\n",
        "\n",
        "In the original Transformer paper, the authors propose a particular choice for ${PE}_i \\in \\reals^{d}$ based on sines and cosines with frequencies depending on the position $i$. Since the original proposal, many follow-up works proposed different positional encoding methods aiming to improve performance and length-generalization. In this problem, we'll use the sinusoidal positional encodings of the original Transformer paper.\n",
        "\n",
        "We provide the code for computing these sinusoidal positional embeddings below. To give some intuition about the structure of the sinusoidal positional embeddings, we also plot a heatmap of the pairwise inner products $\\langle PE_i, PE_j \\rangle$. We see that that positions that are closer together have more similar positional embeddings, with additional oscillatory behavior on top of that."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a33552bc",
      "metadata": {
        "id": "a33552bc"
      },
      "source": [
        "### Problem 2.3: Describe the positional embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "544c34a7",
      "metadata": {
        "id": "544c34a7"
      },
      "source": [
        "Below is an implementation of the positional embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "5f07281e",
      "metadata": {
        "id": "5f07281e"
      },
      "outputs": [],
      "source": [
        "def get_sinusoidal_positional_embeddings(sequence_length, dim, base=10000):\n",
        "    inv_freq = 1.0 / (base ** (np.arange(0, dim, 2) / dim))\n",
        "    t = np.arange(sequence_length)\n",
        "    sinusoid_inp = np.einsum(\"i,j->ij\", t, inv_freq)\n",
        "\n",
        "    sin, cos = np.sin(sinusoid_inp), np.cos(sinusoid_inp)\n",
        "\n",
        "    emb = np.concatenate((sin, cos), axis=-1)\n",
        "    return emb"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f36259d6",
      "metadata": {
        "id": "f36259d6"
      },
      "source": [
        "Explore the positional embeddings by computing the inner-product between all pairs of positional embeddings, and then plotting the similarity matrix as a heat map. Do the results make sense? What are the positional embeddings designed to model? Do they do this effectively? Comment below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "49ff03b0",
      "metadata": {
        "id": "49ff03b0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 484
        },
        "outputId": "3afad713-9ac8-4f1d-892f-5c156bd2bfcd"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "pe = get_sinusoidal_positional_embeddings(128, d_model)\n",
        "\n",
        "#Your code here\n",
        "pe_similarity = pe @ pe.T\n",
        "sns.heatmap(pe_similarity)\n",
        "plt.title('Similarity Structure of Sinusoidal Positional Embeddings')\n",
        "plt.xlabel('Position')\n",
        "plt.ylabel('Position')\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1f98808d",
      "metadata": {
        "id": "1f98808d"
      },
      "source": [
        "[Your markdown here]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The similarity heatmap\n",
        "$$\n",
        "S_{ij} = \\langle PE_i, PE_j \\rangle\n",
        "$$\n",
        "shows a strong bright diagonal, meaning that each position is most similar to itself and to nearby positions. As the distance $|i-j|$ increases, the similarity gradually decreases, with smooth oscillatory patterns caused by the sine and cosine components at different frequencies.\n",
        "\n",
        "This structure is exactly what sinusoidal positional embeddings are designed to model: they encode **absolute position** through a unique combination of sinusoidal frequencies, while also making **relative distances** between positions recoverable from inner products. Nearby positions have similar embeddings, and distant positions become less similar in a predictable, periodic way.\n",
        "\n",
        "From the plot, we clearly observe this intended behavior, so the sinusoidal positional embeddings appear to effectively capture both positional identity and relative ordering information required by the Transformer.\n"
      ],
      "metadata": {
        "id": "FswTMgEKHvCn"
      },
      "id": "FswTMgEKHvCn"
    },
    {
      "cell_type": "markdown",
      "id": "06ea1c4f",
      "metadata": {
        "id": "06ea1c4f"
      },
      "source": [
        "### Multi-Head Attention\n",
        "\n",
        "The core operation in a Transformer is multi-head attention, sometimes called self-attention. In this problem, we will implement multi-head attention in numpy from scratch, given the trained parameters of the model.\n",
        "\n",
        "The input is a sequence of vectors $X = (x_1, ..., x_n) \\in \\reals^{n \\times d_{model}}$. For each attention head $h \\in [n_h]$, the parameters consist of a query projection matrix $W_q^h \\in \\reals^{d_{model} \\times d_h}$, a key projection matrix $W_k^h \\in \\reals^{d_{model} \\times d_h}$, and a value projection matrix $W_k^h \\in \\reals^{d_{model} \\times d_h}$. Here, $d_h$ is the \"head dimension\", taken to be $d_{model} / n_h$ (to maintain the same dimensionality between the input and output). The algorithm, for each head, is the following:\n",
        "1. Compute the queries $Q = X W_q^h$, keys $K = X W_k^h$, and values $V = X W_v^h$. Note that the linear maps are applied independently for each token across the embedding dimension (not sequence dimension), such that $Q, K, V \\in \\reals^{n \\times d_h}$.\n",
        "2. Compare the queries and keys via inner products to get an $n \\times n$ attention matrix $A = \\mathrm{Softmax}(Q K^{\\intercal}) \\in \\reals^{n \\times n}$.\n",
        "3. Use the attention scores $A$ to select values, producing the output of the self-attention head: $\\mathrm{head}_h = A V \\in \\reals^{n \\times d_h}$.\n",
        "We then concatenate the retrieved values across all heads, and apply a final linear map. Putting this all together yields:\n",
        "$$\n",
        "\\begin{align}\n",
        "    \\mathrm{head}_h &= \\mathrm{Softmax}((X W_q^h) (X W_k^h)^{\\intercal}) X W_v^h\\\\\n",
        "    \\mathrm{MultiHeadAttention}(X) &= \\mathrm{concat}(\\mathrm{head}_1, ..., \\mathrm{head}_{n_h}) W_o\\\\\n",
        "\\end{align}\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c4ef44d4",
      "metadata": {
        "id": "c4ef44d4"
      },
      "source": [
        "### Problem 2.4: Implement multi-head attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "e80ba0d4",
      "metadata": {
        "id": "e80ba0d4"
      },
      "outputs": [],
      "source": [
        "# first, implement softmax and causal masking\n",
        "def softmax(x, axis=-1):\n",
        "    \"\"\"\n",
        "    Numerically stable softmax over the given axis.\n",
        "    \"\"\"\n",
        "    # subtract max for numerical stability\n",
        "    x_max = np.max(x, axis=axis, keepdims=True)\n",
        "    x_shifted = x - x_max\n",
        "    exp_x = np.exp(x_shifted)\n",
        "    sum_exp = np.sum(exp_x, axis=axis, keepdims=True)\n",
        "    return exp_x / sum_exp\n",
        "\n",
        "def apply_presoftmax_causal_mask(attn_scores):\n",
        "    \"\"\"\n",
        "    Apply a causal mask to the attention scores:\n",
        "    set all entries above the main diagonal to -inf,\n",
        "    so each position can only attend to itself and previous positions.\n",
        "\n",
        "    attn_scores: (..., seq_len, seq_len)\n",
        "    \"\"\"\n",
        "    seq_len = attn_scores.shape[-1]\n",
        "\n",
        "    # upper triangle (k=1) is \"future\" positions\n",
        "    mask = np.triu(np.ones((seq_len, seq_len), dtype=bool), k=1)\n",
        "\n",
        "    # copy to avoid in-place modification surprises\n",
        "    masked_scores = attn_scores.copy()\n",
        "    masked_scores[..., mask] = -np.inf\n",
        "    return masked_scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "b2bb2cec",
      "metadata": {
        "id": "b2bb2cec",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b746c98e-418f-4cc5-a4df-d3f64efc01e1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Softmax:\n",
            " [[0.25 0.25 0.25 0.25]\n",
            " [0.25 0.25 0.25 0.25]\n",
            " [0.25 0.25 0.25 0.25]\n",
            " [0.25 0.25 0.25 0.25]]\n",
            "Masked Attention Scores:\n",
            " [[  1. -inf -inf -inf]\n",
            " [  1.   1. -inf -inf]\n",
            " [  1.   1.   1. -inf]\n",
            " [  1.   1.   1.   1.]]\n",
            "Masked Softmax:\n",
            " [[1.         0.         0.         0.        ]\n",
            " [0.5        0.5        0.         0.        ]\n",
            " [0.33333333 0.33333333 0.33333333 0.        ]\n",
            " [0.25       0.25       0.25       0.25      ]]\n"
          ]
        }
      ],
      "source": [
        "# check your implementation\n",
        "x = np.ones((4,4))\n",
        "softmax_x = softmax(x, axis=1)\n",
        "print(\"Softmax:\\n\", softmax_x)\n",
        "\n",
        "# apply_presoftmax_causal_mask test\n",
        "attn_scores = np.ones((4,4))\n",
        "masked_scores = apply_presoftmax_causal_mask(attn_scores)\n",
        "print(\"Masked Attention Scores:\\n\", masked_scores)\n",
        "\n",
        "masked_softmax = softmax(masked_scores, axis=-1)\n",
        "print(\"Masked Softmax:\\n\", masked_softmax)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "06fd01b7",
      "metadata": {
        "id": "06fd01b7"
      },
      "source": [
        "Expected results:\n",
        "```\n",
        "Softmax:\n",
        " [[0.25 0.25 0.25 0.25]\n",
        " [0.25 0.25 0.25 0.25]\n",
        " [0.25 0.25 0.25 0.25]\n",
        " [0.25 0.25 0.25 0.25]]\n",
        "Masked Attention Scores:\n",
        " [[  1. -inf -inf -inf]\n",
        " [  1.   1. -inf -inf]\n",
        " [  1.   1.   1. -inf]\n",
        " [  1.   1.   1.   1.]]\n",
        "Masked Softmax:\n",
        " [[1.         0.         0.         0.        ]\n",
        " [0.5        0.5        0.         0.        ]\n",
        " [0.33333333 0.33333333 0.33333333 0.        ]\n",
        " [0.25       0.25       0.25       0.25      ]]\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "39e1ad32",
      "metadata": {
        "id": "39e1ad32"
      },
      "outputs": [],
      "source": [
        "\n",
        "def multi_head_attention(x, params, layer_prefix='layers.0'):\n",
        "    \"\"\"\n",
        "    Compute multi-head self-attention.\n",
        "\n",
        "    Args:\n",
        "        x (np.array): input tensor, shape (n, d_model)\n",
        "        params (dict): dictionary containing the model parameters\n",
        "        layer_prefix (str): prefix of parameter names corresponding to the layer\n",
        "        verbose (bool): whether to print intermediate shapes\n",
        "    \"\"\"\n",
        "\n",
        "    # get parameters of multi-head attention layer\n",
        "    wq = params[f'{layer_prefix}.attention.wq.weight'].T # (d_model, d_model)\n",
        "    wk = params[f'{layer_prefix}.attention.wk.weight'].T # (d_model, d_model)\n",
        "    wv = params[f'{layer_prefix}.attention.wv.weight'].T # (d_model, d_model)\n",
        "    wo = params[f'{layer_prefix}.attention.wo.weight'].T # (d_model, d_model)\n",
        "\n",
        "    head_dim = d_model // n_heads # dimension of each head\n",
        "    attn_scale = 1 / math.sqrt(head_dim) # scaling factor for attention scores\n",
        "\n",
        "    # the wq, wk, wv, wo matrices contain weights for all heads, concatenated\n",
        "    # first, we split wq, wk, wv, wo into heads\n",
        "    # note: there are more efficient implementations, but this is more verbose/pedagogical\n",
        "    wq = wq.reshape(d_model, n_heads, head_dim).transpose(1, 0, 2) # (n_heads, d_model, head_dim)\n",
        "    wk = wk.reshape(d_model, n_heads, head_dim).transpose(1, 0, 2) # (n_heads, d_model, head_dim)\n",
        "    wv = wv.reshape(d_model, n_heads, head_dim).transpose(1, 0, 2) # (n_heads, d_model, head_dim)\n",
        "\n",
        "    head_outputs = []\n",
        "    for head in range(n_heads):\n",
        "\n",
        "        # get head-specific parameters (these are the query/key/value projections for this head)\n",
        "        # your code ehre\n",
        "        wqh = wq[head] # (d_model, head_dim)\n",
        "        wkh = wk[head] # (d_model, head_dim)\n",
        "        wvh = wv[head] # (d_model, head_dim)\n",
        "\n",
        "\n",
        "        # compute queries, keys, values\n",
        "        # your code here\n",
        "        q = x @ wqh      # (n, head_dim)\n",
        "        k = x @ wkh      # (n, head_dim)\n",
        "        v = x @ wvh      # (n, head_dim)\n",
        "\n",
        "        # compute attention scores\n",
        "        # your code here\n",
        "        # compute dot product\n",
        "        attn_scores = q @ k.T # (n, n)\n",
        "        # multiply by scaling factor\n",
        "        attn_scores = attn_scores * attn_scale # (n, n)\n",
        "\n",
        "        # apply causal mask\n",
        "        attn_scores = apply_presoftmax_causal_mask(attn_scores) # (n, n)\n",
        "        # apply softmax\n",
        "        attn_scores = softmax(attn_scores, axis=-1) # (n, n)\n",
        "\n",
        "        # apply attention scores to values\n",
        "        # your code here\n",
        "        head_out = attn_scores @ v # (n, head_dim)\n",
        "\n",
        "        # store the head output\n",
        "        head_outputs.append(head_out)\n",
        "\n",
        "    # concatenate all head outputs\n",
        "    head_outputs = np.concatenate(head_outputs, axis=-1) # (n, d_model)\n",
        "\n",
        "    # apply output linear map W_o to concatenated head outputs\n",
        "    # your code here\n",
        "    output = head_outputs @ wo # (n, d_model)\n",
        "\n",
        "    return output"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9d4747b5",
      "metadata": {
        "id": "9d4747b5"
      },
      "source": [
        "#### Test your Attention implementation\n",
        "\n",
        "To test if you have the correct implementation, you can run the following\n",
        "test line. We show the expected output if your implementation is correct."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "0ae5584c",
      "metadata": {
        "id": "0ae5584c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "45c7b259-e67a-4b45-eaaf-9bd530fe4bf8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-0.61194002, -0.31581021,  0.17525476, -0.03176344,  0.23671877],\n",
              "       [-0.61194002, -0.31581021,  0.17525476, -0.03176344,  0.23671877],\n",
              "       [-0.61194002, -0.31581021,  0.17525476, -0.03176344,  0.23671877]])"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "multi_head_attention(np.ones((block_size, d_model)), transformer_model_weights)[:3, :5]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7b9237d4",
      "metadata": {
        "id": "7b9237d4"
      },
      "source": [
        "Expected output:\n",
        "\n",
        "```\n",
        "array([[-0.61194002, -0.31581021,  0.17525476, -0.03176344,  0.23671877],\n",
        "       [-0.61194002, -0.31581021,  0.17525476, -0.03176344,  0.23671877],\n",
        "       [-0.61194002, -0.31581021,  0.17525476, -0.03176344,  0.23671877]])\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2d4cc0b0",
      "metadata": {
        "id": "2d4cc0b0"
      },
      "source": [
        "### MLP\n",
        "\n",
        "Each Transformer layer (i.e., block) consists of two operations: 1) (multi-head) self-attention, which enables exchange of information between tokens, and 2) a multi-layer perceptron, which processes each token independently. A Transformer model is essentially just alternating between these two operations. In this problem, we will implement the multi-layer perceptron step. Typically, the MLP at each layer is simply a two-layer (one hidden layer) MLP or Feed Forward Network. In our model, we use a ReLU activation in the hidden layer, though other activations are possible. The same MLP network is applied to each token embedding in the sequence independently.\n",
        "\n",
        "Given $X = (x_1, ..., x_n) \\in \\reals^{n \\times d_{model}}$, we apply the MLP as follows:\n",
        "\n",
        "$$\\mathrm{MLP}(X) = \\mathrm{ReLU}(X W_1) W_2$$\n",
        "\n",
        "Note that we don't use biases for simplicity."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b3b77441",
      "metadata": {
        "id": "b3b77441"
      },
      "source": [
        "### Problem 2.5: Implement the MLP\n",
        "\n",
        "Next, we need to apply the multi-layer perceptron in each layer. Complete the implementation below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "7f8c36a4",
      "metadata": {
        "id": "7f8c36a4"
      },
      "outputs": [],
      "source": [
        "def relu(x):\n",
        "    return np.maximum(x, 0)\n",
        "\n",
        "def mlp(x, params, layer_prefix='layers.0'):\n",
        "    # get MLP parameters\n",
        "    w1 = params[f'{layer_prefix}.feed_forward.0.weight'].T # (d_model, d_ff)\n",
        "    w2 = params[f'{layer_prefix}.feed_forward.2.weight'].T # (d_ff, d_model)\n",
        "\n",
        "\n",
        "    # Your code here\n",
        "\n",
        "    h = relu(x @ w1)\n",
        "    o = h @ w2 # (n, d_model)\n",
        "\n",
        "    return o"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a4a42184",
      "metadata": {
        "id": "a4a42184"
      },
      "source": [
        "#### Test your MLP implementation\n",
        "\n",
        "To test if you have the correct MLP implementation, you can run the following\n",
        "test line. We show the expected output if your implementation is correct."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "e9425352",
      "metadata": {
        "id": "e9425352",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b5f37c1d-04a5-48c1-b971-608c4562ef46"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0.36650751, -0.02109491,  0.13528369, -0.0459696 ,  0.2035102 ],\n",
              "       [ 0.36650751, -0.02109491,  0.13528369, -0.0459696 ,  0.2035102 ],\n",
              "       [ 0.36650751, -0.02109491,  0.13528369, -0.0459696 ,  0.2035102 ]])"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ],
      "source": [
        "mlp(np.ones((block_size, d_model)), transformer_model_weights)[:3, :5]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7b08660e",
      "metadata": {
        "id": "7b08660e"
      },
      "source": [
        "Expected output:\n",
        "\n",
        "```\n",
        "array([[ 0.36650751, -0.02109491,  0.13528369, -0.0459696 ,  0.2035102 ],\n",
        "       [ 0.36650751, -0.02109491,  0.13528369, -0.0459696 ,  0.2035102 ],\n",
        "       [ 0.36650751, -0.02109491,  0.13528369, -0.0459696 ,  0.2035102 ]])\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "14897e04",
      "metadata": {
        "id": "14897e04"
      },
      "source": [
        "### Problem 2.6: Implement Layer Normalization\n",
        "\n",
        "Layer Normalization is a technique that normalizes the inputs across the features of a layer to stabilize and accelerate training in deep neural networks. It rescales activations to have zero mean and unit variance *within* each layer, then applies a learned gain and shift. In Transformers, LayerNorm is applied before or after sublayers (like attention and feedforward blocks), stabilizing training and maintaining consistent scaling across tokens and layers.\n",
        "\n",
        "In this problem, we will implement layer normalization. For a vector of activations $x \\in \\reals^d$, layer normalization returns the normalized feature vector\n",
        "$$\\mathrm{LayerNorm}(x) = \\frac{x - E[X]}{\\sqrt{\\mathrm{Var}[x] + \\epsilon}} * \\gamma + \\beta,$$\n",
        "where $\\gamma \\in \\reals^d$ is the weight parameter, and $\\beta \\in \\reals^d$ is the bias parameter. Here, $E[\\cdot]$ and $\\mathrm{Var}[\\cdot]$ indicates the mean and variance over the embedding dimension $d_{\\mathrm{model}}$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "ed2acc24",
      "metadata": {
        "id": "ed2acc24"
      },
      "outputs": [],
      "source": [
        "def layer_norm(x, params, layer_prefix='layers.0', norm_type='attn_norm', norm_eps=1e-5):\n",
        "    # get layer norm params\n",
        "    weight = params[f'{layer_prefix}.{norm_type}.weight'] # (d_model,)\n",
        "    bias = params[f'{layer_prefix}.{norm_type}.bias'] # (d_model,)\n",
        "\n",
        "    # your code here\n",
        "\n",
        "    # compute mean and variance across feature dimension (last axis)\n",
        "    mean = np.mean(x, axis=-1, keepdims=True)              # (n, 1)\n",
        "    var  = np.var(x, axis=-1, keepdims=True)               # (n, 1)\n",
        "\n",
        "    # normalize\n",
        "    x_hat = (x - mean) / np.sqrt(var + norm_eps)           # (n, d_model)\n",
        "\n",
        "    # scale and shift\n",
        "    normed_x = x_hat * weight + bias\n",
        "\n",
        "    return normed_x"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2dcb4f90",
      "metadata": {
        "id": "2dcb4f90"
      },
      "source": [
        "#### Test your LayerNorm implementation\n",
        "\n",
        "To test if you have the correct LayerNorm implementation, you can run the following test line. We show the expected output if your implementation is correct."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "db015561",
      "metadata": {
        "id": "db015561",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6a4dd311-ab85-4926-8f6f-975c30440a5b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-1.76364724, -1.75155978, -1.7455241 , -1.74605486, -1.72934376],\n",
              "       [-1.76364724, -1.75155978, -1.7455241 , -1.74605486, -1.72934376],\n",
              "       [-1.76364724, -1.75155978, -1.7455241 , -1.74605486, -1.72934376]])"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ],
      "source": [
        "layer_norm(np.arange(block_size * d_model).reshape(block_size, d_model), transformer_model_weights, layer_prefix='layers.0', norm_type='attn_norm')[:3, :5]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9a7263c3",
      "metadata": {
        "id": "9a7263c3"
      },
      "source": [
        "Expected Output:\n",
        "```\n",
        "array([[-1.76364724, -1.75155978, -1.7455241 , -1.74605486, -1.72934376],\n",
        "       [-1.76364724, -1.75155978, -1.7455241 , -1.74605486, -1.72934376],\n",
        "       [-1.76364724, -1.75155978, -1.7455241 , -1.74605486, -1.72934376]])\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "84a63140",
      "metadata": {
        "id": "84a63140"
      },
      "source": [
        "### Problem 2.7: Exploring Normalization in Residual Architectures\n",
        "\n",
        "Transformers are a type of *residual network*. At each layer, the embeddings of each token are iteratively refined by adding the result of a non-linear learnable transformation (either attention or MLP). In general, a residual architecture has the form\n",
        "$$x^{(\\ell + 1)} = x^{(\\ell)} + \\mathcal{F}(x^{(\\ell)}),$$\n",
        "where $x^{(\\ell)}$ is the internal representation at layer $\\ell$ and $\\mathcal{F}(\\cdot)$ is a learned transformation. You can think of $\\mathcal{F}(\\cdot)$ as modeling the *residual* $x^{(\\ell)} \\mapsto x^{(\\ell + 1)} - x^{(\\ell)}$ rather than needing to directly model the full $x^{(\\ell)} \\mapsto x^{(\\ell + 1)}$ map. This improves training stability and mitigates vanishing gradient problems.\n",
        "\n",
        "There are different ways that normalization can be applied to residual networks. The form above includes no normalization.\n",
        "\n",
        "In the original Transformer paper, the authors use so-called \"post-normalization\", where the normalization is applied *after* the residual block:\n",
        "$$x^{(\\ell + 1)} = \\mathrm{LayerNorm}(x^{(\\ell)} + \\mathcal{F}(x^{(\\ell)})).$$\n",
        "\n",
        "In modern architectures, it is more common to use \"pre-normalization\", where the normalization before the learned transformation:\n",
        "$$x^{(\\ell + 1)} = x^{(\\ell)} + \\mathcal{F}(\\mathrm{LayerNorm}(x^{(\\ell)})).$$\n",
        "\n",
        "In this problem, we will explore the effects of normalization on the magnitude of the feature vectors at each layer. Complete the code below to implement each form of normalization: 1) no norm, 2) post-norm, 3) pre-norm. Plot the average embedding norm across layers for each form of normalization. Interpret the results. Does this shed light onto why normalization is a useful empirical technique to stabilize and accelerate training?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "baa42b2c",
      "metadata": {
        "id": "baa42b2c"
      },
      "outputs": [],
      "source": [
        "# Experiment: compare NoNorm, Pre-LN, Post-LN for a stack of feedforward blocks\n",
        "np.random.seed(0)\n",
        "\n",
        "seq_len = block_size\n",
        "# d_model = D\n",
        "d_model = d_model\n",
        "#d_ff = F * D\n",
        "d_ff = 4 * d_model\n",
        "n_layers_exp = 24\n",
        "\n",
        "\n",
        "# shared random input X0\n",
        "X0 = np.random.randn(seq_len, d_model)\n",
        "\n",
        "def simple_layer_norm(x, eps=1e-5):\n",
        "    # for purposes of exploration, implement a simple layer norm with weight = 1 and bias = 0\n",
        "\n",
        "    # your code here\n",
        "    mean = np.mean(x, axis=-1, keepdims=True)          # (seq_len, 1)\n",
        "    var  = np.var(x, axis=-1, keepdims=True)           # (seq_len, 1)\n",
        "    norm_x = (x - mean) / np.sqrt(var + eps)           # (seq_len, d_model)\n",
        "    return norm_x\n",
        "\n",
        "# random FFN blocks to play the role of $\\mathcal{F}$\n",
        "# initialize independent W1/W2 per layer\n",
        "W1_list = [np.random.randn(d_model, d_ff) * (1.0 / np.sqrt(d_model)) for _ in range(n_layers_exp)]\n",
        "W2_list = [np.random.randn(d_ff, d_model) * (1.0 / np.sqrt(d_ff)) for _ in range(n_layers_exp)]\n",
        "\n",
        "def ffn(x, W1, W2):\n",
        "    return relu(np.dot(x, W1)).dot(W2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "59b7772f",
      "metadata": {
        "id": "59b7772f"
      },
      "outputs": [],
      "source": [
        "# complete the implementation\n",
        "def run_scheme(scheme_name):\n",
        "    x = X0.copy()\n",
        "    norms = [np.mean(np.linalg.norm(x, axis=1))]  # layer 0\n",
        "    for i in range(n_layers_exp):\n",
        "        W1, W2 = W1_list[i], W2_list[i]\n",
        "        # residual without normalization\n",
        "        if scheme_name == 'no_norm':\n",
        "            x = x + ffn(x, W1, W2)\n",
        "\n",
        "        # apply \"pre-norm\" normalization\n",
        "        elif scheme_name == 'pre_ln':\n",
        "            x = x + ffn(simple_layer_norm(x), W1, W2)\n",
        "\n",
        "        # apply \"post-norm\" normalization\n",
        "        elif scheme_name == 'post_ln':\n",
        "            x = simple_layer_norm(x + ffn(x, W1, W2))\n",
        "        else:\n",
        "            raise ValueError(scheme_name)\n",
        "\n",
        "        # compute norm of each token embedding at current layer\n",
        "        avg_norm_this_layer = np.mean(np.linalg.norm(x, axis=1)) # your code here\n",
        "        norms.append(avg_norm_this_layer)\n",
        "    return np.array(norms)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "98e20af4",
      "metadata": {
        "id": "98e20af4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 407
        },
        "outputId": "7399fc3e-a10a-44ee-ddfd-3a2bd118529d"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 600x400 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "\n",
        "schemes = ['no_norm', 'pre_ln', 'post_ln']\n",
        "results = {s: run_scheme(s) for s in schemes}\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(6,4))\n",
        "layers = np.arange(0, n_layers_exp + 1)\n",
        "plt.plot(layers, results['no_norm'], marker='o', label='No Norm')\n",
        "plt.plot(layers, results['pre_ln'], marker='o', label='Pre-LayerNorm')\n",
        "plt.plot(layers, results['post_ln'], marker='o', label='Post-LayerNorm')\n",
        "plt.xlabel('Layer index')\n",
        "plt.ylabel('Mean token L2 norm')\n",
        "plt.title('Residual stream norm vs depth: No Norm / Pre-LN / Post-LN')\n",
        "plt.xticks(layers)\n",
        "plt.grid(alpha=0.3)\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "90f3730f",
      "metadata": {
        "id": "90f3730f"
      },
      "source": [
        "[Your Markdown Here]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this experiment, we compare how the mean L2 norm of the token embeddings evolves with depth under three schemes:\n",
        "\n",
        "- **No normalization**  \n",
        "  $$\n",
        "  x^{(\\ell + 1)} = x^{(\\ell)} + \\mathcal{F}\\big(x^{(\\ell)}\\big)\n",
        "  $$\n",
        "- **Pre-LayerNorm**  \n",
        "  $$\n",
        "  x^{(\\ell + 1)} = x^{(\\ell)} + \\mathcal{F}\\big(\\mathrm{LayerNorm}(x^{(\\ell)})\\big)\n",
        "  $$\n",
        "- **Post-LayerNorm**  \n",
        "  $$\n",
        "  x^{(\\ell + 1)} = \\mathrm{LayerNorm}\\big(x^{(\\ell)} + \\mathcal{F}(x^{(\\ell)})\\big)\n",
        "  $$\n",
        "\n",
        "From the plotted curves of the mean token L2 norm vs. layer index, we typically observe:\n",
        "\n",
        "1. **NoNorm**  \n",
        "   Without any normalization, the residual updates $\\mathcal{F}(x^{(\\ell)})$ can systematically increase (or sometimes decrease) the scale of the representations. As we stack many layers, these effects accumulate, so the average norm of $x^{(\\ell)}$ may grow or shrink dramatically. This makes the representations poorly scaled and can lead to unstable optimization (exploding or vanishing activations and gradients).\n",
        "\n",
        "2. **Post-LayerNorm**  \n",
        "   In the post-norm setting we first add the residual and then normalize:\n",
        "   $$\n",
        "   x^{(\\ell + 1)} = \\mathrm{LayerNorm}\\big(x^{(\\ell)} + \\mathcal{F}(x^{(\\ell)})\\big).\n",
        "   $$\n",
        "   This keeps the output of each block at a roughly fixed scale. The norms across layers stay more controlled than in the no-norm case. However, the residual branch $x^{(\\ell)} + \\mathcal{F}(x^{(\\ell)})$ is still unnormalized *before* gradients are backpropagated through it, which can make training deep networks harder in practice.\n",
        "\n",
        "3. **Pre-LayerNorm**  \n",
        "   In the pre-norm setting, each transformation sees a normalized input:\n",
        "   $$\n",
        "   x^{(\\ell + 1)} = x^{(\\ell)} + \\mathcal{F}\\big(\\mathrm{LayerNorm}(x^{(\\ell)})\\big).\n",
        "   $$\n",
        "   This means the function $\\mathcal{F}(\\cdot)$ always operates on activations with stable mean and variance. As a result, the residual stream $x^{(\\ell)}$ tends to have a much more stable norm across depth. This leads to smoother gradient flow and better training stability for very deep Transformers.\n",
        "\n",
        "**Conclusion.**  \n",
        "The experiment shows that:\n",
        "\n",
        "- Without normalization, the scale of the residual stream can drift significantly with depth.\n",
        "- Normalization (especially pre-LayerNorm) keeps the norms of the representations well-behaved and relatively constant across layers.\n",
        "\n",
        "This provides intuition for why normalization is a powerful empirical tool: by controlling the magnitude of activations at each layer, LayerNorm improves numerical stability, helps prevent exploding/vanishing gradients, and makes optimization of deep residual architectures like Transformers much easier.\n"
      ],
      "metadata": {
        "id": "mtd8SsDMW8pk"
      },
      "id": "mtd8SsDMW8pk"
    },
    {
      "cell_type": "markdown",
      "id": "ac85b5cb",
      "metadata": {
        "id": "ac85b5cb"
      },
      "source": [
        "### Final Prediction Layer\n",
        "\n",
        "A Transformer model iteratively applies multi-head attention and MLP layers to process the input. This produces a processed representation of shape $n \\times d_{model}$. To make the final prediction (e.g., predict the next token), we need to map the $d_{model}$-dimensional embedding vectors to logits over the output vocabulary. To do this, we simply apply a linear map that maps from $d_{model}$ to $\\mathtt{vocab\\_size}$.\n",
        "\n",
        "The starter code for this is given below; you need to complete it."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "03bc5f78",
      "metadata": {
        "id": "03bc5f78"
      },
      "source": [
        "### Problem 2.8: Implement the prediction layer as logits."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "5646afc5",
      "metadata": {
        "id": "5646afc5"
      },
      "outputs": [],
      "source": [
        "def prediction_head(x, params):\n",
        "    # get needed parameters\n",
        "    w = params['fc_out.weight'].T # (d_model, vocab_size)\n",
        "    b = params['fc_out.bias'] # (vocab_size,)\n",
        "\n",
        "    # Your code here\n",
        "    logits = x @ w + b  # (n, vocab_size)\n",
        "\n",
        "    return logits"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ebb2f111",
      "metadata": {
        "id": "ebb2f111"
      },
      "source": [
        "### Test the prediction head"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "7e515dac",
      "metadata": {
        "id": "7e515dac",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e6f435f0-73cb-41ed-a045-acc1e0b43cc7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-1.52472634, -0.92704511, -0.07956709, -0.27001108, -0.2583226 ],\n",
              "       [-1.52472634, -0.92704511, -0.07956709, -0.27001108, -0.2583226 ],\n",
              "       [-1.52472634, -0.92704511, -0.07956709, -0.27001108, -0.2583226 ]])"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ],
      "source": [
        "prediction_head(np.ones((block_size, d_model)), transformer_model_weights)[:3, :5]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c9aac831",
      "metadata": {
        "id": "c9aac831"
      },
      "source": [
        "```\n",
        "array([[-1.52472634, -0.92704511, -0.07956709, -0.27001108, -0.2583226 ],\n",
        "       [-1.52472634, -0.92704511, -0.07956709, -0.27001108, -0.2583226 ],\n",
        "       [-1.52472634, -0.92704511, -0.07956709, -0.27001108, -0.2583226 ]])\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6453d59e",
      "metadata": {
        "id": "6453d59e"
      },
      "source": [
        "### Putting it all together: A Full Transformer Language Model\n",
        "\n",
        "We are now ready to put this all together to assemble our Transformer Language Model. Recall that the Transformer architecture consists of iteratively applying multi-head attention and MLPs. Each time we apply attention or the MLP, we also apply a *residual connection*: $X^{(\\ell + 1)} = X^{(\\ell)} + F(X^{(\\ell)})$. This can be interpreted as a mechanism to enable easy communication between different layers (some people call refer to this idea as the \"residual stream\"). Real Transformers also include layer normalization in each layer, but we omit this for simplicity in this problem.\n",
        "\n",
        "The full algorithm is given below:\n",
        "1. Embed the tokens using the embedding lookup table: $(t_1, ..., t_n) \\mapsto (E_{t_1}, ..., E_{t_n}) =: X^{(0)}$\n",
        "2. Add the positional embeddings: $X^{(0)} \\gets X^{(0)} + (PE_1, ..., PE_n)$\n",
        "3. For each layer $\\ell = 1, ..., L$:\n",
        "    1. Apply Multi-Head Attention: $\\tilde{X}^{(\\ell)} \\gets X^{(\\ell-1)} + \\mathrm{MultiHeadAttention}(\\mathrm{LayerNorm}(X^{(\\ell-1)}))$.\n",
        "    2. Apply the MLP: $X^{(\\ell)} \\gets \\tilde{X}^{(\\ell)} + \\mathrm{MLP}(\\mathrm{LayerNorm}(\\tilde{X}^{(\\ell)}))$.\n",
        "4. Compute the logits"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1a7193e8",
      "metadata": {
        "id": "1a7193e8"
      },
      "source": [
        "### Problem 2.9: Complete the implementation\n",
        "\n",
        "Complete the starter code below, which takes embeddings, adds positional encoding,\n",
        "and then adds the attention and MLP components to each layer. Remember that\n",
        "everything is added together, with the computations in one layer added to the outputs of the previous layer, forming the \"residual stream\".\n",
        "\n",
        "Hint: to complete the implementation, you will need to use the `layer_norm`, `multi_head_attention`, `mlp`, and `prediction_head` functions you implemented above. Recall that these functions take `param` as input (which are the weights of the pre-trained model). They use the `layer_prefix` argument to fetch the weights at the given layer. For example, `layer_prefix=f'layers.{i}'` will the corresponding module for layer `i`. Additionally, since each layer has two LayerNorms, one for the attention module and one for the MLP module, the `layer_norm` function additionally takes `norm_type` argument, which can be `'attn_norm'` or `'ff_norm'`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "abd9dccb",
      "metadata": {
        "id": "abd9dccb"
      },
      "outputs": [],
      "source": [
        "def transformer(tokens, params):\n",
        "    # tokens: (n,) integer array\n",
        "    # params: dictionary of parameters\n",
        "\n",
        "    # map tokens to embeddings using embed_tokens\n",
        "    x = embed_tokens(tokens, params)  # (n, d_model)\n",
        "\n",
        "    # add positional embeddings\n",
        "    pe = get_sinusoidal_positional_embeddings(x.shape[0], x.shape[1])  # (n, d_model)\n",
        "    x = x + pe  # (n, d_model)\n",
        "\n",
        "    # transformer blocks\n",
        "    for i in range(n_layers):\n",
        "\n",
        "        layer_prefix = f'layers.{i}'\n",
        "\n",
        "        # compute multi-head self-attention and add residual with pre-normalization\n",
        "        normed_x = layer_norm(\n",
        "            x, params,\n",
        "            layer_prefix=layer_prefix,\n",
        "            norm_type='attn_norm'\n",
        "        )  # (n, d_model)\n",
        "\n",
        "        attn_out = multi_head_attention(\n",
        "            normed_x, params,\n",
        "            layer_prefix=layer_prefix\n",
        "        )  # (n, d_model)\n",
        "\n",
        "        # residual connection\n",
        "        x = x + attn_out  # (n, d_model)\n",
        "\n",
        "        # compute MLP and add residual with pre-normalization\n",
        "        normed_x = layer_norm(\n",
        "            x, params,\n",
        "            layer_prefix=layer_prefix,\n",
        "            norm_type='ff_norm'\n",
        "        )  # (n, d_model)\n",
        "\n",
        "        mlp_out = mlp(\n",
        "            normed_x, params,\n",
        "            layer_prefix=layer_prefix\n",
        "        )  # (n, d_model)\n",
        "\n",
        "        # residual connection\n",
        "        x = x + mlp_out  # (n, d_model)\n",
        "\n",
        "    # compute logits via the prediction_head\n",
        "    logits = prediction_head(x, params)  # (n, vocab_size)\n",
        "\n",
        "    return logits\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fafbacc5",
      "metadata": {
        "id": "fafbacc5"
      },
      "source": [
        "### Test your implementation\n",
        "\n",
        "You can check your implementation against the expected output below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "id": "a1d6b0e1",
      "metadata": {
        "id": "a1d6b0e1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "52b9293b-5efa-4942-82e3-fbcbd15280b9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-3.30795907, -1.9419719 , -0.67287022, -1.71627247, -1.54158904],\n",
              "       [-4.15893293, -1.83363694, -2.26268612, -2.63614069, -2.48212268],\n",
              "       [-3.51487281, -2.31248407, -3.52562579, -3.18057024, -1.76104192]])"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ],
      "source": [
        "transformer([0, 1, 2], params=transformer_model_weights)[:3, :5]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3d05fa35",
      "metadata": {
        "id": "3d05fa35"
      },
      "source": [
        "Expected Output:\n",
        "\n",
        "```\n",
        "array([[-3.30795902, -1.94197185, -0.67287023, -1.7162725 , -1.541589  ],\n",
        "       [-4.15893294, -1.8336369 , -2.26268609, -2.63614073, -2.48212268],\n",
        "       [-3.51487274, -2.31248397, -3.52562574, -3.18057025, -1.76104193]])\n",
        "``````"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cceff165",
      "metadata": {
        "id": "cceff165"
      },
      "source": [
        "### Generate some text\n",
        "\n",
        "Below, we provide some code for generating text from a Transformer language model. The sampling procedure is *autoregressive*. This means that we input some text to the model and it outputs a distribution over next tokens. We sample the next token and append it to the text, then repeat the procedure."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f4dcb151",
      "metadata": {
        "id": "f4dcb151"
      },
      "source": [
        "### Problem 2.10: Complete the next token generator\n",
        "\n",
        "Complete the next token generator, but filling in the missing code below. This uses \"temperature\" to focus on the more probable tokens in a given context (as the temperature decreases)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "id": "1583451e",
      "metadata": {
        "id": "1583451e"
      },
      "outputs": [],
      "source": [
        "def generate_with_transformer(prefix_text, params, max_len=128, greedy=False, temperature=0.9):\n",
        "    # encode seed text\n",
        "    prefix_tokens = list(enc.encode(prefix_text))\n",
        "\n",
        "    # initialize generated tokens\n",
        "    generated_tokens = prefix_tokens\n",
        "\n",
        "    # generate new tokens\n",
        "    for i in range(max_len):\n",
        "        # predict next token\n",
        "        logits = transformer(generated_tokens, params)\n",
        "        next_logits = logits[-1]\n",
        "        # hint: logits[-1] corresponds to prediction of the next token\n",
        "        if greedy:\n",
        "            # Your code here\n",
        "            next_token = int(np.argmax(next_logits))\n",
        "        else:\n",
        "            # Your code here\n",
        "            scaled = next_logits / max(temperature, 1e-8)\n",
        "            probs = softmax(scaled, axis=-1).reshape(-1)    # (vocab_size,)\n",
        "            next_token = int(np.random.choice(len(probs), p=probs))\n",
        "\n",
        "        # add next token to generated tokens\n",
        "        generated_tokens.append(next_token)\n",
        "\n",
        "    # This converts the tokens to text, using the tiktoken decoder:\n",
        "    generated_text = enc.decode(generated_tokens)\n",
        "\n",
        "    return generated_text"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0c564fc9",
      "metadata": {
        "id": "0c564fc9"
      },
      "source": [
        "### Test your implementation by generating text. You're the Bard!\n",
        "\n",
        "Use your implementation to generate text according to the model. Generate text at different temperatures. Do the results make sense? Comment on the quality of the model. What changes to the model would lead to better results? Comment in the Markdown cell below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "id": "2e61d567",
      "metadata": {
        "id": "2e61d567"
      },
      "outputs": [],
      "source": [
        " prefix_text = \"\"\"STUDENT:\n",
        " O though Transformer, wrought of mind and code,\n",
        " Thou art the loom where language weaveth thought!\n",
        " Each token, like a spark of meaning sown,\n",
        " Attendeth all, and all attend to one.\"\"\"\n",
        "\n",
        "#prefix_text = \"\"\"HAMLET:\n",
        "#To be, or not to be: that is the question\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "id": "8fe61599",
      "metadata": {
        "id": "8fe61599",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "07351813-aa7b-4337-b3cc-4d05e06f140f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "STUDENT:\n",
            "O though Transformer, wrought of mind and code,\n",
            "Thou art the loom where language weaveth thought!\n",
            "Each token, like a spark of meaning sown,\n",
            "Attendeth all, and all attend to one.\n",
            "\n",
            "\n",
            "\n",
            "Second Citizen:\n",
            "Well, sir;\n",
            "What he's the belly.\n",
            "Second Citizen:\n",
            "Well, what then?\n",
            "First Citizen:\n",
            "First Citizen:\n",
            "First Citizen:\n",
            "\n",
            "\n",
            "If you'll batriciusers, what he hath alre's answer.\n",
            "\n",
            "\n",
            "\n",
            "If I'll hear, what he hath done fell, what he hath done fell, what answer agused men can bell:\n",
            "\n",
            "If I'll hear the bell:\n",
            "If you must not masters\n"
          ]
        }
      ],
      "source": [
        "generated_text = generate_with_transformer(prefix_text, transformer_model_weights, greedy=True, max_len=128)\n",
        "print(generated_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "id": "c00cffe4",
      "metadata": {
        "id": "c00cffe4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0f600138-b38a-4910-9135-c4043147e38c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "STUDENT:\n",
            "O though Transformer, wrought of mind and code,\n",
            "Thou art the loom where language weaveth thought!\n",
            "Each token, like a spark of meaning sown,\n",
            "Attendeth all, and all attend to one.\n",
            "\n",
            "Second Citizen:\n",
            "What accountry heard it.\n",
            "\n",
            "cormall--\n",
            "What would all the belly say you have help in the body say he should famous mutinion.\n",
            "If you have baturerously-cons him you have\n",
            "\n",
            "There was answer.\n",
            "That,\n",
            "The for the belly was for his counown to the body was deliberate, even tailant eye is the Citizen:\n",
            "That, my good friends, they did\n"
          ]
        }
      ],
      "source": [
        "generated_text = generate_with_transformer(prefix_text, transformer_model_weights,\n",
        "    greedy=False, temperature=0.8, max_len=128)\n",
        "print(generated_text)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generated_text = generate_with_transformer(prefix_text, transformer_model_weights,\n",
        "    greedy=False, temperature=0.5, max_len=128)\n",
        "print(generated_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8YNcgaXfrOD_",
        "outputId": "39b11a7a-8f30-4f18-964e-f3484d455eed"
      },
      "id": "8YNcgaXfrOD_",
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "STUDENT:\n",
            "O though Transformer, wrought of mind and code,\n",
            "Thou art the loom where language weaveth thought!\n",
            "Each token, like a spark of meaning sown,\n",
            "Attendeth all, and all attend to one.\n",
            "\n",
            "Make your strong\n",
            "\n",
            "\n",
            "\n",
            "Didst the great to the commont, we'll show 'tty helplices, eail. The he should first to that even talic barth,\n",
            "\n",
            "\n",
            "You must in the one that\n",
            "Against in the commontue is\n",
            "\n",
            "Le, who comes here?\n",
            "\n",
            "What helain, whose offence awhile batrike at the belly whosedusedungsuse will give himself with\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generated_text = generate_with_transformer(prefix_text, transformer_model_weights,\n",
        "    greedy=False, temperature=1, max_len=128)\n",
        "print(generated_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3brtWrVjrQaf",
        "outputId": "9117d611-3f4e-4fc9-90a8-ebc5ce19c1cb"
      },
      "id": "3brtWrVjrQaf",
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "STUDENT:\n",
            "O though Transformer, wrought of mind and code,\n",
            "Thou art the loom where language weaveth thought!\n",
            "Each token, like a spark of meaning sown,\n",
            "Attendeth all, and all attend to one.\n",
            "\n",
            "\n",
            "Second Citizen:\n",
            "have belly.\n",
            "\n",
            "Ye be authouse and restrain on would receive\n",
            "MENENIUS:\n",
            "dward, most find you res done first to\n",
            "That only,\n",
            "Your most that\n",
            "That his vigil.\n",
            "What's the one accusin;\n",
            "Well'd against the commont!\n",
            "Well, good belly was delivers well. W kind you the m eafferalt them to them, thirst to do deliver out to till you\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "117f5643",
      "metadata": {
        "id": "117f5643"
      },
      "source": [
        "[Your markdown here]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "- **Greedy decoding (temperature not used)**  \n",
        "  With greedy decoding, the model always chooses the single most likely next token.  \n",
        "  In my runs, this often leads to **loops and repetition**, e.g. the phrase *“First Citizen:”* is repeated many times. The text quickly becomes formulaic and stuck in a local pattern.\n",
        "\n",
        "- **Sampling with temperature (e.g. $T = 0.8$)**  \n",
        "  When I use stochastic sampling with temperature $T = 0.8$, the text is **more diverse and sometimes more interesting**, with different characters and lines appearing.  \n",
        "  However, it also becomes **less grammatical** and occasionally drifts into odd or non-word tokens, because the model sometimes samples lower-probability tokens.\n",
        "\n",
        "- **Overall model quality**  \n",
        "  The model successfully captures some **Shakespeare-like structure**:\n",
        "  - character names and dialogue formatting,\n",
        "  - colons and line breaks,\n",
        "  - some archaic wording and rhythms.  \n",
        "\n",
        "  But it still frequently produces:\n",
        "  - nonsense words or malformed phrases,\n",
        "  - excessive repetition,\n",
        "  - weak long-range coherence across sentences and speakers.  \n",
        "\n",
        "  This is expected for a **small Transformer** trained on a **limited Shakespeare corpus** with a **short context window**.\n",
        "\n",
        "- **Possible improvements**\n",
        "  - Use a **larger model** (bigger $d_{\\text{model}}$, more layers, more heads).\n",
        "  - Train on **more data** (all of Shakespeare, possibly similar plays or Early Modern English text).\n",
        "  - **Train longer** or with better regularization/optimization.\n",
        "  - Use improved **decoding strategies**:\n",
        "    - top-$k$ or nucleus (top-$p$) sampling,\n",
        "    - repetition penalties or n-gram blocking to reduce loops,\n",
        "    - temperature tuning per layer or per position.\n"
      ],
      "metadata": {
        "id": "HxR3-xH1tRLl"
      },
      "id": "HxR3-xH1tRLl"
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}