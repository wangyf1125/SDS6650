{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "5ccb1d8a",
      "metadata": {
        "id": "5ccb1d8a"
      },
      "source": [
        "# Intermediate Machine Learning: Assignment 5\n",
        "\n",
        "**Deadline**\n",
        "\n",
        "Assignment 5 is due Thursday, December 4 by 11:59pm. Late work will not be accepted as per the course policies (see the Syllabus and Course policies on Canvas).\n",
        "\n",
        "Directly sharing answers is not okay, but discussing problems with the course staff or with other students is encouraged. Acknowledge any use of an AI system such as ChatGPT or Copilot.\n",
        "\n",
        "You should start early so that you have time to get help if you're stuck. The drop-in office hours schedule can be found on Canvas. You can also post questions or start discussions on Ed Discussion. The assignment may look long at first glance, but the problems are broken up into steps that should help you to make steady progress.\n",
        "\n",
        "**Submission**\n",
        "\n",
        "Submit your assignment as a pdf file on Gradescope, and as a notebook (.ipynb) on Canvas. You can access Gradescope through Canvas on the left-side of the class home page. The problems in each homework assignment are numbered. Note: When submitting on Gradescope, please select the correct pages of your pdf that correspond to each problem. This will allow graders to more easily find your complete solution to each problem.\n",
        "\n",
        "To produce the .pdf, please convert to html and then print to pdf. (You may want to use your pdf print menu to scale the pages to be sure that cells are not truncated.) To convert to html, you can use this [converter notebook](https://colab.research.google.com/github/YData123/sds365-fa25/blob/main/assignments/Convert_ipynb_to_HTML_in_Colab.ipynb).\n",
        "\n",
        "You should start early so that you have time to get help if you're stuck. The drop-in office hours schedule can be found on Canvas. You can also post questions or start discussions on Ed Discussion. The assignment may look long at first glance, but the problems are broken up into steps that should help you to make steady progress.\n",
        "\n",
        "**Topics**\n",
        "\n",
        " * RNNs and GRUs\n",
        " * Transformers\n",
        "\n",
        "After doing this assignment you will have a working knowledge of RNNs and Transformers, and more solid Python skills."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "78f64b66",
      "metadata": {
        "id": "78f64b66"
      },
      "source": [
        "## Problem 1: Elephants Can Remember (25 points)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ba3663cd",
      "metadata": {
        "id": "ba3663cd"
      },
      "source": [
        "![ECR](https://upload.wikimedia.org/wikipedia/en/e/e3/Elephants_can_Remember_First_Edition_Cover_1972.jpg)\n",
        "\n",
        "In this problem, we will work with \"vanilla\" Recurrent Neural Networks (RNNs) and Recurrent Neural Networks with Gated Recurrent Units (GRUs). The models in this part of the assignment will be character-based models, trained on an extract of the book [Elephants Can Remember](https://en.wikipedia.org/wiki/Elephants_Can_Remember) by Agatha Christie. To reduce the size of our vocabulary, the text is pre-processed by converting the letters to lower case and removing numbers. The code below shows some information about our training and test set. All the necessary files for this problem are available on [github](https://github.com/YData123/sds365-fa25/tree/main/assignments/assn5).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "0804721c",
      "metadata": {
        "id": "0804721c"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import tensorflow as tf\n",
        "import random\n",
        "import json\n",
        "\n",
        "# Ensure tf.keras is the main keras module\n",
        "# import tensorflow.keras as keras # This line is no longer strictly necessary if all components are explicitly imported below\n",
        "\n",
        "from tensorflow.keras.models import Sequential, model_from_json\n",
        "from tensorflow.keras.layers import Dense, Activation\n",
        "from tensorflow.keras.layers import GRU, SimpleRNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "3qF67L0FBdmT",
      "metadata": {
        "id": "3qF67L0FBdmT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "795cba3e-3bdf-46a6-d476-9af662f1295f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "data_path = '/content/drive/MyDrive/assn5/' # set to the path where you store the assignment files from github\n",
        "                # e.g., '/content/drive/MyDrive/assn5/'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "f93de082",
      "metadata": {
        "id": "f93de082"
      },
      "outputs": [],
      "source": [
        "with open(data_path + 'gru_problem/Agatha_Christie_train.txt', 'r') as file:\n",
        "    train_text = file.read()\n",
        "\n",
        "with open(data_path + 'gru_problem/Agatha_Christie_test.txt', 'r') as file:\n",
        "    test_text = file.read()\n",
        "\n",
        "vocabulary = sorted(list(set(train_text + test_text)))\n",
        "vocab_size = len(vocabulary)\n",
        "\n",
        "# Dictionaries to go from a character to index and vice versa\n",
        "char_to_indices = dict((c, i) for i, c in enumerate(vocabulary))\n",
        "indices_to_char = dict((i, c) for i, c in enumerate(vocabulary))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "7320ed64",
      "metadata": {
        "id": "7320ed64",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "689f81bf-b5b5-4151-b96e-0991e4bc0017"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'mrs. oliver looked at herself in the glass. she gave a brief, sideways look towards the clock on the mantelpiece, which she had some idea was twenty minutes slow. then she resumed her study of her coiffure. the trouble with mrs. oliver was--and she admitted it freely--that her styles of hairdressing were always being changed. she had tried almost everything in turn. a severe pompadour at one time, then a wind-swept style where you brushed back your locks to display an intellectual brow, at least'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "# The first 500 characters of our training set\n",
        "train_text[0:500]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "3a50abac",
      "metadata": {
        "id": "3a50abac",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4cadd037-3ef5-4865-cdef-31dc500339cd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The vocabulary contains 44 characters\n",
            "The training set contains 262174 characters\n",
            "The test set contains 7209 characters\n"
          ]
        }
      ],
      "source": [
        "print(\"The vocabulary contains\", vocab_size, \"characters\")\n",
        "print(\"The training set contains\", len(train_text) ,\"characters\")\n",
        "print(\"The test set contains\", len(test_text) ,\"characters\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d02ce399",
      "metadata": {
        "id": "d02ce399"
      },
      "source": [
        "### Problem 1.1: The Diversity of Language Models"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6951a417",
      "metadata": {
        "id": "6951a417"
      },
      "source": [
        "Before jumping into coding, let's start with comparing the language models we will be using in this assigment."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "472741ad",
      "metadata": {
        "id": "472741ad"
      },
      "source": [
        "1. Describe the differences between a Vanilla RNN and a GRU network. In your explanation, make sure you mention the issues with vanilla RNNs and how GRUs try to solve them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "06832930",
      "metadata": {
        "id": "06832930"
      },
      "outputs": [],
      "source": [
        "# Your markdown here"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Differences between a Vanilla RNN and a GRU Network\n",
        "\n",
        "A **Vanilla Recurrent Neural Network (RNN)** updates its hidden state using a simple transition:\n",
        "\n",
        "$$\n",
        "h_t = \\tanh(W_h h_{t-1} + W_x x_t + b)\n",
        "$$\n",
        "\n",
        "Although vanilla RNNs are able to model sequential data, they suffer from **vanishing and exploding gradient problems** during backpropagation through time (BPTT). As a result, they struggle to learn **long-term dependencies**, meaning information from far earlier time steps is often forgotten.\n",
        "\n",
        "A **Gated Recurrent Unit (GRU)** addresses these issues by introducing **gating mechanisms** that control how information flows through time. A GRU contains two gates:\n",
        "\n",
        "- **Update gate** $$ z_t $$: controls how much of the previous hidden state is kept  \n",
        "- **Reset gate** $$ r_t $$: controls how much past information is forgotten  \n",
        "\n",
        "The GRU updates are given by:\n",
        "\n",
        "$$\n",
        "z_t = \\sigma(W_z x_t + U_z h_{t-1})\n",
        "$$\n",
        "\n",
        "$$\n",
        "r_t = \\sigma(W_r x_t + U_r h_{t-1})\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\tilde{h}_t = \\tanh(W x_t + U (r_t \\odot h_{t-1}))\n",
        "$$\n",
        "\n",
        "$$\n",
        "h_t = (1 - z_t) \\odot h_{t-1} + z_t \\odot \\tilde{h}_t\n",
        "$$\n",
        "\n",
        "By using these gates, GRUs can:\n",
        "- Preserve important long-term information  \n",
        "- Mitigate vanishing gradients  \n",
        "- Adaptively forget irrelevant history  \n",
        "\n",
        "As a result, **GRUs are more stable and effective than vanilla RNNs for modeling long sequences**, such as character-based text.\n"
      ],
      "metadata": {
        "id": "T6tOrRskiODx"
      },
      "id": "T6tOrRskiODx"
    },
    {
      "cell_type": "markdown",
      "id": "61c45e86",
      "metadata": {
        "id": "61c45e86"
      },
      "source": [
        "2. Describe at least two advantages of a character based language model over a word based language model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8b07db8a",
      "metadata": {
        "id": "8b07db8a"
      },
      "outputs": [],
      "source": [
        "# Your markdown here"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### 2. Advantages of a Character-Based Language Model over a Word-Based Model\n",
        "\n",
        "#### (1) No Out-of-Vocabulary (OOV) Problem\n",
        "\n",
        "A character-based model works on individual characters instead of full words. Therefore, it can generate **any word**, including:\n",
        "- Rare words  \n",
        "- New words  \n",
        "- Misspelled words  \n",
        "- Names not seen in training  \n",
        "\n",
        "In contrast, word-based models cannot handle words that are not in their predefined vocabulary.\n",
        "\n",
        "---\n",
        "\n",
        "#### (2) Smaller and Simpler Vocabulary\n",
        "\n",
        "Character vocabularies are typically very small (e.g., 40–100 characters), whereas word vocabularies often contain **tens of thousands of unique tokens**. This leads to:\n",
        "- Lower memory usage  \n",
        "- Smaller embedding/output layers  \n",
        "- Easier training and deployment  \n",
        "\n",
        "---\n",
        "\n",
        "#### (3) Better Modeling of Morphology and Spelling\n",
        "\n",
        "Character-based models naturally learn:\n",
        "- Prefixes and suffixes  \n",
        "- Spelling patterns  \n",
        "- Punctuation and formatting  \n",
        "\n",
        "This is especially helpful for modeling **literary text**, such as novels, where stylistic details matter."
      ],
      "metadata": {
        "id": "Kxhfe99vieqA"
      },
      "id": "Kxhfe99vieqA"
    },
    {
      "cell_type": "markdown",
      "id": "ccac684e",
      "metadata": {
        "id": "ccac684e"
      },
      "source": [
        "### Problem 1.2: Generating Text with the Vanilla RNN"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "85bdca8b",
      "metadata": {
        "id": "85bdca8b"
      },
      "source": [
        "The code below loads in a pretrained vanilla RNN model with two layers. The model is set up exactly like in the lecture slides (with tanh activation layers in the recurrent layers) with the addition of biases (intercepts) in every layer (i.e. the recurrent layer and the dense layer). The training process consisted of 30 epochs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "45bd5d3b",
      "metadata": {
        "id": "45bd5d3b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "741e8c08-97b8-4f64-c583-36dcedfbb859"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/rnn/rnn.py:199: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        }
      ],
      "source": [
        "# load json and create model\n",
        "json_file = open(data_path + 'gru_problem/RNN_model.json', 'r')\n",
        "loaded_model_json_str = json_file.read()\n",
        "json_file.close()\n",
        "\n",
        "RNN_model = model_from_json(loaded_model_json_str, custom_objects={'Sequential': Sequential, 'SimpleRNN': SimpleRNN})\n",
        "RNN_model.load_weights(data_path + \"gru_problem/RNN_model.h5\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "648273ea",
      "metadata": {
        "id": "648273ea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 259
        },
        "outputId": "6ca14cba-ea1b-4d03-a63c-4845cb904c19"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ Vanilla_RNN_1 (\u001b[38;5;33mSimpleRNN\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │        \u001b[38;5;34m22,144\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ Vanilla_RNN_2 (\u001b[38;5;33mSimpleRNN\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │        \u001b[38;5;34m12,352\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ Dense_layer (\u001b[38;5;33mDense\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m44\u001b[0m)             │         \u001b[38;5;34m2,860\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ Softmax_layer (\u001b[38;5;33mActivation\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m44\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ Vanilla_RNN_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SimpleRNN</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │        <span style=\"color: #00af00; text-decoration-color: #00af00\">22,144</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ Vanilla_RNN_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SimpleRNN</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">12,352</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ Dense_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">44</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,860</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ Softmax_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">44</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m37,356\u001b[0m (145.92 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">37,356</span> (145.92 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m37,356\u001b[0m (145.92 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">37,356</span> (145.92 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# load in the weights and show summary\n",
        "weights_RNN = RNN_model.get_weights()\n",
        "RNN_model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a4bcccd8",
      "metadata": {
        "id": "a4bcccd8"
      },
      "source": [
        "Finish the following function that uses a vanilla RNN architecture to generate text, given the weights of the RNN model, a text prompt, and the number of characters to return. The function should be completed by **only using numpy functions**. Use your knowledge of how every weight plays its role in the RNN architecture. Do not worry about the weight extraction part, this is already provided for you. The weight matrix $W_{xh1}$, for example, denotes the weight matrix to go from the input x to the first hidden state layer h1. The hidden states $h_1$ and $h_2$ are initialized to a vector of zeros.\n",
        "\n",
        "The embedding of each character has to be done by a one-hot encoding, where you will need the dictionaries defined in the introduction to go from a character to an index position."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "736ff633",
      "metadata": {
        "id": "736ff633"
      },
      "outputs": [],
      "source": [
        "def sample_text_RNN(weights, prompt, N):\n",
        "    '''\n",
        "    Uses a pretrained RNN to generate text, starting from a prompt,\n",
        "    only using the weights and numpy commands\n",
        "            Parameters:\n",
        "                    weights (list): Weights of the pretrained RNN model\n",
        "                    prompt (string): Start of generated sentence\n",
        "                    N (int): Length of output sentence (including prompt)\n",
        "            Returns:\n",
        "                    output_sentence (string): Text generated by RNN\n",
        "    '''\n",
        "    # Extracting weights and biases\n",
        "    # Dimensions of matrices are same format as lecture slides\n",
        "\n",
        "    # First Recurrent Layer\n",
        "    W_xh1 = weights[0].T\n",
        "    W_h1h1 = weights[1].T\n",
        "    b_h1 = np.expand_dims(weights[2], axis=1)\n",
        "\n",
        "    # Second Recurrent Layer\n",
        "    W_h1h2 = weights[3].T\n",
        "    W_h2h2 = weights[4].T\n",
        "    b_h2 = np.expand_dims(weights[5], axis=1)\n",
        "\n",
        "    # Linear (dense) layer\n",
        "    W_h2y = weights[6].T\n",
        "    b_y = np.expand_dims(weights[7], axis=1)\n",
        "\n",
        "    # Initiate the hidden states\n",
        "    h1 = np.zeros((W_h1h1.shape[0], 1))\n",
        "    h2 = np.zeros((W_h2h2.shape[0], 1))\n",
        "\n",
        "    # -----------------------------------------------\n",
        "\n",
        "    # Your code starts here\n",
        "    # output_sentence = \"\"\n",
        "\n",
        "    vocab_size = W_h2y.shape[0]\n",
        "\n",
        "    # ----- helper functions (numpy only) -----\n",
        "    def one_hot(char_idx):\n",
        "        x = np.zeros((vocab_size, 1))\n",
        "        x[char_idx, 0] = 1.0\n",
        "        return x\n",
        "\n",
        "    def softmax(z):\n",
        "        z = z - np.max(z)        # numerical stability\n",
        "        e = np.exp(z)\n",
        "        return e / np.sum(e)\n",
        "\n",
        "    # If prompt is empty, start from a space character (or any valid char)\n",
        "    if len(prompt) == 0:\n",
        "        # choose a deterministic start, here: first char in vocabulary\n",
        "        first_idx = 0\n",
        "        prompt = indices_to_char[first_idx]\n",
        "\n",
        "    output_sentence = prompt\n",
        "\n",
        "    # ---------- 1) Run the RNN over the prompt ----------\n",
        "    # We update h1, h2 sequentially for every character in the prompt\n",
        "    for c in prompt:\n",
        "        idx = char_to_indices[c]\n",
        "        x_t = one_hot(idx)\n",
        "\n",
        "        # First recurrent layer\n",
        "        h1 = np.tanh(W_xh1 @ x_t + W_h1h1 @ h1 + b_h1)\n",
        "\n",
        "        # Second recurrent layer\n",
        "        h2 = np.tanh(W_h1h2 @ h1 + W_h2h2 @ h2 + b_h2)\n",
        "\n",
        "        # Output layer (logits and softmax)\n",
        "        y_logits = W_h2y @ h2 + b_y\n",
        "        y_prob = softmax(y_logits)   # (V, 1)\n",
        "\n",
        "    # At this point, h1, h2, y_prob encode the information after the prompt\n",
        "\n",
        "    # ---------- 2) Generate new characters ----------\n",
        "    num_to_generate = max(0, N - len(prompt))\n",
        "\n",
        "    for _ in range(num_to_generate):\n",
        "        # Sample next character index according to the predicted distribution\n",
        "        next_idx = np.random.choice(np.arange(vocab_size), p=y_prob.ravel())\n",
        "        next_char = indices_to_char[next_idx]\n",
        "        output_sentence += next_char\n",
        "\n",
        "        # Use the generated character as input for the next time step\n",
        "        x_t = one_hot(next_idx)\n",
        "\n",
        "        h1 = np.tanh(W_xh1 @ x_t + W_h1h1 @ h1 + b_h1)\n",
        "        h2 = np.tanh(W_h1h2 @ h1 + W_h2h2 @ h2 + b_h2)\n",
        "\n",
        "        y_logits = W_h2y @ h2 + b_y\n",
        "        y_prob = softmax(y_logits)\n",
        "\n",
        "\n",
        "    return output_sentence"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cfc7420f",
      "metadata": {
        "id": "cfc7420f"
      },
      "source": [
        "Test out your function by running the following code cell. Use it as a sanity check that your code is working. The generated text should not be perfect English, but at least you should be able to recognize some words."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "12458ad0",
      "metadata": {
        "id": "12458ad0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dc471e79-6e73-40ea-d964-1f78f4e1e0b3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mrs. oliver looked at herself in the glass. she gave a brief, sideways looke.\" \"any well, year yepentions to round afo\"d'ble. wered to be llopt. befter there elephants. you don't nat it's addn't now.\" \"yes, and you meanor you would be she come forge. yes, it lasterly, was susted remember. will or suery yor wenterd foor find has dict an trespencaly. he differeactiftly beens look, there was and thacky to,\" said mrs. oliver. what will de there illnot abwe the sareing prefined befuly, really of therin the flow roteln to and feether ged it or that the garre?fing that's not the fasher once siqued.\" \"ahe fordenasted. order, went astered ind and something they seemes. alined. you've known e sapplire her people belifior som abouthired. \"worden, it wasn't any yeverys plothen, i dwan's anyway, i'm sust could, \"thiek-profft of her tond il owe things to anythen there wele another frosling,\" said mrs. oliver. \"one but it many all. nere wile she-ly as reapsed about years, the have home tog-ty-she hap\n"
          ]
        }
      ],
      "source": [
        "print(sample_text_RNN(weights_RNN,\n",
        "                      'mrs. oliver looked at herself in the glass. she gave a brief, sideways look',\n",
        "                      1000))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c950eae8",
      "metadata": {
        "id": "c950eae8"
      },
      "source": [
        "### Problem 1.3: Generating Text with the GRU"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1dd93ee7",
      "metadata": {
        "id": "1dd93ee7"
      },
      "source": [
        "The code below loads in a pretrained GRU model. The model is set up exactly like in the lecture slides (with sigmoid activation layers for the gates and tanh activation layers in the recurrent layer). The model is trained for only 10 epochs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "82291909",
      "metadata": {
        "id": "82291909",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6e8298ca-64cc-4341-ce2b-e016ea0a6113"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/rnn/rnn.py:199: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        }
      ],
      "source": [
        "# load json and create model\n",
        "json_file = open(data_path + 'gru_problem/GRU_model.json', 'r')\n",
        "loaded_model_json_str = json_file.read()\n",
        "json_file.close()\n",
        "\n",
        "GRU_model = model_from_json(loaded_model_json_str, custom_objects={'Sequential': Sequential, 'GRU': GRU})\n",
        "GRU_model.load_weights(data_path + \"gru_problem/GRU_model.h5\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "738974b5",
      "metadata": {
        "id": "738974b5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 226
        },
        "outputId": "857c7fe7-06e4-4898-8eef-dda601cb2d1c"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ gru (\u001b[38;5;33mGRU\u001b[0m)                       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │       \u001b[38;5;34m857,088\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m44\u001b[0m)             │        \u001b[38;5;34m22,572\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ activation (\u001b[38;5;33mActivation\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m44\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ gru (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GRU</span>)                       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">857,088</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">44</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">22,572</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ activation (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">44</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m879,660\u001b[0m (3.36 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">879,660</span> (3.36 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m879,660\u001b[0m (3.36 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">879,660</span> (3.36 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# load in the weights and show summary\n",
        "weights_GRU = GRU_model.get_weights()\n",
        "GRU_model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cad5b34f",
      "metadata": {
        "id": "cad5b34f"
      },
      "source": [
        "Finish the following function that uses a GRU architecture to generate text, given the weights of the GRU model, a text prompt, and the number of characters to return. The function should be completed by **only using numpy functions**. Use your knowledge of how every weight plays its role in the GRU architecture. Do not worry about the weight extraction part, this is already provided for you. The hidden state $h$ is initialized to a vector of zeros.\n",
        "\n",
        "The embedding of each character has to be done by a one-hot encoding, where you will need the dictionaries defined in the introduction to go from a character to an index position.\n",
        "\n",
        "Note: a slightly different version of the GRU is used, where the candidate state $c_t$ is calculated as:\n",
        "\n",
        "$$\n",
        "c_t = \\text{tanh} \\left(W_{hx} x_t \\ + \\ \\Gamma_t^r \\odot (W_{hh} h_{t-1}) \\ + \\ b_h \\right)\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "259c1d40",
      "metadata": {
        "id": "259c1d40"
      },
      "outputs": [],
      "source": [
        "# Helper function\n",
        "def sigmoid(z):\n",
        "    return 1 / (1 + np.exp(-z))\n",
        "\n",
        "def sample_text_GRU(weights, prompt, N):\n",
        "    '''\n",
        "    Uses a pretrained GRU to generate text, starting from a prompt,\n",
        "    only using the weights and numpy commands\n",
        "            Parameters:\n",
        "                    weights (list): Weights of the pretrained GRU model\n",
        "                    prompt (string): Start of generated sentence\n",
        "                    N (int): Total length of output sentence\n",
        "            Returns:\n",
        "                    output_sentence (string): Text generated by GRU\n",
        "    '''\n",
        "    # Extracting weights and biases\n",
        "    # Dimensions of matrices are same format as lecture slides\n",
        "\n",
        "    # GRU Layer\n",
        "    W_ux, W_rx, W_hx = np.split(weights[0].T, 3, axis = 0)\n",
        "    W_uh, W_rh, W_hh = np.split(weights[1].T, 3, axis = 0)\n",
        "\n",
        "    bias = np.sum(weights[2], axis=0)\n",
        "    b_u, b_r, b_h = np.split(np.expand_dims(bias, axis=1), 3)\n",
        "\n",
        "    # Linear (dense) layer\n",
        "    W_y = weights[3].T\n",
        "    b_y = np.expand_dims(weights[4], axis=1)\n",
        "\n",
        "    # Initiate hidden state\n",
        "    h = np.zeros((W_hh.shape[0], 1))\n",
        "\n",
        "    # -----------------------------------------------\n",
        "    # Your code starts here\n",
        "    # output_sentence = \"\"\n",
        "    vocab_size = W_y.shape[0]\n",
        "\n",
        "    # ---------- helper functions ----------\n",
        "    def one_hot(char_idx):\n",
        "        x = np.zeros((vocab_size, 1))\n",
        "        x[char_idx, 0] = 1.0\n",
        "        return x\n",
        "\n",
        "    def softmax(z):\n",
        "        z = z - np.max(z)      # numerical stability\n",
        "        e = np.exp(z)\n",
        "        return e / np.sum(e)\n",
        "\n",
        "    # If prompt is empty, start from a fixed character (e.g. first in vocab)\n",
        "    if len(prompt) == 0:\n",
        "        prompt = indices_to_char[0]\n",
        "\n",
        "    output_sentence = prompt\n",
        "\n",
        "    # ---------- 1) Run GRU over the prompt (warm-up) ----------\n",
        "    for c in prompt:\n",
        "        idx = char_to_indices[c]\n",
        "        x_t = one_hot(idx)\n",
        "\n",
        "        # Gates\n",
        "        u_t = sigmoid(W_ux @ x_t + W_uh @ h + b_u)          # update gate\n",
        "        r_t = sigmoid(W_rx @ x_t + W_rh @ h + b_r)          # reset gate\n",
        "\n",
        "        # Candidate state (note the form given in the assignment)\n",
        "        c_t = np.tanh(W_hx @ x_t + r_t * (W_hh @ h) + b_h)\n",
        "\n",
        "        # New hidden state\n",
        "        h = (1 - u_t) * h + u_t * c_t\n",
        "\n",
        "        # Output distribution (not strictly needed during warm-up,\n",
        "        # but we compute it so the code is uniform)\n",
        "        logits = W_y @ h + b_y\n",
        "        y_prob = softmax(logits)\n",
        "\n",
        "    # ---------- 2) Generate new characters ----------\n",
        "    num_to_generate = max(0, N - len(prompt))\n",
        "\n",
        "    for _ in range(num_to_generate):\n",
        "        # Sample index according to last y_prob\n",
        "        next_idx = np.random.choice(np.arange(vocab_size), p=y_prob.ravel())\n",
        "        next_char = indices_to_char[next_idx]\n",
        "        output_sentence += next_char\n",
        "\n",
        "        # Feed sampled char back as input\n",
        "        x_t = one_hot(next_idx)\n",
        "\n",
        "        u_t = sigmoid(W_ux @ x_t + W_uh @ h + b_u)\n",
        "        r_t = sigmoid(W_rx @ x_t + W_rh @ h + b_r)\n",
        "        c_t = np.tanh(W_hx @ x_t + r_t * (W_hh @ h) + b_h)\n",
        "        h = (1 - u_t) * h + u_t * c_t\n",
        "\n",
        "        logits = W_y @ h + b_y\n",
        "        y_prob = softmax(logits)\n",
        "\n",
        "    return output_sentence\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "117fdc8f",
      "metadata": {
        "id": "117fdc8f"
      },
      "source": [
        "Test out your function by running the following code cell. Use it as a sanity check that your code is working. The generated text should not be perfect English, but at least you should be able to recognize some words."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "3fc14584",
      "metadata": {
        "id": "3fc14584",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "59cefd96-e951-44eb-f5db-84a0cf0070f6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mrs. oliver looked at herself in the glass. she gave a brief, sideways lookd hthet[e t  iotrepsoni ttay sotwrawlse s.s.t itt iteeewe c(pt at teeitlott. thaotlitrenosit ttyeon thte.gbellse steees. icteaple isthacquinheadrs ht t wouulepeedheat. t.  me mottsstttautw oadoreas.. st. zeathteseitht'tw i te kit tleindn t.e? aene fat iorist.wo t iots se ttuu moontt s easetelye. s inthasat ealle, \" as isttoli nthew say oomitt.st, w ous. iot.hew.\" t aveis st oror.wefe awhtweearotht iuibol! se fotur seittolehe setse uehat ythe. o ise phteest h t ao gte ionttha fft.ee bitew enid del,\" tayeathal t atst oenes entit ecerise th tffathetorours thalatr as kitw s theifthabtaelle d efeseee n  df.mebedeit th't w iree maalfi te?no f peed oht erervenit de divader. \" its poth o potw ? t boiedrasbuitthast. t n wtythrruwle treep nenoomdrerit ctnoodrit teeicvemobust, te ht eateseithhty t,\" thtotstoitlte shuitit snt iffea. mt ilawi. d reawblifaeyoen ielin twwaryit zeea tt hlsenonot\"it si lditae, s nt l sese iset n\n"
          ]
        }
      ],
      "source": [
        "print(sample_text_GRU(weights_GRU,\n",
        "                      'mrs. oliver looked at herself in the glass. she gave a brief, sideways look',\n",
        "                      1000))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6a35ba68",
      "metadata": {
        "id": "6a35ba68"
      },
      "source": [
        "### Problem 1.4: Can Elephants Remember Better?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "09cd85c4",
      "metadata": {
        "id": "09cd85c4"
      },
      "source": [
        "Perplexity is a measure to quantify how \"good\" a language model $M$ is, based on a test (or validation) set. The perplexity on a sequence $s$ of characters $a_i$ of size $N$ is defined as:\n",
        "\n",
        "$$\n",
        "\\text{Perplexity}(M) = M(s)^{(-1/N)} = \\left\\{p(a_1, \\ldots, a_N)\\right\\}^{(-1/N)} = \\left\\{p(a_1) \\ p(a_2|a_1) \\ \\ldots \\ p(a_N|a_1, \\ldots, a_{N-1})\\right\\}^{(-1/N)}\n",
        "$$\n",
        "\n",
        "> The intuition behind this metric is that, if a model assigns a high probability to a test set, it is not surprised to see it (not perplexed by it), which means the model $M$ has a good understanding of how the language works. Hence, a good model has, in theory, a lower perplexity. The exponent $(-1/N)$ in the formula is just a normalizing strategy (geometric average), because adding more characters to a test set would otherwise introduce more uncertainty (i.e. larger test sets would have lower probability). So by introducing the geometric average, we have a metric that is independent of the size of the test set.\n",
        "\n",
        "When calculating the perplexity, it is important to know that taking the product of a bunch of probabilities will most likely lead to a zero value by the computer. To prevent this, make use of a log-transformation:\n",
        "\n",
        "$$\n",
        "\\text{Log-Perplexity}(M) = -\\frac{1}{N} log\\left\\{p(a_1, \\ldots, a_N)\\right\\} = -\\frac{1}{N} \\left\\{log \\ p(a_1) + \\ log \\ p(a_2|a_1) + \\ \\ldots \\ + log \\  p(a_N|a_1, \\ldots, a_{N-1})\\right\\}\n",
        "$$\n",
        "\n",
        "Don't forget to go back to the normal perplexity after this transformation."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "709c6d4e",
      "metadata": {
        "id": "709c6d4e"
      },
      "source": [
        "1. Before calculating the perplexity of a test sequence, start with comparing the outputs of 2.2 and 2.3. Do you see any differences in the generated text of the Vanilla RNN model and the GRU model? Rerun your functions a couple of times (because of stochasticity) and use different prompts. Briefly discuss why you would expect (or not expect) certain differences."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3ccf8083",
      "metadata": {
        "id": "3ccf8083"
      },
      "outputs": [],
      "source": [
        "# Your markdown here"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### (1) Comparison between Vanilla RNN and GRU text generation\n",
        "\n",
        "When I compare the samples generated by the vanilla RNN (Problem 1.2) and the GRU (Problem 1.3), I observe several qualitative differences.\n",
        "\n",
        "- **Local spelling and recognizable words.**  \n",
        "  Both models are character-based and can produce recognizable English words such as *“mrs. oliver”*, *“remember”*, *“years”*, and so on. This indicates that both models have learned local spelling patterns and frequent n-grams from the training text.\n",
        "\n",
        "- **Global coherence.**  \n",
        "  The vanilla RNN model (trained for 30 epochs) often produces more well-formed full words and sometimes even short phrases that look grammatically reasonable.  \n",
        "  The GRU model, however, was only trained for 10 epochs. Its samples contain many repeated characters and incomplete or nonsense words, and the sentences tend to degrade more quickly. In my runs, the GRU often looks *less* coherent than the RNN, despite having a more powerful architecture.\n",
        "\n",
        "- **Stochasticity and prompts.**  \n",
        "  Because we sample characters from a softmax distribution, different runs and different prompts yield different texts. A more “natural” prompt usually leads to more readable continuations for both models, while a short or strange prompt makes the models quickly drift into nonsense.\n",
        "\n",
        "**Why should expect these differences?**\n",
        "\n",
        "In principle, a GRU should be better than a vanilla RNN at modeling long-range dependencies, thanks to its update and reset gates that mitigate vanishing gradients. With enough training, we would expect the GRU to produce more coherent long sequences than the vanilla RNN.\n",
        "\n",
        "However, in this assignment the vanilla RNN is trained for 30 epochs, whereas the GRU is trained for only 10 epochs. The under-trained GRU does not fully exploit its architectural advantages, so its generated text can actually look worse than that of the RNN. The observed differences are therefore a combination of **model capacity** (GRU vs RNN) and **training budget** (number of epochs).\n"
      ],
      "metadata": {
        "id": "zjaqkZGNzsQe"
      },
      "id": "zjaqkZGNzsQe"
    },
    {
      "cell_type": "markdown",
      "id": "5c9a5bf5",
      "metadata": {
        "id": "5c9a5bf5"
      },
      "source": [
        "2. Calculate the perplexity of each language model by using test_text, an unseen extract of the book. Choose the prompt as the first $m$ letters of the test set, where $m$ is a parameter that you can choose yourself. You should be able to reuse the majority of your previous code in this calculation. Discuss your results at the end."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "e0c2a390",
      "metadata": {
        "id": "e0c2a390",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6c955830-c269-4953-9b69-8b840bd27865"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vanilla RNN log-perplexity: 1.7341, perplexity: 5.6640\n",
            "GRU        log-perplexity: 2.9679, perplexity: 19.4503\n"
          ]
        }
      ],
      "source": [
        "# Your code here\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "# ---- helper functions shared by both models ----\n",
        "\n",
        "def softmax_np(z):\n",
        "    z = z - np.max(z)      # numerical stability\n",
        "    e = np.exp(z)\n",
        "    return e / np.sum(e)\n",
        "\n",
        "def one_hot_idx(idx, vocab_size):\n",
        "    x = np.zeros((vocab_size, 1))\n",
        "    x[idx, 0] = 1.0\n",
        "    return x\n",
        "\n",
        "# ========= Perplexity for the Vanilla RNN =========\n",
        "\n",
        "def log_perplexity_RNN(weights, text, m):\n",
        "    \"\"\"\n",
        "    Compute log-perplexity of the vanilla RNN model on a character sequence.\n",
        "    We use the first m characters as prompt (context) and then evaluate the\n",
        "    conditional probabilities of subsequent characters.\n",
        "    \"\"\"\n",
        "    # Extract weights\n",
        "    W_xh1 = weights[0].T\n",
        "    W_h1h1 = weights[1].T\n",
        "    b_h1 = np.expand_dims(weights[2], axis=1)\n",
        "\n",
        "    W_h1h2 = weights[3].T\n",
        "    W_h2h2 = weights[4].T\n",
        "    b_h2 = np.expand_dims(weights[5], axis=1)\n",
        "\n",
        "    W_h2y = weights[6].T\n",
        "    b_y = np.expand_dims(weights[7], axis=1)\n",
        "\n",
        "    # hidden states\n",
        "    h1 = np.zeros((W_h1h1.shape[0], 1))\n",
        "    h2 = np.zeros((W_h2h2.shape[0], 1))\n",
        "\n",
        "    vocab_size = W_h2y.shape[0]\n",
        "\n",
        "    # 1) warm-up on the first m characters\n",
        "    for t in range(m):\n",
        "        c = text[t]\n",
        "        idx = char_to_indices[c]\n",
        "        x_t = one_hot_idx(idx, vocab_size)\n",
        "\n",
        "        h1 = np.tanh(W_xh1 @ x_t + W_h1h1 @ h1 + b_h1)\n",
        "        h2 = np.tanh(W_h1h2 @ h1 + W_h2h2 @ h2 + b_h2)\n",
        "\n",
        "    # 2) accumulate log-probabilities from position m to end-1\n",
        "    log_prob_sum = 0.0\n",
        "    count = 0\n",
        "    eps = 1e-12\n",
        "\n",
        "    for t in range(m, len(text) - 1):\n",
        "        # current input\n",
        "        idx_in = char_to_indices[text[t]]\n",
        "        x_t = one_hot_idx(idx_in, vocab_size)\n",
        "\n",
        "        # forward step\n",
        "        h1 = np.tanh(W_xh1 @ x_t + W_h1h1 @ h1 + b_h1)\n",
        "        h2 = np.tanh(W_h1h2 @ h1 + W_h2h2 @ h2 + b_h2)\n",
        "\n",
        "        logits = W_h2y @ h2 + b_y\n",
        "        y_prob = softmax_np(logits)  # (V,1)\n",
        "\n",
        "        # target is the next character in the test text\n",
        "        idx_target = char_to_indices[text[t + 1]]\n",
        "        p_next = y_prob[idx_target, 0]\n",
        "\n",
        "        log_prob_sum += np.log(p_next + eps)\n",
        "        count += 1\n",
        "\n",
        "    # log-perplexity\n",
        "    log_perp = - log_prob_sum / count\n",
        "    return log_perp\n",
        "\n",
        "\n",
        "# ========= Perplexity for the GRU =========\n",
        "\n",
        "def log_perplexity_GRU(weights, text, m):\n",
        "    \"\"\"\n",
        "    Compute log-perplexity of the GRU model on a character sequence.\n",
        "    We use the first m characters as prompt and evaluate the conditional\n",
        "    probabilities of subsequent characters.\n",
        "    \"\"\"\n",
        "    # Extract GRU weights\n",
        "    W_ux, W_rx, W_hx = np.split(weights[0].T, 3, axis=0)\n",
        "    W_uh, W_rh, W_hh = np.split(weights[1].T, 3, axis=0)\n",
        "\n",
        "    bias = np.sum(weights[2], axis=0)\n",
        "    b_u, b_r, b_h = np.split(np.expand_dims(bias, axis=1), 3)\n",
        "\n",
        "    # Dense layer\n",
        "    W_y = weights[3].T\n",
        "    b_y = np.expand_dims(weights[4], axis=1)\n",
        "\n",
        "    # hidden state\n",
        "    h = np.zeros((W_hh.shape[0], 1))\n",
        "    vocab_size = W_y.shape[0]\n",
        "\n",
        "    # 1) warm-up on first m characters\n",
        "    for t in range(m):\n",
        "        c = text[t]\n",
        "        idx = char_to_indices[c]\n",
        "        x_t = one_hot_idx(idx, vocab_size)\n",
        "\n",
        "        # GRU gates\n",
        "        u_t = 1.0 / (1.0 + np.exp(-(W_ux @ x_t + W_uh @ h + b_u)))   # update\n",
        "        r_t = 1.0 / (1.0 + np.exp(-(W_rx @ x_t + W_rh @ h + b_r)))   # reset\n",
        "\n",
        "        c_t = np.tanh(W_hx @ x_t + r_t * (W_hh @ h) + b_h)\n",
        "\n",
        "        h = (1 - u_t) * h + u_t * c_t\n",
        "\n",
        "    # 2) accumulate log-probabilities\n",
        "    log_prob_sum = 0.0\n",
        "    count = 0\n",
        "    eps = 1e-12\n",
        "\n",
        "    for t in range(m, len(text) - 1):\n",
        "        idx_in = char_to_indices[text[t]]\n",
        "        x_t = one_hot_idx(idx_in, vocab_size)\n",
        "\n",
        "        u_t = 1.0 / (1.0 + np.exp(-(W_ux @ x_t + W_uh @ h + b_u)))\n",
        "        r_t = 1.0 / (1.0 + np.exp(-(W_rx @ x_t + W_rh @ h + b_r)))\n",
        "        c_t = np.tanh(W_hx @ x_t + r_t * (W_hh @ h) + b_h)\n",
        "        h = (1 - u_t) * h + u_t * c_t\n",
        "\n",
        "        logits = W_y @ h + b_y\n",
        "        y_prob = softmax_np(logits)\n",
        "\n",
        "        idx_target = char_to_indices[text[t + 1]]\n",
        "        p_next = y_prob[idx_target, 0]\n",
        "\n",
        "        log_prob_sum += np.log(p_next + eps)\n",
        "        count += 1\n",
        "\n",
        "    log_perp = - log_prob_sum / count\n",
        "    return log_perp\n",
        "\n",
        "\n",
        "# ========= actually compute perplexities =========\n",
        "\n",
        "m = 100   # length of prompt, you can choose any reasonable number\n",
        "\n",
        "log_perp_RNN = log_perplexity_RNN(weights_RNN, test_text, m)\n",
        "log_perp_GRU = log_perplexity_GRU(weights_GRU, test_text, m)\n",
        "\n",
        "perp_RNN = np.exp(log_perp_RNN)\n",
        "perp_GRU = np.exp(log_perp_GRU)\n",
        "\n",
        "print(f\"Vanilla RNN log-perplexity: {log_perp_RNN:.4f}, perplexity: {perp_RNN:.4f}\")\n",
        "print(f\"GRU        log-perplexity: {log_perp_GRU:.4f}, perplexity: {perp_GRU:.4f}\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### (2) Perplexity Results and Discussion\n",
        "\n",
        "Using the first \\( m = 100 \\) characters of the test set as the prompt, we obtained the following perplexity values on the unseen test sequence:\n",
        "\n",
        "- **Vanilla RNN**  \n",
        "  Log-perplexity: **1.7341**  \n",
        "  Perplexity: **5.6640**\n",
        "\n",
        "- **GRU**  \n",
        "  Log-perplexity: **2.9679**  \n",
        "  Perplexity: **19.4503**\n",
        "\n",
        "Since **lower perplexity indicates a better language model**, the vanilla RNN clearly outperforms the GRU on this test set. The RNN assigns significantly higher probability to the unseen text and is therefore less “surprised” by the test data.\n",
        "\n",
        "This result may seem counterintuitive at first, because GRUs are generally more powerful than vanilla RNNs due to their gating mechanisms. However, in this assignment the **vanilla RNN was trained for 30 epochs**, while the **GRU was trained for only 10 epochs**. As a result, the GRU is likely **under-trained** and does not fully exploit its architectural advantages. Therefore, the higher perplexity of the GRU mainly reflects insufficient training rather than an inferior model structure.\n"
      ],
      "metadata": {
        "id": "ne-KdRaO3jNR"
      },
      "id": "ne-KdRaO3jNR"
    },
    {
      "cell_type": "markdown",
      "id": "9ab728c9",
      "metadata": {
        "id": "9ab728c9"
      },
      "source": [
        "3. As seen in part 2 and 3 of this problem, the text generation is not perfect. Describe some possible model improvements that could make the quality of the generated text better."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8df914d2",
      "metadata": {
        "id": "8df914d2"
      },
      "outputs": [],
      "source": [
        "# Your markdown here"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "The text generated by both the vanilla RNN and the GRU is still far from perfect English. Several improvements could be made to enhance the quality of the generated text:\n",
        "\n",
        "1. **Train for more epochs and use more data.**  \n",
        "   The models in this assignment are trained on a limited corpus for a relatively small number of epochs (especially the GRU). Training longer and with more data would help the model better capture long-range dependencies and rare patterns.\n",
        "\n",
        "2. **Use deeper and more expressive architectures.**  \n",
        "   We only use shallow recurrent networks (two-layer RNN and single-layer GRU). Deeper GRU or LSTM models can better model complex temporal dependencies and typically achieve lower perplexity in language modeling tasks.\n",
        "\n",
        "3. **Change the tokenization strategy.**  \n",
        "   Character-based models are good at capturing spelling patterns but struggle with long-range syntax and semantics. Using subword-level or word-level tokens would allow the model to operate on more meaningful linguistic units and improve grammatical consistency.\n",
        "\n",
        "4. **Apply regularization and better optimization.**  \n",
        "   Techniques such as dropout, layer normalization, or improved learning-rate schedules could help stabilize training and prevent overfitting, leading to better generalization on unseen text.\n",
        "\n",
        "5. **Use improved sampling strategies.**  \n",
        "   Instead of directly sampling from the full softmax distribution, techniques such as temperature scaling, top-\\(k\\) sampling, or nucleus (top-\\(p\\)) sampling could reduce extremely low-probability characters and produce more coherent text.\n"
      ],
      "metadata": {
        "id": "nundTnH43fCY"
      },
      "id": "nundTnH43fCY"
    },
    {
      "cell_type": "markdown",
      "id": "b13b7fe6",
      "metadata": {
        "id": "b13b7fe6"
      },
      "source": [
        "## Problem 2: Be the Bard (35 points)\n",
        "\n",
        "<img src=\"https://upload.wikimedia.org/wikipedia/commons/a/a2/Shakespeare.jpg\" alt=\"William\" width=\"200\" style=\"float:left; padding:15px\"/>\n",
        "\n",
        "Transformer models are the current state of the art in many sequence modeling tasks, and the Transformer architecture underlies most Large Language Models (LLMs), including ChatGPT, Llama, Mistral, etc.\n",
        "\n",
        "In this problem, we will implement a Transformer language model from scratch in `numpy`. You will be provided with the weights of a small, slightly simplified Transformer language model that we trained on the works of Shakespeare. We will walk through implementing each component of the Transformer architecture and ultimately assemble this into a language model that can generate some text in the style of the Bard."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "8e4a7940",
      "metadata": {
        "id": "8e4a7940"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pickle\n",
        "#!pip install tiktoken\n",
        "import tiktoken\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "28091130",
      "metadata": {
        "id": "28091130"
      },
      "outputs": [],
      "source": [
        "d_model = 512\n",
        "n_layers = 4\n",
        "n_heads = 8\n",
        "d_ff = 4 * d_model\n",
        "vocab_size = 1024\n",
        "block_size = 128\n",
        "\n",
        "model_name = f'BardGPT_weights_D{d_model}L{n_layers}H{n_heads}.npy'\n",
        "transformer_model_weights = np.load(data_path + '/transformer_problem/' + model_name, allow_pickle=True).item()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "2d91252f",
      "metadata": {
        "id": "2d91252f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5d8d79f9-3c6c-453c-8a50-91bacf9d76cb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parameters:\n",
            "-----------\n",
            "word_embedding.weight: (1024, 512) [float32]\n",
            "layers.0.attention.wq.weight: (512, 512) [float32]\n",
            "layers.0.attention.wk.weight: (512, 512) [float32]\n",
            "layers.0.attention.wv.weight: (512, 512) [float32]\n",
            "layers.0.attention.wo.weight: (512, 512) [float32]\n",
            "layers.0.attn_norm.weight: (512,) [float32]\n",
            "layers.0.attn_norm.bias: (512,) [float32]\n",
            "layers.0.feed_forward.0.weight: (2048, 512) [float32]\n",
            "layers.0.feed_forward.2.weight: (512, 2048) [float32]\n",
            "layers.0.ff_norm.weight: (512,) [float32]\n",
            "layers.0.ff_norm.bias: (512,) [float32]\n",
            "layers.1.attention.wq.weight: (512, 512) [float32]\n",
            "layers.1.attention.wk.weight: (512, 512) [float32]\n",
            "layers.1.attention.wv.weight: (512, 512) [float32]\n",
            "layers.1.attention.wo.weight: (512, 512) [float32]\n",
            "layers.1.attn_norm.weight: (512,) [float32]\n",
            "layers.1.attn_norm.bias: (512,) [float32]\n",
            "layers.1.feed_forward.0.weight: (2048, 512) [float32]\n",
            "layers.1.feed_forward.2.weight: (512, 2048) [float32]\n",
            "layers.1.ff_norm.weight: (512,) [float32]\n",
            "layers.1.ff_norm.bias: (512,) [float32]\n",
            "layers.2.attention.wq.weight: (512, 512) [float32]\n",
            "layers.2.attention.wk.weight: (512, 512) [float32]\n",
            "layers.2.attention.wv.weight: (512, 512) [float32]\n",
            "layers.2.attention.wo.weight: (512, 512) [float32]\n",
            "layers.2.attn_norm.weight: (512,) [float32]\n",
            "layers.2.attn_norm.bias: (512,) [float32]\n",
            "layers.2.feed_forward.0.weight: (2048, 512) [float32]\n",
            "layers.2.feed_forward.2.weight: (512, 2048) [float32]\n",
            "layers.2.ff_norm.weight: (512,) [float32]\n",
            "layers.2.ff_norm.bias: (512,) [float32]\n",
            "layers.3.attention.wq.weight: (512, 512) [float32]\n",
            "layers.3.attention.wk.weight: (512, 512) [float32]\n",
            "layers.3.attention.wv.weight: (512, 512) [float32]\n",
            "layers.3.attention.wo.weight: (512, 512) [float32]\n",
            "layers.3.attn_norm.weight: (512,) [float32]\n",
            "layers.3.attn_norm.bias: (512,) [float32]\n",
            "layers.3.feed_forward.0.weight: (2048, 512) [float32]\n",
            "layers.3.feed_forward.2.weight: (512, 2048) [float32]\n",
            "layers.3.ff_norm.weight: (512,) [float32]\n",
            "layers.3.ff_norm.bias: (512,) [float32]\n",
            "fc_out.weight: (1024, 512) [float32]\n",
            "fc_out.bias: (1024,) [float32]\n"
          ]
        }
      ],
      "source": [
        "print('Parameters:')\n",
        "print('-----------' )\n",
        "for k, v in transformer_model_weights.items():\n",
        "    print(f'{k}: {v.shape} [{v.dtype}]')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "65e3edfe",
      "metadata": {
        "id": "65e3edfe"
      },
      "source": [
        "### Problem 2.1: Describe & Count the model parameters\n",
        "\n",
        "**Part (a):** Describe the role of the parameters of the model. What are the weights `wq.weight`, `wk.weight`, `wv.weight`, and `wo.weight`? What are the dimensions of these matrices? What about the role and dimensions of the feedforward weights `feed_forward.0.weight` and `feed_forward.2.weight`?\n",
        "\n",
        "You can refer to the descriptions below as well as lecture notes. Note, in particular, in the comments preceding Problem 2.4 that the attention matrices across heads are \"packed together\" when you read in the parameters in each layer.\n",
        "\n",
        "**Part (b):** Write an expression for the total number of parameters in the model, broken down by each component (Embedding Layer, Attention Layers, Feed Forward Layers, Normalization Layers, and final fully connected layer). Confirm that your answer matches the parameter count in the model weights given to you. Consult the list of parameters above and their shape to resolve any ambiguity regarding variations in Transformer architectures. Write down the expression in terms of: vocabulary size $V$, model dimension $D$, number of layers $L$, and feedforward expansion factor $F$."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ea271309",
      "metadata": {
        "id": "ea271309"
      },
      "source": [
        "[Your Markdown Here]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Problem 2.1: Describe & Count the Model Parameters\n",
        "\n",
        "---\n",
        "\n",
        "### (a) Role and Dimensions of the Main Weights\n",
        "\n",
        "Let the model dimension be $D$, number of layers $L$, number of heads $H$, and feed-forward expansion factor $F$.\n",
        "\n",
        "**Attention weights** `wq.weight`, `wk.weight`, `wv.weight`, `wo.weight` (each layer):\n",
        "\n",
        "All have shape $$(D, D).$$\n",
        "\n",
        "Their roles are:\n",
        "- $W_Q$ (`wq.weight`): projects hidden states to **queries**\n",
        "- $W_K$ (`wk.weight`): projects hidden states to **keys**\n",
        "- $W_V$ (`wv.weight`): projects hidden states to **values**\n",
        "- $W_O$ (`wo.weight`): projects concatenated multi-head outputs back to dimension $D$\n",
        "\n",
        "They implement:\n",
        "$$\n",
        "Q = X W_Q, \\quad K = X W_K, \\quad V = X W_V,\n",
        "$$\n",
        "$$\n",
        "\\text{Attention}(X) = \\text{Concat}(\\text{heads}) W_O.\n",
        "$$\n",
        "\n",
        "All $H$ heads are **packed inside** these $D \\times D$ matrices.\n",
        "\n",
        "---\n",
        "\n",
        "**Feed-forward weights**:\n",
        "\n",
        "- `feed_forward.0.weight`: shape $$(F D, D)$$  \n",
        "  Expands the hidden vector from $D$ to $F D$.\n",
        "- `feed_forward.2.weight`: shape $$(D, F D)$$  \n",
        "  Projects it back from $F D$ to $D$.\n",
        "\n",
        "The feed-forward block computes:\n",
        "$$\n",
        "\\text{FFN}(x) = W_2 \\, \\sigma(W_1 x),\n",
        "$$\n",
        "with $W_1 \\in \\mathbb{R}^{F D \\times D}$ and $W_2 \\in \\mathbb{R}^{D \\times F D}$.\n",
        "\n",
        "---\n",
        "\n",
        "### (b) Total Parameter Count\n",
        "\n",
        "Let:\n",
        "- Vocabulary size = $V$\n",
        "- Model dimension = $D$\n",
        "- Number of layers = $L$\n",
        "- Feed-forward expansion factor = $F$\n",
        "\n",
        "---\n",
        "\n",
        "**1. Embedding layer**\n",
        "$$\n",
        "\\#\\text{params}_{\\text{embed}} = V D\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "**2. Attention parameters (per layer)**  \n",
        "- 4 projection matrices: $4 D^2$  \n",
        "- LayerNorm scale and bias: $2D$\n",
        "\n",
        "$$\n",
        "\\#\\text{params}_{\\text{attn, per layer}} = 4 D^2 + 2D\n",
        "$$\n",
        "\n",
        "For all layers:\n",
        "$$\n",
        "\\#\\text{params}_{\\text{attn}} = L (4 D^2 + 2D)\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "**3. Feed-forward parameters (per layer)**  \n",
        "- Two linear maps: $2 F D^2$  \n",
        "- LayerNorm scale and bias: $2D$\n",
        "\n",
        "$$\n",
        "\\#\\text{params}_{\\text{ff, per layer}} = 2 F D^2 + 2D\n",
        "$$\n",
        "\n",
        "For all layers:\n",
        "$$\n",
        "\\#\\text{params}_{\\text{ff}} = L (2 F D^2 + 2D)\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "**4. Final output layer**\n",
        "- Weight: $V D$\n",
        "- Bias: $V$\n",
        "\n",
        "$$\n",
        "\\#\\text{params}_{\\text{fc}} = V D + V\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "### **Total Parameter Formula**\n",
        "\n",
        "$$\n",
        "\\#\\text{params}_{\\text{total}}\n",
        "= V D\n",
        "+ L (4 D^2 + 2D)\n",
        "+ L (2 F D^2 + 2D)\n",
        "+ (V D + V)\n",
        "$$\n",
        "\n",
        "Simplified:\n",
        "$$\n",
        "\\#\\text{params}_{\\text{total}}\n",
        "= 2 V D + L \\big[(4 + 2F) D^2 + 4D \\big] + V\n",
        "$$\n",
        "\n",
        "For $V = 1024,\\; D = 512,\\; L = 4,\\; F = 4$, this matches the parameter count printed from the model weights.\n"
      ],
      "metadata": {
        "id": "BYgtJM18A3iF"
      },
      "id": "BYgtJM18A3iF"
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "aba563fb",
      "metadata": {
        "id": "aba563fb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c0afd3bc-4135-4aae-833d-b9eef9c594b1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# of Parameters: 13,640,704\n"
          ]
        }
      ],
      "source": [
        "param_count = np.sum([np.prod(v.shape) for k,v in transformer_model_weights.items()])\n",
        "print(f\"# of Parameters: {param_count:,}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "80b1b0b8",
      "metadata": {
        "id": "80b1b0b8"
      },
      "source": [
        "### Tokenizer\n",
        "\n",
        "To process text with a neural network model, we need to first tokenize it in order to convert it to a numerical format that the model can understand and process. The tokenizer converts strings into sequences of integer tokens in a fixed vocabulary. There are different ways to do this. For this problem, we trained a custom [Byte-Pair Encoding](https://en.wikipedia.org/wiki/Byte_pair_encoding)  (BPE) tokenizer on Shakespeare text, setting the vocabulary size to 1024. The code below demonstrates how it works."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "f3856d12",
      "metadata": {
        "id": "f3856d12"
      },
      "outputs": [],
      "source": [
        "# load the BPE tokenizer\n",
        "with open(data_path + 'transformer_problem/bpe1024_enc_full.pkl', 'rb') as pickle_file:\n",
        "    enc = pickle.load(pickle_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "603324d0",
      "metadata": {
        "id": "603324d0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d18cea7f-64b7-4f38-ef15-fb22ef8189ef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original text:\n",
            "What's in a name? that which we call a rose\n",
            "By any other name would smell as sweet;\n",
            "\n",
            "Encoded:\n",
            "[462, 320, 307, 258, 813, 63, 323, 621, 331, 800, 258, 697, 305, 10, 889, 801, 845, 813, 504, 260, 109, 408, 366, 818, 59]\n",
            "\n",
            "Tokens: \n",
            "['What', \"'s\", ' in', ' a', ' name', '?', ' that', ' which', ' we', ' call', ' a', ' ro', 'se', '\\n', 'By', ' any', ' other', ' name', ' would', ' s', 'm', 'ell', ' as', ' sweet', ';']\n",
            "\n",
            "Decoded text:\n",
            "What's in a name? that which we call a rose\n",
            "By any other name would smell as sweet;\n"
          ]
        }
      ],
      "source": [
        "# text to tokenize\n",
        "text = \"\"\"What's in a name? that which we call a rose\n",
        "By any other name would smell as sweet;\"\"\"\n",
        "\n",
        "print('Original text:')\n",
        "print(text)\n",
        "encoded = enc.encode(text)\n",
        "print()\n",
        "\n",
        "print('Encoded:')\n",
        "print(encoded)\n",
        "print()\n",
        "\n",
        "print('Tokens: ')\n",
        "print([enc.decode([idx]) for idx in encoded])\n",
        "decoded = enc.decode(encoded)\n",
        "print()\n",
        "\n",
        "print('Decoded text:')\n",
        "print(decoded)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "af975db1",
      "metadata": {
        "id": "af975db1"
      },
      "source": [
        "### Problem 2.2: Token Embeddings\n",
        "\n",
        "After tokenization, we get a sequence of integers that represent the text to processed, with each integer index corresponding to a particular token in the vocabulary (e.g., a word or word-part). The first step in processing this text is to turn it into a vector representation. This is done via a learned embedding look-up table. For each token in the vocabulary $t \\in \\mathcal{V}$, we learn an embedding $E_t \\in \\reals^d$. A sequence of tokens $(t_1, ..., t_n)$ is transformed to a vector representation by mapping each token to its embedding $(E_{t_1}, ..., E_{t_n}) \\in \\reals^{n \\times d}$. This sequence of vectors is what the neural network model ultimately operates over."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "11b0b829",
      "metadata": {
        "id": "11b0b829"
      },
      "outputs": [],
      "source": [
        "def embed_tokens(tokens, params):\n",
        "    \"\"\"\n",
        "    Embed tokens using the input embeddings.\n",
        "\n",
        "    Args:\n",
        "        tokens (np.array): array of token indices, shape (n_tokens,)\n",
        "        params (dict): dictionary containing the model parameters\n",
        "    \"\"\"\n",
        "\n",
        "    # get needed parameters\n",
        "    embeddings = params['word_embedding.weight'] # shape (vocab_size, d_model)\n",
        "    # embeddings is a look up table for embeddings, with rows corresponding to token indices\n",
        "\n",
        "    # look up embeddings\n",
        "    # your code here\n",
        "    embedded_tokens = embeddings[tokens]  # shape (n_tokens, d_model)\n",
        "\n",
        "    return embedded_tokens\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "14fa3cf4",
      "metadata": {
        "id": "14fa3cf4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "29fa34c4-6a8b-4728-e623-7f0658dcb425"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 1.8202915 ,  0.49289942, -0.08474425, -1.4825039 ,  1.1505613 ],\n",
              "       [ 0.01529888,  0.4088627 , -1.3310498 ,  1.5467082 ,  0.7893555 ],\n",
              "       [ 1.2610923 , -0.06854534, -0.3776905 , -0.45263755,  1.1006896 ]],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ],
      "source": [
        "embed_tokens(np.array([1, 2, 3]), params=transformer_model_weights)[:3, :5]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c8416544",
      "metadata": {
        "id": "c8416544"
      },
      "source": [
        "Expected answer:\n",
        "\n",
        "```\n",
        "array([[ 1.8202915 ,  0.49289942, -0.08474425, -1.4825039 ,  1.1505613 ],\n",
        "       [ 0.01529888,  0.4088627 , -1.3310498 ,  1.5467082 ,  0.7893555 ],\n",
        "       [ 1.2610923 , -0.06854534, -0.3776905 , -0.45263755,  1.1006896 ]],\n",
        "      dtype=float32)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3db9b1c2",
      "metadata": {
        "id": "3db9b1c2"
      },
      "source": [
        "### Positional Encoding\n",
        "\n",
        "Transformer models are by-default permutation-equivariant. That is, they don't understand order or position. To make them understand positional information, we need to encode it directly in the token embeddings. One way to do this is to represent each possible position $i$ with its own position embedding ${PE}_i \\in \\mathbb{R}^{d}$, and to simpthe positional embedding representing the position of the token.\n",
        "\n",
        "In the original Transformer paper, the authors propose a particular choice for ${PE}_i \\in \\reals^{d}$ based on sines and cosines with frequencies depending on the position $i$. Since the original proposal, many follow-up works proposed different positional encoding methods aiming to improve performance and length-generalization. In this problem, we'll use the sinusoidal positional encodings of the original Transformer paper.\n",
        "\n",
        "We provide the code for computing these sinusoidal positional embeddings below. To give some intuition about the structure of the sinusoidal positional embeddings, we also plot a heatmap of the pairwise inner products $\\langle PE_i, PE_j \\rangle$. We see that that positions that are closer together have more similar positional embeddings, with additional oscillatory behavior on top of that."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a33552bc",
      "metadata": {
        "id": "a33552bc"
      },
      "source": [
        "### Problem 2.3: Describe the positional embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "544c34a7",
      "metadata": {
        "id": "544c34a7"
      },
      "source": [
        "Below is an implementation of the positional embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5f07281e",
      "metadata": {
        "id": "5f07281e"
      },
      "outputs": [],
      "source": [
        "def get_sinusoidal_positional_embeddings(sequence_length, dim, base=10000):\n",
        "    inv_freq = 1.0 / (base ** (np.arange(0, dim, 2) / dim))\n",
        "    t = np.arange(sequence_length)\n",
        "    sinusoid_inp = np.einsum(\"i,j->ij\", t, inv_freq)\n",
        "\n",
        "    sin, cos = np.sin(sinusoid_inp), np.cos(sinusoid_inp)\n",
        "\n",
        "    emb = np.concatenate((sin, cos), axis=-1)\n",
        "    return emb"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f36259d6",
      "metadata": {
        "id": "f36259d6"
      },
      "source": [
        "Explore the positional embeddings by computing the inner-product between all pairs of positional embeddings, and then plotting the similarity matrix as a heat map. Do the results make sense? What are the positional embeddings designed to model? Do they do this effectively? Comment below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "49ff03b0",
      "metadata": {
        "id": "49ff03b0"
      },
      "outputs": [],
      "source": [
        "pe = get_sinusoidal_positional_embeddings(128, d_model)\n",
        "\n",
        "#Your code here\n",
        "pe_similarity = ...\n",
        "sns.heatmap(pe_similarity)\n",
        "plt.title('Similarity Structure of Sinusoidal Positional Embeddings')\n",
        "plt.xlabel('Position')\n",
        "plt.ylabel('Position')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1f98808d",
      "metadata": {
        "id": "1f98808d"
      },
      "source": [
        "[Your markdown here]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "06ea1c4f",
      "metadata": {
        "id": "06ea1c4f"
      },
      "source": [
        "### Multi-Head Attention\n",
        "\n",
        "The core operation in a Transformer is multi-head attention, sometimes called self-attention. In this problem, we will implement multi-head attention in numpy from scratch, given the trained parameters of the model.\n",
        "\n",
        "The input is a sequence of vectors $X = (x_1, ..., x_n) \\in \\reals^{n \\times d_{model}}$. For each attention head $h \\in [n_h]$, the parameters consist of a query projection matrix $W_q^h \\in \\reals^{d_{model} \\times d_h}$, a key projection matrix $W_k^h \\in \\reals^{d_{model} \\times d_h}$, and a value projection matrix $W_k^h \\in \\reals^{d_{model} \\times d_h}$. Here, $d_h$ is the \"head dimension\", taken to be $d_{model} / n_h$ (to maintain the same dimensionality between the input and output). The algorithm, for each head, is the following:\n",
        "1. Compute the queries $Q = X W_q^h$, keys $K = X W_k^h$, and values $V = X W_v^h$. Note that the linear maps are applied independently for each token across the embedding dimension (not sequence dimension), such that $Q, K, V \\in \\reals^{n \\times d_h}$.\n",
        "2. Compare the queries and keys via inner products to get an $n \\times n$ attention matrix $A = \\mathrm{Softmax}(Q K^{\\intercal}) \\in \\reals^{n \\times n}$.\n",
        "3. Use the attention scores $A$ to select values, producing the output of the self-attention head: $\\mathrm{head}_h = A V \\in \\reals^{n \\times d_h}$.\n",
        "We then concatenate the retrieved values across all heads, and apply a final linear map. Putting this all together yields:\n",
        "$$\n",
        "\\begin{align}\n",
        "    \\mathrm{head}_h &= \\mathrm{Softmax}((X W_q^h) (X W_k^h)^{\\intercal}) X W_v^h\\\\\n",
        "    \\mathrm{MultiHeadAttention}(X) &= \\mathrm{concat}(\\mathrm{head}_1, ..., \\mathrm{head}_{n_h}) W_o\\\\\n",
        "\\end{align}\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c4ef44d4",
      "metadata": {
        "id": "c4ef44d4"
      },
      "source": [
        "### Problem 2.4: Implement multi-head attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e80ba0d4",
      "metadata": {
        "id": "e80ba0d4"
      },
      "outputs": [],
      "source": [
        "# first, implement softmax and causal masking\n",
        "def softmax(x, axis=-1):\n",
        "\n",
        "    # your code here\n",
        "\n",
        "    return ...\n",
        "\n",
        "def apply_presoftmax_causal_mask(attn_scores):\n",
        "    # apply a causal mask to the attention scores (set the entries above the diagonal to -inf)\n",
        "\n",
        "    # your code here\n",
        "\n",
        "    return ..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b2bb2cec",
      "metadata": {
        "id": "b2bb2cec"
      },
      "outputs": [],
      "source": [
        "# check your implementation\n",
        "x = np.ones((4,4))\n",
        "softmax_x = softmax(x, axis=1)\n",
        "print(\"Softmax:\\n\", softmax_x)\n",
        "\n",
        "# apply_presoftmax_causal_mask test\n",
        "attn_scores = np.ones((4,4))\n",
        "masked_scores = apply_presoftmax_causal_mask(attn_scores)\n",
        "print(\"Masked Attention Scores:\\n\", masked_scores)\n",
        "\n",
        "masked_softmax = softmax(masked_scores, axis=-1)\n",
        "print(\"Masked Softmax:\\n\", masked_softmax)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "06fd01b7",
      "metadata": {
        "id": "06fd01b7"
      },
      "source": [
        "Expected results:\n",
        "```\n",
        "Softmax:\n",
        " [[0.25 0.25 0.25 0.25]\n",
        " [0.25 0.25 0.25 0.25]\n",
        " [0.25 0.25 0.25 0.25]\n",
        " [0.25 0.25 0.25 0.25]]\n",
        "Masked Attention Scores:\n",
        " [[  1. -inf -inf -inf]\n",
        " [  1.   1. -inf -inf]\n",
        " [  1.   1.   1. -inf]\n",
        " [  1.   1.   1.   1.]]\n",
        "Masked Softmax:\n",
        " [[1.         0.         0.         0.        ]\n",
        " [0.5        0.5        0.         0.        ]\n",
        " [0.33333333 0.33333333 0.33333333 0.        ]\n",
        " [0.25       0.25       0.25       0.25      ]]\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "39e1ad32",
      "metadata": {
        "id": "39e1ad32"
      },
      "outputs": [],
      "source": [
        "\n",
        "def multi_head_attention(x, params, layer_prefix='layers.0'):\n",
        "    \"\"\"\n",
        "    Compute multi-head self-attention.\n",
        "\n",
        "    Args:\n",
        "        x (np.array): input tensor, shape (n, d_model)\n",
        "        params (dict): dictionary containing the model parameters\n",
        "        layer_prefix (str): prefix of parameter names corresponding to the layer\n",
        "        verbose (bool): whether to print intermediate shapes\n",
        "    \"\"\"\n",
        "\n",
        "    # get parameters of multi-head attention layer\n",
        "    wq = params[f'{layer_prefix}.attention.wq.weight'].T # (d_model, d_model)\n",
        "    wk = params[f'{layer_prefix}.attention.wk.weight'].T # (d_model, d_model)\n",
        "    wv = params[f'{layer_prefix}.attention.wv.weight'].T # (d_model, d_model)\n",
        "    wo = params[f'{layer_prefix}.attention.wo.weight'].T # (d_model, d_model)\n",
        "\n",
        "    head_dim = d_model // n_heads # dimension of each head\n",
        "    attn_scale = 1 / math.sqrt(head_dim) # scaling factor for attention scores\n",
        "\n",
        "    # the wq, wk, wv, wo matrices contain weights for all heads, concatenated\n",
        "    # first, we split wq, wk, wv, wo into heads\n",
        "    # note: there are more efficient implementations, but this is more verbose/pedagogical\n",
        "    wq = wq.reshape(d_model, n_heads, head_dim).transpose(1, 0, 2) # (n_heads, d_model, head_dim)\n",
        "    wk = wk.reshape(d_model, n_heads, head_dim).transpose(1, 0, 2) # (n_heads, d_model, head_dim)\n",
        "    wv = wv.reshape(d_model, n_heads, head_dim).transpose(1, 0, 2) # (n_heads, d_model, head_dim)\n",
        "\n",
        "    head_outputs = []\n",
        "    for head in range(n_heads):\n",
        "\n",
        "        # get head-specific parameters (these are the query/key/value projections for this head)\n",
        "        # your code ehre\n",
        "        wqh = wq[head] # (d_model, head_dim)\n",
        "        wkh = wk[head] # (d_model, head_dim)\n",
        "        wvh = wv[head] # (d_model, head_dim)\n",
        "\n",
        "        # compute queries, keys, values\n",
        "        # your code here\n",
        "        q = ... # (n, head_dim)\n",
        "        k = ... # (n, head_dim)\n",
        "        v = ... # (n, head_dim)\n",
        "\n",
        "        # compute attention scores\n",
        "        # your code here\n",
        "        # compute dot product\n",
        "        attn_scores = ... # (n, n)\n",
        "        # multiply by scaling factor\n",
        "        attn_scores = attn_scores * attn_scale # (n, n)\n",
        "\n",
        "        # apply causal mask\n",
        "        attn_scores = ... # (n, n)\n",
        "        # apply softmax\n",
        "        attn_scores = ... # (n, n)\n",
        "\n",
        "        # apply attention scores to values\n",
        "        # your code here\n",
        "        head_out = ... # (n, head_dim)\n",
        "\n",
        "        # store the head output\n",
        "        head_outputs.append(head_out)\n",
        "\n",
        "    # concatenate all head outputs\n",
        "    head_outputs = ... # (n, d_model)\n",
        "\n",
        "    # apply output linear map W_o to concatenated head outputs\n",
        "    # your code here\n",
        "    output = ... # (n, d_model)\n",
        "\n",
        "    return output"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9d4747b5",
      "metadata": {
        "id": "9d4747b5"
      },
      "source": [
        "#### Test your Attention implementation\n",
        "\n",
        "To test if you have the correct implementation, you can run the following\n",
        "test line. We show the expected output if your implementation is correct."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0ae5584c",
      "metadata": {
        "id": "0ae5584c"
      },
      "outputs": [],
      "source": [
        "multi_head_attention(np.ones((block_size, d_model)), transformer_model_weights)[:3, :5]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7b9237d4",
      "metadata": {
        "id": "7b9237d4"
      },
      "source": [
        "Expected output:\n",
        "\n",
        "```\n",
        "array([[-0.61194002, -0.31581021,  0.17525476, -0.03176344,  0.23671877],\n",
        "       [-0.61194002, -0.31581021,  0.17525476, -0.03176344,  0.23671877],\n",
        "       [-0.61194002, -0.31581021,  0.17525476, -0.03176344,  0.23671877]])\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2d4cc0b0",
      "metadata": {
        "id": "2d4cc0b0"
      },
      "source": [
        "### MLP\n",
        "\n",
        "Each Transformer layer (i.e., block) consists of two operations: 1) (multi-head) self-attention, which enables exchange of information between tokens, and 2) a multi-layer perceptron, which processes each token independently. A Transformer model is essentially just alternating between these two operations. In this problem, we will implement the multi-layer perceptron step. Typically, the MLP at each layer is simply a two-layer (one hidden layer) MLP or Feed Forward Network. In our model, we use a ReLU activation in the hidden layer, though other activations are possible. The same MLP network is applied to each token embedding in the sequence independently.\n",
        "\n",
        "Given $X = (x_1, ..., x_n) \\in \\reals^{n \\times d_{model}}$, we apply the MLP as follows:\n",
        "\n",
        "$$\\mathrm{MLP}(X) = \\mathrm{ReLU}(X W_1) W_2$$\n",
        "\n",
        "Note that we don't use biases for simplicity."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b3b77441",
      "metadata": {
        "id": "b3b77441"
      },
      "source": [
        "### Problem 2.5: Implement the MLP\n",
        "\n",
        "Next, we need to apply the multi-layer perceptron in each layer. Complete the implementation below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7f8c36a4",
      "metadata": {
        "id": "7f8c36a4"
      },
      "outputs": [],
      "source": [
        "def relu(x):\n",
        "    return np.maximum(x, 0)\n",
        "\n",
        "def mlp(x, params, layer_prefix='layers.0'):\n",
        "    # get MLP parameters\n",
        "    w1 = params[f'{layer_prefix}.feed_forward.0.weight'].T # (d_model, d_ff)\n",
        "    w2 = params[f'{layer_prefix}.feed_forward.2.weight'].T # (d_ff, d_model)\n",
        "\n",
        "    # Your code here\n",
        "    o = ... # (n, d_model)\n",
        "\n",
        "    return o"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a4a42184",
      "metadata": {
        "id": "a4a42184"
      },
      "source": [
        "#### Test your MLP implementation\n",
        "\n",
        "To test if you have the correct MLP implementation, you can run the following\n",
        "test line. We show the expected output if your implementation is correct."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e9425352",
      "metadata": {
        "id": "e9425352"
      },
      "outputs": [],
      "source": [
        "mlp(np.ones((block_size, d_model)), transformer_model_weights)[:3, :5]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7b08660e",
      "metadata": {
        "id": "7b08660e"
      },
      "source": [
        "Expected output:\n",
        "\n",
        "```\n",
        "array([[ 0.36650751, -0.02109491,  0.13528369, -0.0459696 ,  0.2035102 ],\n",
        "       [ 0.36650751, -0.02109491,  0.13528369, -0.0459696 ,  0.2035102 ],\n",
        "       [ 0.36650751, -0.02109491,  0.13528369, -0.0459696 ,  0.2035102 ]])\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "14897e04",
      "metadata": {
        "id": "14897e04"
      },
      "source": [
        "### Problem 2.6: Implement Layer Normalization\n",
        "\n",
        "Layer Normalization is a technique that normalizes the inputs across the features of a layer to stabilize and accelerate training in deep neural networks. It rescales activations to have zero mean and unit variance *within* each layer, then applies a learned gain and shift. In Transformers, LayerNorm is applied before or after sublayers (like attention and feedforward blocks), stabilizing training and maintaining consistent scaling across tokens and layers.\n",
        "\n",
        "In this problem, we will implement layer normalization. For a vector of activations $x \\in \\reals^d$, layer normalization returns the normalized feature vector\n",
        "$$\\mathrm{LayerNorm}(x) = \\frac{x - E[X]}{\\sqrt{\\mathrm{Var}[x] + \\epsilon}} * \\gamma + \\beta,$$\n",
        "where $\\gamma \\in \\reals^d$ is the weight parameter, and $\\beta \\in \\reals^d$ is the bias parameter. Here, $E[\\cdot]$ and $\\mathrm{Var}[\\cdot]$ indicates the mean and variance over the embedding dimension $d_{\\mathrm{model}}$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ed2acc24",
      "metadata": {
        "id": "ed2acc24"
      },
      "outputs": [],
      "source": [
        "def layer_norm(x, params, layer_prefix='layers.0', norm_type='attn_norm', norm_eps=1e-5):\n",
        "    # get layer norm params\n",
        "    weight = params[f'{layer_prefix}.{norm_type}.weight'] # (d_model,)\n",
        "    bias = params[f'{layer_prefix}.{norm_type}.bias'] # (d_model,)\n",
        "\n",
        "    # your code here\n",
        "    ...\n",
        "\n",
        "    return normed_x"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2dcb4f90",
      "metadata": {
        "id": "2dcb4f90"
      },
      "source": [
        "#### Test your LayerNorm implementation\n",
        "\n",
        "To test if you have the correct LayerNorm implementation, you can run the following test line. We show the expected output if your implementation is correct."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "db015561",
      "metadata": {
        "id": "db015561"
      },
      "outputs": [],
      "source": [
        "layer_norm(np.arange(block_size * d_model).reshape(block_size, d_model), transformer_model_weights, layer_prefix='layers.0', norm_type='attn_norm')[:3, :5]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9a7263c3",
      "metadata": {
        "id": "9a7263c3"
      },
      "source": [
        "Expected Output:\n",
        "```\n",
        "array([[-1.76364724, -1.75155978, -1.7455241 , -1.74605486, -1.72934376],\n",
        "       [-1.76364724, -1.75155978, -1.7455241 , -1.74605486, -1.72934376],\n",
        "       [-1.76364724, -1.75155978, -1.7455241 , -1.74605486, -1.72934376]])\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "84a63140",
      "metadata": {
        "id": "84a63140"
      },
      "source": [
        "### Problem 2.7: Exploring Normalization in Residual Architectures\n",
        "\n",
        "Transformers are a type of *residual network*. At each layer, the embeddings of each token are iteratively refined by adding the result of a non-linear learnable transformation (either attention or MLP). In general, a residual architecture has the form\n",
        "$$x^{(\\ell + 1)} = x^{(\\ell)} + \\mathcal{F}(x^{(\\ell)}),$$\n",
        "where $x^{(\\ell)}$ is the internal representation at layer $\\ell$ and $\\mathcal{F}(\\cdot)$ is a learned transformation. You can think of $\\mathcal{F}(\\cdot)$ as modeling the *residual* $x^{(\\ell)} \\mapsto x^{(\\ell + 1)} - x^{(\\ell)}$ rather than needing to directly model the full $x^{(\\ell)} \\mapsto x^{(\\ell + 1)}$ map. This improves training stability and mitigates vanishing gradient problems.\n",
        "\n",
        "There are different ways that normalization can be applied to residual networks. The form above includes no normalization.\n",
        "\n",
        "In the original Transformer paper, the authors use so-called \"post-normalization\", where the normalization is applied *after* the residual block:\n",
        "$$x^{(\\ell + 1)} = \\mathrm{LayerNorm}(x^{(\\ell)} + \\mathcal{F}(x^{(\\ell)})).$$\n",
        "\n",
        "In modern architectures, it is more common to use \"pre-normalization\", where the normalization before the learned transformation:\n",
        "$$x^{(\\ell + 1)} = x^{(\\ell)} + \\mathcal{F}(\\mathrm{LayerNorm}(x^{(\\ell)})).$$\n",
        "\n",
        "In this problem, we will explore the effects of normalization on the magnitude of the feature vectors at each layer. Complete the code below to implement each form of normalization: 1) no norm, 2) post-norm, 3) pre-norm. Plot the average embedding norm across layers for each form of normalization. Interpret the results. Does this shed light onto why normalization is a useful empirical technique to stabilize and accelerate training?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "baa42b2c",
      "metadata": {
        "id": "baa42b2c"
      },
      "outputs": [],
      "source": [
        "# Experiment: compare NoNorm, Pre-LN, Post-LN for a stack of feedforward blocks\n",
        "np.random.seed(0)\n",
        "\n",
        "seq_len = block_size\n",
        "d_model = D\n",
        "d_ff = F * D\n",
        "n_layers_exp = 24\n",
        "\n",
        "# shared random input X0\n",
        "X0 = np.random.randn(seq_len, d_model)\n",
        "\n",
        "def simple_layer_norm(x, eps=1e-5):\n",
        "    # for purposes of exploration, implement a simple layer norm with weight = 1 and bias = 0\n",
        "\n",
        "    # your code here\n",
        "    ...\n",
        "\n",
        "    return norm_x\n",
        "\n",
        "# random FFN blocks to play the role of $\\mathcal{F}$\n",
        "# initialize independent W1/W2 per layer\n",
        "W1_list = [np.random.randn(d_model, d_ff) * (1.0 / np.sqrt(d_model)) for _ in range(n_layers_exp)]\n",
        "W2_list = [np.random.randn(d_ff, d_model) * (1.0 / np.sqrt(d_ff)) for _ in range(n_layers_exp)]\n",
        "\n",
        "def ffn(x, W1, W2):\n",
        "    return relu(np.dot(x, W1)).dot(W2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "59b7772f",
      "metadata": {
        "id": "59b7772f"
      },
      "outputs": [],
      "source": [
        "# complete the implementation\n",
        "def run_scheme(scheme_name):\n",
        "    x = X0.copy()\n",
        "    norms = [np.mean(np.linalg.norm(x, axis=1))]  # layer 0\n",
        "    for i in range(n_layers_exp):\n",
        "        W1, W2 = W1_list[i], W2_list[i]\n",
        "        # residual without normalization\n",
        "        if scheme_name == 'no_norm':\n",
        "            x = ... # your code here\n",
        "\n",
        "        # apply \"pre-norm\" normalization\n",
        "        elif scheme_name == 'pre_ln':\n",
        "            x = ... # your code here\n",
        "\n",
        "        # apply \"post-norm\" normalization\n",
        "        elif scheme_name == 'post_ln':\n",
        "            x = ... # your code here\n",
        "        else:\n",
        "            raise ValueError(scheme_name)\n",
        "\n",
        "        # compute norm of each token embedding at current layer\n",
        "        avg_norm_this_layer = np.mean(np.linalg.norm(x, axis=1)) # your code here\n",
        "        norms.append(avg_norm_this_layer)\n",
        "    return np.array(norms)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "98e20af4",
      "metadata": {
        "id": "98e20af4"
      },
      "outputs": [],
      "source": [
        "\n",
        "schemes = ['no_norm', 'pre_ln', 'post_ln']\n",
        "results = {s: run_scheme(s) for s in schemes}\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(6,4))\n",
        "layers = np.arange(0, n_layers_exp + 1)\n",
        "plt.plot(layers, results['no_norm'], marker='o', label='No Norm')\n",
        "plt.plot(layers, results['pre_ln'], marker='o', label='Pre-LayerNorm')\n",
        "plt.plot(layers, results['post_ln'], marker='o', label='Post-LayerNorm')\n",
        "plt.xlabel('Layer index')\n",
        "plt.ylabel('Mean token L2 norm')\n",
        "plt.title('Residual stream norm vs depth: No Norm / Pre-LN / Post-LN')\n",
        "plt.xticks(layers)\n",
        "plt.grid(alpha=0.3)\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "90f3730f",
      "metadata": {
        "id": "90f3730f"
      },
      "source": [
        "[Your Markdown Here]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ac85b5cb",
      "metadata": {
        "id": "ac85b5cb"
      },
      "source": [
        "### Final Prediction Layer\n",
        "\n",
        "A Transformer model iteratively applies multi-head attention and MLP layers to process the input. This produces a processed representation of shape $n \\times d_{model}$. To make the final prediction (e.g., predict the next token), we need to map the $d_{model}$-dimensional embedding vectors to logits over the output vocabulary. To do this, we simply apply a linear map that maps from $d_{model}$ to $\\mathtt{vocab\\_size}$.\n",
        "\n",
        "The starter code for this is given below; you need to complete it."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "03bc5f78",
      "metadata": {
        "id": "03bc5f78"
      },
      "source": [
        "### Problem 2.8: Implement the prediction layer as logits."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5646afc5",
      "metadata": {
        "id": "5646afc5"
      },
      "outputs": [],
      "source": [
        "def prediction_head(x, params):\n",
        "    # get needed parameters\n",
        "    w = params['fc_out.weight'].T # (d_model, vocab_size)\n",
        "    b = params['fc_out.bias'] # (vocab_size,)\n",
        "\n",
        "    # Your code here\n",
        "    logits = ... # (n, vocab_size)\n",
        "\n",
        "    return logits"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ebb2f111",
      "metadata": {
        "id": "ebb2f111"
      },
      "source": [
        "### Test the prediction head"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7e515dac",
      "metadata": {
        "id": "7e515dac"
      },
      "outputs": [],
      "source": [
        "prediction_head(np.ones((block_size, d_model)), transformer_model_weights)[:3, :5]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c9aac831",
      "metadata": {
        "id": "c9aac831"
      },
      "source": [
        "```\n",
        "array([[-1.52472634, -0.92704511, -0.07956709, -0.27001108, -0.2583226 ],\n",
        "       [-1.52472634, -0.92704511, -0.07956709, -0.27001108, -0.2583226 ],\n",
        "       [-1.52472634, -0.92704511, -0.07956709, -0.27001108, -0.2583226 ]])\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6453d59e",
      "metadata": {
        "id": "6453d59e"
      },
      "source": [
        "### Putting it all together: A Full Transformer Language Model\n",
        "\n",
        "We are now ready to put this all together to assemble our Transformer Language Model. Recall that the Transformer architecture consists of iteratively applying multi-head attention and MLPs. Each time we apply attention or the MLP, we also apply a *residual connection*: $X^{(\\ell + 1)} = X^{(\\ell)} + F(X^{(\\ell)})$. This can be interpreted as a mechanism to enable easy communication between different layers (some people call refer to this idea as the \"residual stream\"). Real Transformers also include layer normalization in each layer, but we omit this for simplicity in this problem.\n",
        "\n",
        "The full algorithm is given below:\n",
        "1. Embed the tokens using the embedding lookup table: $(t_1, ..., t_n) \\mapsto (E_{t_1}, ..., E_{t_n}) =: X^{(0)}$\n",
        "2. Add the positional embeddings: $X^{(0)} \\gets X^{(0)} + (PE_1, ..., PE_n)$\n",
        "3. For each layer $\\ell = 1, ..., L$:\n",
        "    1. Apply Multi-Head Attention: $\\tilde{X}^{(\\ell)} \\gets X^{(\\ell-1)} + \\mathrm{MultiHeadAttention}(\\mathrm{LayerNorm}(X^{(\\ell-1)}))$.\n",
        "    2. Apply the MLP: $X^{(\\ell)} \\gets \\tilde{X}^{(\\ell)} + \\mathrm{MLP}(\\mathrm{LayerNorm}(\\tilde{X}^{(\\ell)}))$.\n",
        "4. Compute the logits"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1a7193e8",
      "metadata": {
        "id": "1a7193e8"
      },
      "source": [
        "### Problem 2.9: Complete the implementation\n",
        "\n",
        "Complete the starter code below, which takes embeddings, adds positional encoding,\n",
        "and then adds the attention and MLP components to each layer. Remember that\n",
        "everything is added together, with the computations in one layer added to the outputs of the previous layer, forming the \"residual stream\".\n",
        "\n",
        "Hint: to complete the implementation, you will need to use the `layer_norm`, `multi_head_attention`, `mlp`, and `prediction_head` functions you implemented above. Recall that these functions take `param` as input (which are the weights of the pre-trained model). They use the `layer_prefix` argument to fetch the weights at the given layer. For example, `layer_prefix=f'layers.{i}'` will the corresponding module for layer `i`. Additionally, since each layer has two LayerNorms, one for the attention module and one for the MLP module, the `layer_norm` function additionally takes `norm_type` argument, which can be `'attn_norm'` or `'ff_norm'`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "abd9dccb",
      "metadata": {
        "id": "abd9dccb"
      },
      "outputs": [],
      "source": [
        "def transformer(tokens, params):\n",
        "    # tokens: (n,) integer array\n",
        "    # params: dictionary of parameters\n",
        "\n",
        "    # map tokens to embeddings using embed_tokens\n",
        "    x = embed_tokens(tokens, params) # (n, d_model)\n",
        "\n",
        "    # add positional embeddings\n",
        "    pe = get_sinusoidal_positional_embeddings(x.shape[0], x.shape[1]) # (n, d_model)\n",
        "    # your code here\n",
        "    x += pe # (n, d_model)\n",
        "\n",
        "    # transformer blocks\n",
        "    for i in range(n_layers):\n",
        "\n",
        "        # compute multi-head self-attention and add residual with pre-normalization\n",
        "        # your code here\n",
        "        normed_x = ... # (n, d_model)\n",
        "        attn_out = ... # (n, d_model)\n",
        "\n",
        "        # residual connection\n",
        "        x = ... # (n, d_model)\n",
        "\n",
        "        # compute MLP and add residual with pre-normalization\n",
        "        # your code here\n",
        "        normed_x = ... # (n, d_model)\n",
        "        mlp_out = ... # (n, d_model)\n",
        "        # residual connection\n",
        "        x = ... # (n, d_model)\n",
        "\n",
        "    # compute logits via the prediction_head\n",
        "    # your code here\n",
        "    logits = ... # (n, vocab_size)\n",
        "\n",
        "    return logits"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fafbacc5",
      "metadata": {
        "id": "fafbacc5"
      },
      "source": [
        "### Test your implementation\n",
        "\n",
        "You can check your implementation against the expected output below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a1d6b0e1",
      "metadata": {
        "id": "a1d6b0e1"
      },
      "outputs": [],
      "source": [
        "transformer([0, 1, 2], params=transformer_model_weights)[:3, :5]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3d05fa35",
      "metadata": {
        "id": "3d05fa35"
      },
      "source": [
        "Expected Output:\n",
        "\n",
        "```\n",
        "array([[-3.30795902, -1.94197185, -0.67287023, -1.7162725 , -1.541589  ],\n",
        "       [-4.15893294, -1.8336369 , -2.26268609, -2.63614073, -2.48212268],\n",
        "       [-3.51487274, -2.31248397, -3.52562574, -3.18057025, -1.76104193]])\n",
        "``````"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cceff165",
      "metadata": {
        "id": "cceff165"
      },
      "source": [
        "### Generate some text\n",
        "\n",
        "Below, we provide some code for generating text from a Transformer language model. The sampling procedure is *autoregressive*. This means that we input some text to the model and it outputs a distribution over next tokens. We sample the next token and append it to the text, then repeat the procedure."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f4dcb151",
      "metadata": {
        "id": "f4dcb151"
      },
      "source": [
        "### Problem 2.10: Complete the next token generator\n",
        "\n",
        "Complete the next token generator, but filling in the missing code below. This uses \"temperature\" to focus on the more probable tokens in a given context (as the temperature decreases)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1583451e",
      "metadata": {
        "id": "1583451e"
      },
      "outputs": [],
      "source": [
        "def generate_with_transformer(prefix_text, params, max_len=128, greedy=False, temperature=0.9):\n",
        "    # encode seed text\n",
        "    prefix_tokens = list(enc.encode(prefix_text))\n",
        "\n",
        "    # initialize generated tokens\n",
        "    generated_tokens = prefix_tokens\n",
        "\n",
        "    # generate new tokens\n",
        "    for i in range(max_len):\n",
        "        # predict next token\n",
        "        logits = transformer(generated_tokens, params)\n",
        "        # hint: logits[-1] corresponds to prediction of the next token\n",
        "        if greedy:\n",
        "            # Your code here\n",
        "            next_token = ...\n",
        "        else:\n",
        "            # Your code here\n",
        "            next_token = ...\n",
        "\n",
        "        # add next token to generated tokens\n",
        "        generated_tokens.append(next_token)\n",
        "\n",
        "    # This converts the tokens to text, using the tiktoken decoder:\n",
        "    generated_text = enc.decode(generated_tokens)\n",
        "\n",
        "    return generated_text"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0c564fc9",
      "metadata": {
        "id": "0c564fc9"
      },
      "source": [
        "### Test your implementation by generating text. You're the Bard!\n",
        "\n",
        "Use your implementation to generate text according to the model. Generate text at different temperatures. Do the results make sense? Comment on the quality of the model. What changes to the model would lead to better results? Comment in the Markdown cell below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2e61d567",
      "metadata": {
        "id": "2e61d567"
      },
      "outputs": [],
      "source": [
        " prefix_text = \"\"\"STUDENT:\n",
        " O though Transformer, wrought of mind and code,\n",
        " Thou art the loom where language weaveth thought!\n",
        " Each token, like a spark of meaning sown,\n",
        " Attendeth all, and all attend to one.\"\"\"\n",
        "\n",
        "#prefix_text = \"\"\"HAMLET:\n",
        "#To be, or not to be: that is the question\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8fe61599",
      "metadata": {
        "id": "8fe61599"
      },
      "outputs": [],
      "source": [
        "generated_text = generate_with_transformer(prefix_text, transformer_model_weights, greedy=True, max_len=128)\n",
        "print(generated_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c00cffe4",
      "metadata": {
        "id": "c00cffe4"
      },
      "outputs": [],
      "source": [
        "generated_text = generate_with_transformer(prefix_text, transformer_model_weights,\n",
        "    greedy=False, temperature=0.8, max_len=128)\n",
        "print(generated_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "117f5643",
      "metadata": {
        "id": "117f5643"
      },
      "source": [
        "[Your markdown here]"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}