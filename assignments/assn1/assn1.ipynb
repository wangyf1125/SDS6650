{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "74xNbvP5dfxm"
      },
      "source": [
        "## Intermediate Machine Learning: Assignment 1\n",
        "\n",
        "**Deadline**\n",
        "\n",
        "Assignment 1 is due Thursday, September 25 11:59 pm. Late work will not be accepted as per the course policies (see the syllabus on Canvas).\n",
        "\n",
        "Directly sharing answers is not okay, but discussing problems with the course staff or with other students is encouraged. Acknowledge any use of an AI system such as ChatGPT or Copilot.\n",
        "\n",
        "You should start early so that you have time to get help if you're stuck. The drop-in office hours schedule can be found on Canvas. You can also post questions or start discussions on Ed Discussion. The assignment may look long at first glance, but the problems are broken up into steps that should help you to make steady progress.\n",
        "\n",
        "**Submission**\n",
        "\n",
        "Submit your assignment as a pdf on Gradescope. You can access Gradescope through Canvas on the left-side of the class home page. The problems in each homework assignment are numbered. Note: When submitting on Gradescope, please select the correct pages of your pdf that correspond to each problem. This will allow graders to more easily find your complete solution to each problem.\n",
        "\n",
        "To produce the pdf on Colab, you can simply use File -> Print (Cmd/Ctrl+P). Please be sure that all of your work appears in the output, and that long lines are not truncated in any cell.\n",
        "\n",
        "**Topics**\n",
        "\n",
        " * Lasso\n",
        " * Bias-variance decomposition\n",
        " * Mercer kernels\n",
        " * LOOCV for kernel smoothing\n",
        "\n",
        "This assignment will also help to solidify your Python and Jupyter notebook skills.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4fSdGuUtdfxn"
      },
      "source": [
        "### Problem 1: Roping variables with the lasso (15 points)\n",
        "\n",
        "In this exercise, we'll employ the lasso regression technique to identify key predictor variables from the diabetes dataset. This dataset is used in the study by Efron, Hastie, Johnstone, and Tibshirani (2004, *Annals of Statistics*, \"Least Angle Regression\"). The primary goal is to predict a quantitative measure of diabetes progression one year after baseline, based on ten standardized physiological and biochemical measurements.\n",
        "\n",
        "\n",
        "The *Diabetes* dataset in the `sklearn.datasets` package is a classic benchmark for regression. It contains 442 samples with 10 features that have been centered to mean zero but not scaled to unit variance. The predictors include age, sex, body mass index (BMI), blood pressure (BP), and six blood serum measures (s1â€“s6: cholesterol, LDL, HDL, cholesterol/HDL ratio, triglycerides, and blood sugar). The target is a quantitative measure of diabetes progression one year after baseline. You can find details at https://scikit-learn.org/stable/datasets/toy_dataset.html#diabetes-dataset.\n",
        "\n",
        "**Your tasks are as follows**:\n",
        "\n",
        "1. **Plotting Lasso Paths**: Generate a visualization of the lasso regularization paths.\n",
        "2. **Identifying Key Predictors**: Determine which coefficients of \\( \\beta \\) are non-zero.\n",
        "3. **Estimating Coefficients**: Provide the best estimate for these non-zero coefficients.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "blIiSTaGdfxo"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import pickle\n",
        "from sklearn.linear_model import Lasso\n",
        "from sklearn.datasets import load_diabetes\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jIPY5hyedfxo"
      },
      "source": [
        "Just run the next cell to read in the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wlMESAOYdfxp"
      },
      "outputs": [],
      "source": [
        "X, y = load_diabetes(return_X_y=True)\n",
        "n, p = X.shape\n",
        "print(\"Number of rows: {}\".format(n))\n",
        "print(\"Number of columns: {}\".format(p))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wp13u91udfxp"
      },
      "source": [
        "### 1.1: Lasso regularization paths\n",
        "\n",
        "Run the lasso and plot the regularization paths. You can use the `Lasso` class from the `sklearn.linear_model` package. Plot the parameter paths with the regularization level $\\lambda$ (`alpha` in the code) on the log-scale, as done in the lasso demo code from class. (As always, be sure to label your axes.)\n",
        "\n",
        "Show two plots, one where you run the lasso on the variables as given in the dataset, another where you standardize the variables to have mean zero and standard deviation one. Describe the differences in the regularization paths, and explain those differences.\n",
        "\n",
        "When the predictors are standardized, what order do they appear in the lasso fits? That is, as $\\lambda$ decreases from infinity to zero, what is the sequence of variables that enter the model with nonzero coefficients? Explain why this ordering may (or may not) make sense.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": false,
        "id": "hCFWNRMtdfxp"
      },
      "outputs": [],
      "source": [
        "# your code and markdown here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HHBAe8E9dfxp"
      },
      "source": [
        "### 1.2: Select, estimate, and predict\n",
        "\n",
        "The true model is linear, and only a subset $S \\subset \\{0,1,\\ldots, 49\\}$ of the 50 variables have non-zero coefficients $\\beta_j$. In this problem you should make three estimates:\n",
        "\n",
        "1. An estimate $\\hat S$ of $S$\n",
        "2. An estimate $\\hat \\beta_j$ for each $j\\in \\hat S$\n",
        "3. An estimate of the predictive risk ${\\mathbb E}(Y - X\\hat\\beta)^2$\n",
        "\n",
        "\n",
        "We are not specifying how you should construct these estimates. You should use your judgement, taste, and\n",
        "the tools provided from class. However, you must clearly explain and justify whatever approach that you use.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jSA_BBQidfxp"
      },
      "outputs": [],
      "source": [
        "# your code (and Markdown if needed) here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3AhLoBVvdfxp"
      },
      "source": [
        "### Problem 2: Risky business (10 points)\n",
        "\n",
        "In class [(and in these notes)](https://github.com/YData123/sds365-fa22/raw/main/notes/kernel-bias-variance.pdf) we sketched a proof that, when the regression function has two bounded derivatives,\n",
        " the bias and variance for kernel smoothing scale as\n",
        "\n",
        "$$ \\mbox{bias}^2 = O\\left(h^4\\right)$$\n",
        "$$ \\mbox{var} = O\\left(\\frac{1}{nh^p}\\right).$$\n",
        "\n",
        "Here $h$ is the bandwidth parameter, $n$ is the sample size, and $p$ is the number of predictor variables. These expressions are asymptotic, meaning that they apply as $n$ gets large and $h$ gets small.  In this problem your job is to reason about the implications of this bias-variance decomposition for prediction.\n",
        "\n",
        "*Note:* For this problem, you may either enter your answers in Markdown using $\\rm\\LaTeX$, or you write them on paper and scan to insert as an image in the notebook; whichever you prefer.\n",
        "\n",
        "\n",
        "### 2.1 Selecting the optimal bandwidth\n",
        "\n",
        "Suppose that the bias and variance are such that\n",
        "\n",
        "$$ \\mbox{bias}^2(\\hat m(x))  \\leq c_1 h^4 $$\n",
        "$$ \\mbox{var}(\\hat m(x)) \\leq c_2 \\frac{1}{nh^p}.$$\n",
        "\n",
        "for two constants $c_1$ and $c_2$. Using these expressions and a little calculus, determine the optimal bandwidth $h$ to minimize the risk function\n",
        "\n",
        "$$R(h) = {\\mathbb E}\\left(\\hat m(x) - m(x)\\right)^2.$$\n",
        "\n",
        "Your answer should involve the constants $c_1, c_2$, and $n$ and $p$. Give a bound on the resulting risk using this bandwidth.\n",
        "\n",
        "\n",
        "### 2.2 Bandwith selection without tears\n",
        "\n",
        "Now, going back to the expressions $\\mbox{bias}^2 = O\\left(h^4\\right)$ and $ \\mbox{var} = O\\left(\\displaystyle\\frac{1}{nh^p}\\right)$, explain why the scaling of the optimal bandwidth (as a function of $n$ and $p$), must satisfy\n",
        "$\\mbox{bias}^2  \\approx \\mbox{var}$; that is, they must be of the same order as $h\\to 0$. Then, without using any calculus, use this argument to determine the optimal scaling of the bandwidth and the fastest rate at which the\n",
        "risk $R(h) = {\\mathbb E}\\left(\\hat m(x) - m(x)\\right)^2$ can approach zero as the sample size increases.\n",
        "\n",
        "\n",
        "### 2.3 The cursed COD\n",
        "\n",
        "Using the risk bound you derive above, make a plot that demonstrates the curse of dimensionality by plotting the sample size required to achieve a given level of risk. Specifically, let the target risk $R$ vary between 0.1 and 0.5, and let the dimension $p$ vary between 1 and 20, and plot the sample size required to achieve that risk. Give a single plot that shows the collection of curves for each dimension.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rc1Kd06odfxp"
      },
      "outputs": [],
      "source": [
        "# your code and markdown with derivations here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T4ELSLf-dfxq"
      },
      "source": [
        "### Problem 3: A kernel of truth (15 points)\n",
        "\n",
        "For problem you will implement nonparametric regression using Mercer kernels and penalization, in 1-dimension. This can be compared to regression using smoothing kernels.\n",
        "\n",
        "As discussed in lecture, nonparametric regression with Mercer kernels is based on the infinite dimensional ridge regression\n",
        "\n",
        "$$ \\hat m = \\mbox{argmin} \\| Y - m \\|^2 + \\lambda \\|m\\|_K^2$$\n",
        "\n",
        "By the representer theorem, this is equivalent to setting $\\hat m(x) = \\sum_{i=1}^n \\hat \\alpha_i K(X_i, x)$ and\n",
        "using the finite dimensional optimization\n",
        "\n",
        "$$ \\hat \\alpha = \\mbox{argmin} \\| Y - {\\mathbb K} \\alpha \\|^2 + \\lambda \\alpha^T {\\mathbb K} \\alpha$$\n",
        "\n",
        "###  3.1 Solve\n",
        "\n",
        "Derive a closed-form expression for the minimizer $\\hat\\alpha$. Show all of the steps in your derivation,\n",
        "and justify each step. (As above, you may either enter your answers in Markdown using $\\rm\\LaTeX$, or insert an image of your handwritten solution.)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_-PWy5xIdfxq"
      },
      "source": [
        "###  3.2 Implement\n",
        "\n",
        "Next you will use your solution above and implement Mercer kernel regression. We give some starter code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-XPqTJDJdfxq"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "from IPython.display import clear_output\n",
        "from time import sleep"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0_9_rCJBdfxq"
      },
      "source": [
        "The following cell defines some \"helper functions\" for this exercise. You don't need to change any of this code.\n",
        "(If you do want to make changes, just describe what you did and why.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2XTRm3_Cdfxq"
      },
      "outputs": [],
      "source": [
        "def plot_estimate(x, f, fhat, X, y, sigma, lmbda, sleeptime=.01):\n",
        "    clear_output(wait=True)\n",
        "    plt.figure(figsize=(10,6))\n",
        "    plt.plot(x, f, color='red', linewidth=2, label='true function')\n",
        "    plt.plot(x, fhat, color='blue', linewidth=2, label='estimated function')\n",
        "    plt.scatter(X, y, color='black', alpha=.5, label='random sample')\n",
        "    plt.ylim(np.min(f)-4*sigma, np.max(f)+4*sigma)\n",
        "    plt.legend(loc='upper left')\n",
        "    plt.title('lambda: %.3g' % lmbda)\n",
        "    plt.xlabel('x')\n",
        "    plt.ylabel('estimated m(x)')\n",
        "    plt.show()\n",
        "    sleep(sleeptime)\n",
        "\n",
        "def true_fn(x):\n",
        "    return 3*x**2\n",
        "\n",
        "def run_simulation(kernel, lmbdas, show_bias_variance=True):\n",
        "    min_x, max_x = -1, 1\n",
        "    x = np.linspace(min_x, max_x, 100)\n",
        "    f = true_fn(x)\n",
        "    sigma = .25\n",
        "    estimates = []\n",
        "    trials = 500\n",
        "\n",
        "    for lmbda in lmbdas:\n",
        "        estimates_lambda = []\n",
        "        for i in np.arange(trials):\n",
        "            X = np.sort(np.random.uniform(low=min_x, high=max_x, size=50))\n",
        "            fX = true_fn(X)\n",
        "            y = fX + sigma*np.random.normal(size=len(X))\n",
        "            fhat = mercer_kernel_regress(kernel, X, y, x, lmbda=lmbda)\n",
        "            if i % 50 == 0:\n",
        "                plot_estimate(x, f, fhat, X, y, sigma, lmbda)\n",
        "            estimates_lambda.append(fhat)\n",
        "        estimates.append(estimates_lambda)\n",
        "\n",
        "    if show_bias_variance == False:\n",
        "        return\n",
        "\n",
        "    fhat = np.array(estimates)\n",
        "    sq_bias = np.zeros(len(lmbdas))\n",
        "    variance = np.zeros(len(lmbdas))\n",
        "\n",
        "    for i in np.arange(len(lmbdas)):\n",
        "        sq_bias[i] = np.mean((np.mean(fhat[i], axis=0) - f)**2)\n",
        "        variance[i] = np.mean(np.var(fhat[i], axis=0))\n",
        "\n",
        "    plt.figure(figsize=(10,6))\n",
        "    plt.plot(lmbdas, sq_bias, label='squared bias', linewidth=2)\n",
        "    plt.plot(lmbdas, variance, label='variance', linewidth=2)\n",
        "    plt.plot(lmbdas, sq_bias + variance, label='risk')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c2CoyP-edfxq"
      },
      "source": [
        "Your job is to implement Mercer kernel regression and run it on two\n",
        "different kernel functions. The two kernels could simply be the Gaussian kernel\n",
        "with two different bandwidths, or you might experiment with other kernels.\n",
        "\n",
        "The function `mercer_kernel_regress` takes a kernel, training data `X` and `y`, an array of values `x` to evaluate the function on, and a regularization parameter. You'll use your derivation above to\n",
        "determine the coefficients $\\alpha$. For some clues and suggestions on how to do the\n",
        "implementation, see our demo code for smoothing kernels. You need to do something very similar.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2jHl-INWdfxq"
      },
      "outputs": [],
      "source": [
        "def mercer_kernel_regress(kernel, X, y, x, lmbda):\n",
        "    # your implementation here\n",
        "    _\n",
        "\n",
        "def kernel1(x,y):\n",
        "    # your implementation here\n",
        "    _\n",
        "\n",
        "\n",
        "def kernel2(x,y):\n",
        "    # your implementation here\n",
        "    _"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qicpLbh8dfxq"
      },
      "source": [
        "###  3.3 Run two simulations and select regularization parameters\n",
        "\n",
        "Finally, using our starter code and your own implementation above, run two simulations, one\n",
        "using `kernel1` and the other using `kernel2`. After each simulation, select a regularization level from the bias-variance tradeoff, and then run a final simulation with that regularization level. In the following\n",
        "starter code, you only need to specify the sequence of regularization parameters.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x_WZvvrjdfxq"
      },
      "outputs": [],
      "source": [
        "lmbdas = # define your sequence of lambdas\n",
        "run_simulation(kernel1, lmbdas)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J9GAYnpHdfxq"
      },
      "outputs": [],
      "source": [
        "lambda_hat = # set the optimal lambda\n",
        "run_simulation(kernel1, [lambda_hat], show_bias_variance=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fpcE3AV9dfxq"
      },
      "outputs": [],
      "source": [
        "lmbdas = # define your sequence of lambdas\n",
        "run_simulation(kernel2, lmbdas)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1mWVEI9qdfxq"
      },
      "source": [
        "### Problem 4: An algebraic simplification of LOOCV (15 points)\n",
        "\n",
        "Leave-One-Out Cross Validation (LOOCV) is a specific type of\n",
        "$K$-fold cross validation where $K$ equals the number of observations in the dataset.\n",
        "It works as follows for a training set with $n$ observations:\n",
        "\n",
        "1. A single observation is used as the validation set,\n",
        "    and the remaining $n-1$ observations serve as the training set.\n",
        "2. A model is trained on the $n-1$ observations and\n",
        "    validated on the single left-out observation.\n",
        "3. This process is repeated $n$ times, each time leaving out a different\n",
        "    observation as the validation set.\n",
        "4. The LOOCV error is then the average error across all $n$ trials.\n",
        "\n",
        "LOOCV is particularly useful because:\n",
        "- It utilizes almost all the data for training,\n",
        "    so it's less prone to high variance compared to other validation schemes.\n",
        "- Since each observation is tested exactly once,\n",
        "    LOOCV provides a very thorough out-of-sample testing mechanism.\n",
        "\n",
        "However, it can be computationally expensive because you have to fit the model $n$ times.\n",
        "    Luckily, for some models, there are algebraic simplifications available\n",
        "    that make it computationally efficient.\n",
        "    Expressing LOOCV in terms of the hat matrix allows for efficient\n",
        "    computation of the LOOCV error without the need to refit the model for\n",
        "    each left-out observation, making it a valuable tool for model evaluation.\n",
        "\n",
        "Recall that the LOOCV error can be expressed as:\n",
        "\n",
        "$$ LOOCV = \\frac{1}{n} \\sum_{i=1}^{n} \\left( y_i - \\hat{y}_{-i} \\right)^2, $$\n",
        "\n",
        "where $\\hat{y}_{-i}$ represents the prediction for the $i^{th}$ observation\n",
        "when it's left out from the training process.\n",
        "In the following questions, you will be deriving an alternative expression\n",
        "of the LOOCV error for both kernel and ridge regression, following the hints below."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s764_USVdfxq"
      },
      "source": [
        "### 1. LOOCV for kernel smoothing:\n",
        "\n",
        "For kernels, we know that the LOOCV error can be equivalently written as the following form:\n",
        "\n",
        "$$ LOOCV = \\frac{1}{n} \\sum_{i=1}^{n} \\left( \\frac{y_i - \\hat y_i}{1 - L_{ii}} \\right)^2, $$\n",
        "where $\\hat y_i$ is the predicted value from the model fit on all data, and\n",
        "$L_{ii}$ is the $i^{th}$ diagonal element of the hat matrix $L$.\n",
        "\n",
        "For kernel regression, we have\n",
        "$$ \\hat{y} = L y, $$\n",
        "where\n",
        "- $ \\hat{y} $ is the vector of predictions.\n",
        "- $ y $ is the observed response values.\n",
        "- $ L $ is the hat matrix and is defined by the kernel (for a given bandwidth).\n",
        "So, each diagonal element $ L_{ii} $ of the matrix $ L $ is defined as:\n",
        "$$ L_{ii} = \\frac{K\\left(x_i, x_i\\right)}{\\sum_{j=1}^{n} K\\left(x_i, x_j\\right)}, $$\n",
        "where\n",
        "- $ K $ is the kernel function.\n",
        "- $ x_i $ and $ x_j $ are the predictor values for observations $ i $ and $ j $, respectively.\n",
        "\n",
        "The diagonal elements $ L_{ii} $ give the \"leverage\" of each observation, which can be interpreted as the influence an observation has on its own prediction.\n",
        "\n",
        "Derive this alternative expression of the LOOCV error for kernel regression. That's to say, for kernel regression, prove that\n",
        "\n",
        "$$ y_i - \\hat{y}_{-i}  =  \\frac{y_i - \\hat y_i}{1 - L_{ii}}$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DWfW-xY5dfxq"
      },
      "outputs": [],
      "source": [
        "# Your markdown here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6hl05q9Zdfxq"
      },
      "source": [
        "## Problem 5: LASSO and Elastic Net\n",
        "\n",
        "In class, we derived the solution for the one-dimensional LASSO using convexity and subgradients. We also know that coordinate descent can be used to solve LASSO in the high-dimensional case. Sometimes, we introduce a penalty that is a mixture of the $\\ell_1$ and $\\ell_2$ norms,  \n",
        "\n",
        "$$\n",
        "\\lambda_1 \\|\\beta\\|_1 + \\lambda_2 \\|\\beta\\|_2^2,\n",
        "$$  \n",
        "\n",
        "which is referred to as the **Elastic Net**. This problem demonstrates how the Elastic Net can be computed using the LASSO.\n",
        "\n",
        "Let us first recall the one-dimensional LASSO objective:\n",
        "\n",
        "$$\n",
        "f(\\beta) = \\frac{a}{2}\\beta^2 - b\\beta + \\lambda |\\beta|,\n",
        "\\qquad a > 0,\\; \\lambda \\ge 0.\n",
        "$$\n",
        "\n",
        "The minimizer is given by\n",
        "\n",
        "$$\n",
        "\\widehat{\\beta} = \\frac{\\operatorname{sign}(b)\\,\\big(|b| - \\lambda\\big)_+}{a},\n",
        "$$\n",
        "\n",
        "where $(x)_+ = \\max\\{x,0\\}$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T4JUhN3udfxq"
      },
      "source": [
        "5.1 Solve the one-dimensional LASSO problem\n",
        "\n",
        "$$\n",
        "\\widehat{\\beta} = \\arg\\min_\\beta \\left\\{ \\frac{1}{2n}\\sum_{i=1}^n (y_i - \\beta x_i)^2 + \\lambda |\\beta| \\right\\},\n",
        "$$\n",
        "\n",
        "where $\\lambda \\geq 0$ and at least one $x_i \\neq 0$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4vZgHz2kdfxq"
      },
      "outputs": [],
      "source": [
        "# Your markdown here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FucWZaHZdfxq"
      },
      "source": [
        "5.2 Solve the one-dimensional Elastic Net\n",
        "$$\n",
        "\\widehat{\\beta} = \\arg\\min_\\beta \\left\\{ \\frac{1}{2n} \\sum_{i=1}^n (y_i - \\beta x_i)^2 + \\lambda_1 |\\beta| + \\lambda_2 \\beta^2 \\right\\},\n",
        "$$\n",
        "\n",
        "where $\\lambda_1 \\geq 0$, $\\lambda_2 \\geq 0$, and at least one $x_i \\neq 0$.  \n",
        "\n",
        "*Hint:* Convert this problem into the form of 5.1. What happens if we observe a new data point $(\\sqrt{2n\\lambda_2}, 0)$?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "69gkHYF0dfxr"
      },
      "outputs": [],
      "source": [
        "# Your markdown here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QVSxO8KKdfxr"
      },
      "source": [
        "5.3 Now consider the case where we have data $\\mathbf{Y} \\in \\mathbb{R}^n$ and $\\mathbf{X} \\in \\mathbb{R}^{n \\times p}$. We want to find an estimator\n",
        "\n",
        "$$\n",
        "\\widehat{\\beta} = \\arg\\min_\\beta \\left\\{ \\frac{1}{2n}\\| \\mathbf{Y} - \\mathbf{X}\\beta\\|_2^2 + \\lambda_1 \\|\\beta\\|_1 + \\lambda_2 \\|\\beta\\|_2^2 \\right\\}.\n",
        "$$\n",
        "\n",
        "Assume we have access to an oracle that can solve the LASSO. How can we use it to solve the Elastic Net?  \n",
        "\n",
        "*Hint:* Construct augmented variables $\\mathbf{Y}^*$ and $\\mathbf{X}^*$\n",
        "(not necessarily the same shapes as $\\mathbf{Y}$ and $\\mathbf{X}$) and show that the objective is equivalent to\n",
        "$$\n",
        "\\frac{1}{2\\,\\mathrm{len}(\\mathbf{Y}^*)}\n",
        "\\|\\mathbf{Y}^* - \\mathbf{X}^*\\beta\\|_2^2\n",
        "+ \\lambda_1 \\|\\beta\\|_1.\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ljAJJIjSdfxr"
      },
      "outputs": [],
      "source": [
        "# Your markdown here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sAgJpSyfdfxr"
      },
      "source": [
        "5.4 Now revisit the diabetes dataset from the first problem. Apply the result you derived, together with the `Lasso` implementation from `sklearn.linear_model`, to solve the elastic net. Specifically, we want to solve\n",
        "\n",
        "$$\n",
        "\\widehat{\\beta} = \\arg\\min_\\beta \\left\\{ \\frac{1}{2n}\\|\\mathbf{Y} - \\mathbf{X}\\beta\\|_2^2\n",
        "+ \\lambda_1 \\|\\beta\\|_1\n",
        "+ \\lambda_2 \\|\\beta\\|_2^2 \\right\\},\n",
        "$$\n",
        "where $\\lambda_1 = \\lambda_2 = 0.1$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yZlsE3Npdfxr"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import pickle\n",
        "from sklearn.linear_model import Lasso\n",
        "from sklearn.linear_model import ElasticNet\n",
        "%matplotlib inline\n",
        "X, y = load_diabetes(return_X_y=True)\n",
        "X = np.hstack([np.ones((X.shape[0], 1)), X])\n",
        "n, p = X.shape\n",
        "n, p = X.shape\n",
        "\n",
        "lambda1,lambda2 = 0.1, 0.1\n",
        "# y_star = your code here\n",
        "# X_star = your code here\n",
        "model = Lasso(alpha=lambda1, fit_intercept=False)\n",
        "model.fit(X_star, y_star)\n",
        "your_coef = model.coef_\n",
        "\n",
        "# You can use the following code to verify your answer\n",
        "from sklearn.linear_model import ElasticNet\n",
        "lambda1, lambda2 = 0.1, 0.1\n",
        "alpha = lambda1 + 2 * lambda2\n",
        "l1_ratio = lambda1 / (lambda1 + 2 * lambda2)\n",
        "\n",
        "# Fit ElasticNet with your X, y\n",
        "model = ElasticNet(alpha=alpha,\n",
        "                   l1_ratio=l1_ratio,\n",
        "                   fit_intercept=False,\n",
        "                   max_iter=10000,\n",
        "                   tol=1e-6,\n",
        "                   selection='cyclic')\n",
        "\n",
        "model.fit(X, y)\n",
        "elastic_coef = model.coef_\n",
        "print(np.linalg.norm(your_coef - elastic_coef)/np.linalg.norm(elastic_coef))  # should be a very small number"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ouh5HFehdfxr"
      },
      "source": [
        "5.5 Which values of $\\lambda_1$ and $\\lambda_2$ are appropriate? A common approach is to use cross-validation. For simplicity, split the data once into training and validation sets, fit the model with different $(\\lambda_1,\\lambda_2)$, and compare the mean squared error (MSE) on the validation set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KEr16-Ardfxr"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_squared_error\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=365)\n",
        "scalerX, scalery = StandardScaler(), StandardScaler()\n",
        "X_train = scalerX.fit_transform(X_train)\n",
        "X_test = scalerX.transform(X_test)\n",
        "y_train = scalery.fit_transform(y_train.reshape(-1,1)).ravel()\n",
        "y_test = scalery.transform(y_test.reshape(-1,1)).ravel()\n",
        "grid = np.linspace(0.001, 1.0, 30)\n",
        "params = []\n",
        "mses = []\n",
        "for lambda1 in grid:\n",
        "    for lambda2 in grid:\n",
        "        # fit a elastic net with lambda1 and lambda2 on (X_train, y_train)\n",
        "        # compute the mse on (X_test, y_test)\n",
        "        # you should use Lasso from sklearn. don't use ElasticNet.\n",
        "        params.append((lambda1, lambda2))\n",
        "        mses.append(mse)\n",
        "your_lambda1,your_lambda2 = params[np.argmin(mses)]\n",
        "\n",
        "#You can verify your answer using the following code\n",
        "params2 = []\n",
        "mses2 = []\n",
        "for lambda1 in grid:\n",
        "    for lambda2 in grid:\n",
        "        alpha = lambda1 + 2 * lambda2\n",
        "        l1_ratio = lambda1 / (lambda1 + 2 * lambda2)\n",
        "        model = ElasticNet(alpha=alpha,\n",
        "                   l1_ratio=l1_ratio,\n",
        "                   fit_intercept=False,\n",
        "                   max_iter=10000,\n",
        "                   tol=1e-6,\n",
        "                   selection='cyclic')\n",
        "        model.fit(X_train, y_train)\n",
        "        mse = mean_squared_error(y_test, model.predict(X_test))\n",
        "        params2.append((lambda1, lambda2))\n",
        "        mses2.append(mse)\n",
        "select_lambda1,select_lambda2 = params2[np.argmin(mses2)]\n",
        "\n",
        "print(your_lambda1 == select_lambda1 and your_lambda2 == select_lambda2)   # should be True"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "vscode": {
      "interpreter": {
        "hash": "b4a7aa81d2787d79eab8f6fb8f7b5343089747e772b346faf833d5b60a4f70bf"
      }
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}